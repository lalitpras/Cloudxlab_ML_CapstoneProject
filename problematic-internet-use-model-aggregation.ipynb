{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"At this stage, we have determined:\n* Our chosen approach to imputing the missing features\n* Our optimal models (data augmentation, PCA, sample weights, actigraph inclusion, target + eval function, model parameters) for:\n    * XGBoost\n    * DNN\n\nHere, we incorporate the above, and look for the best way to combine the predictions from our models trained on the labelled data.\n\nOptions:\n* single model\n* simple average of multiple models\n* weighted average based on confidence score of models","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Steps\n1. Data import, split and preprocessing\n2. Imputation of missing features\n3. Create supplementary datasets and functions necessary for our models\n4. Models\n    * XGBoost\n    * DNN\n5. Aggregation approaches","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\n\nimport polars as pl\nfrom glob import glob\nfrom tqdm.auto import tqdm\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport random\n\nnp.random.seed(42)\ntf.random.set_seed(42)\nrandom.seed(42)\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n#from sklearn.impute import KNNImputer\n\n#from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.metrics import cohen_kappa_score\n\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom tensorflow.keras import layers, models, regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\nfrom sklearn.svm import SVR\n\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T09:47:10.028507Z","iopub.execute_input":"2025-01-26T09:47:10.028845Z","iopub.status.idle":"2025-01-26T09:47:29.686796Z","shell.execute_reply.started":"2025-01-26T09:47:10.028818Z","shell.execute_reply":"2025-01-26T09:47:29.685648Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Data import and preprocessing","metadata":{}},{"cell_type":"code","source":"train_data=pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest_data = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T09:47:29.688383Z","iopub.execute_input":"2025-01-26T09:47:29.689363Z","iopub.status.idle":"2025-01-26T09:47:29.766684Z","shell.execute_reply.started":"2025-01-26T09:47:29.689333Z","shell.execute_reply":"2025-01-26T09:47:29.765774Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T09:52:16.153224Z","iopub.execute_input":"2025-01-26T09:52:16.153676Z","iopub.status.idle":"2025-01-26T09:52:16.181450Z","shell.execute_reply.started":"2025-01-26T09:52:16.153640Z","shell.execute_reply":"2025-01-26T09:52:16.180210Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"         id Basic_Demos-Enroll_Season  Basic_Demos-Age  Basic_Demos-Sex  \\\n0  00008ff9                      Fall                5                0   \n1  000fd460                    Summer                9                0   \n2  00105258                    Summer               10                1   \n3  00115b9f                    Winter                9                0   \n4  0016bb22                    Spring               18                1   \n\n  CGAS-Season  CGAS-CGAS_Score Physical-Season  Physical-BMI  Physical-Height  \\\n0      Winter             51.0            Fall     16.877316             46.0   \n1         NaN              NaN            Fall     14.035590             48.0   \n2        Fall             71.0            Fall     16.648696             56.5   \n3        Fall             71.0          Summer     18.292347             56.0   \n4      Summer              NaN             NaN           NaN              NaN   \n\n   Physical-Weight  ...  PCIAT-PCIAT_18  PCIAT-PCIAT_19  PCIAT-PCIAT_20  \\\n0             50.8  ...             4.0             2.0             4.0   \n1             46.0  ...             0.0             0.0             0.0   \n2             75.6  ...             2.0             1.0             1.0   \n3             81.6  ...             3.0             4.0             1.0   \n4              NaN  ...             NaN             NaN             NaN   \n\n   PCIAT-PCIAT_Total SDS-Season  SDS-SDS_Total_Raw  SDS-SDS_Total_T  \\\n0               55.0        NaN                NaN              NaN   \n1                0.0       Fall               46.0             64.0   \n2               28.0       Fall               38.0             54.0   \n3               44.0     Summer               31.0             45.0   \n4                NaN        NaN                NaN              NaN   \n\n   PreInt_EduHx-Season PreInt_EduHx-computerinternet_hoursday  sii  \n0                 Fall                                    3.0  2.0  \n1               Summer                                    0.0  0.0  \n2               Summer                                    2.0  0.0  \n3               Winter                                    0.0  1.0  \n4                  NaN                                    NaN  NaN  \n\n[5 rows x 82 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Basic_Demos-Enroll_Season</th>\n      <th>Basic_Demos-Age</th>\n      <th>Basic_Demos-Sex</th>\n      <th>CGAS-Season</th>\n      <th>CGAS-CGAS_Score</th>\n      <th>Physical-Season</th>\n      <th>Physical-BMI</th>\n      <th>Physical-Height</th>\n      <th>Physical-Weight</th>\n      <th>...</th>\n      <th>PCIAT-PCIAT_18</th>\n      <th>PCIAT-PCIAT_19</th>\n      <th>PCIAT-PCIAT_20</th>\n      <th>PCIAT-PCIAT_Total</th>\n      <th>SDS-Season</th>\n      <th>SDS-SDS_Total_Raw</th>\n      <th>SDS-SDS_Total_T</th>\n      <th>PreInt_EduHx-Season</th>\n      <th>PreInt_EduHx-computerinternet_hoursday</th>\n      <th>sii</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>Fall</td>\n      <td>5</td>\n      <td>0</td>\n      <td>Winter</td>\n      <td>51.0</td>\n      <td>Fall</td>\n      <td>16.877316</td>\n      <td>46.0</td>\n      <td>50.8</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>55.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Fall</td>\n      <td>3.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>Summer</td>\n      <td>9</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Fall</td>\n      <td>14.035590</td>\n      <td>48.0</td>\n      <td>46.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Fall</td>\n      <td>46.0</td>\n      <td>64.0</td>\n      <td>Summer</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>Summer</td>\n      <td>10</td>\n      <td>1</td>\n      <td>Fall</td>\n      <td>71.0</td>\n      <td>Fall</td>\n      <td>16.648696</td>\n      <td>56.5</td>\n      <td>75.6</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>Fall</td>\n      <td>38.0</td>\n      <td>54.0</td>\n      <td>Summer</td>\n      <td>2.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>Winter</td>\n      <td>9</td>\n      <td>0</td>\n      <td>Fall</td>\n      <td>71.0</td>\n      <td>Summer</td>\n      <td>18.292347</td>\n      <td>56.0</td>\n      <td>81.6</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>44.0</td>\n      <td>Summer</td>\n      <td>31.0</td>\n      <td>45.0</td>\n      <td>Winter</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>Spring</td>\n      <td>18</td>\n      <td>1</td>\n      <td>Summer</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 82 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"INPUT_DIR = \"/kaggle/input/child-mind-institute-problematic-internet-use/\"\n\n# Import aggregate fields from parquet files\n# Modified code from rsakata: https://www.kaggle.com/code/rsakata/cmi-piu-16th-place-solution\n\nfiles_train = glob(INPUT_DIR + \"series_train.parquet/*\")\n#if IS_SUBMIT:\n#    files += glob(INPUT_DIR + \"series_test.parquet/*\")\n\nlist_df_train = []\nfor file in tqdm(files_train):\n    df_series = (\n        pl.read_parquet(file)\n        .with_columns(\n            (\n                (pl.col(\"relative_date_PCIAT\") - pl.col(\"relative_date_PCIAT\").min())*24\n                + (pl.col(\"time_of_day\") // int(1e9)) / 3600\n            ).floor().cast(int).alias(\"total_hours\")\n        )\n        .filter(pl.col(\"non-wear_flag\") != 1)\n        .filter(pl.col(\"step\").count().over(\"total_hours\") == 12 * 60)\n        .group_by(\"total_hours\").agg(\n            pl.col(\"enmo\").std().alias(\"enmo_std\"),\n            pl.col(\"anglez\").std().alias(\"anglez_std\"),\n            pl.col(\"light\").std().alias(\"light_std\")\n        )\n        .with_columns(\n            (pl.col(\"total_hours\") % 24).alias(\"hour\"),\n            pl.lit(file.split(\"/\")[-1][3:]).alias(\"id\")\n        )\n    )\n    list_df_train.append(df_series.to_pandas())\n\ndf_series = pd.concat(list_df_train)\ndf_series[\"enmo_std\"] = np.log(df_series[\"enmo_std\"] + 0.01)\ndf_series[\"anglez_std\"] = np.log(df_series[\"anglez_std\"] + 1)\ndf_series[\"light_std\"] = np.log(df_series[\"light_std\"] + 0.01)\n\ndf_agg_train = df_series.groupby(\"id\")[[\"enmo_std\", \"anglez_std\", \"light_std\"]].agg([\"mean\", \"std\"]).reset_index()\ndf_agg_train.columns = [cols[0] + \"_\" + cols[1] if cols[1] != \"\" else cols[0] for cols in df_agg_train.columns]\ndf_agg_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T09:47:29.768066Z","iopub.execute_input":"2025-01-26T09:47:29.768384Z","iopub.status.idle":"2025-01-26T09:48:33.880402Z","shell.execute_reply.started":"2025-01-26T09:47:29.768357Z","shell.execute_reply":"2025-01-26T09:48:33.879217Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/996 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a20ef885e12412a8555fc97d6955c71"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"           id  enmo_std_mean  enmo_std_std  anglez_std_mean  anglez_std_std  \\\n0    00115b9f      -4.000377           NaN         1.989905             NaN   \n1    001f3379      -3.514671      0.652348         3.236993        0.678888   \n2    00f332d1      -3.071176      0.927238         3.249122        0.463244   \n3    01085eb3      -2.902040      0.791255         3.389762        0.315061   \n4    012cadd8      -2.806918      1.171675         3.337322        0.388409   \n..        ...            ...           ...              ...             ...   \n964  fe9c71d8      -3.116904      0.961804         3.037607        0.943554   \n965  fecc07d6      -3.969482      0.981531         1.332831        1.428363   \n966  ff18b749      -2.820076      0.937540         3.258458        0.417267   \n967  ffcd4dbd      -3.271800      0.827489         3.183395        0.629553   \n968  ffed1dd5      -3.359100      1.151259         2.489726        1.391600   \n\n     light_std_mean  light_std_std  \n0          0.051475            NaN  \n1          0.774591       2.945807  \n2          1.138379       2.939823  \n3          1.054698       2.185839  \n4          0.823770       3.350365  \n..              ...            ...  \n964       -0.394200       2.742634  \n965       -0.438018       1.795653  \n966        1.236652       3.341580  \n967        0.521227       2.665325  \n968       -1.597793       2.344771  \n\n[969 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>enmo_std_mean</th>\n      <th>enmo_std_std</th>\n      <th>anglez_std_mean</th>\n      <th>anglez_std_std</th>\n      <th>light_std_mean</th>\n      <th>light_std_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00115b9f</td>\n      <td>-4.000377</td>\n      <td>NaN</td>\n      <td>1.989905</td>\n      <td>NaN</td>\n      <td>0.051475</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001f3379</td>\n      <td>-3.514671</td>\n      <td>0.652348</td>\n      <td>3.236993</td>\n      <td>0.678888</td>\n      <td>0.774591</td>\n      <td>2.945807</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00f332d1</td>\n      <td>-3.071176</td>\n      <td>0.927238</td>\n      <td>3.249122</td>\n      <td>0.463244</td>\n      <td>1.138379</td>\n      <td>2.939823</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>01085eb3</td>\n      <td>-2.902040</td>\n      <td>0.791255</td>\n      <td>3.389762</td>\n      <td>0.315061</td>\n      <td>1.054698</td>\n      <td>2.185839</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>012cadd8</td>\n      <td>-2.806918</td>\n      <td>1.171675</td>\n      <td>3.337322</td>\n      <td>0.388409</td>\n      <td>0.823770</td>\n      <td>3.350365</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>964</th>\n      <td>fe9c71d8</td>\n      <td>-3.116904</td>\n      <td>0.961804</td>\n      <td>3.037607</td>\n      <td>0.943554</td>\n      <td>-0.394200</td>\n      <td>2.742634</td>\n    </tr>\n    <tr>\n      <th>965</th>\n      <td>fecc07d6</td>\n      <td>-3.969482</td>\n      <td>0.981531</td>\n      <td>1.332831</td>\n      <td>1.428363</td>\n      <td>-0.438018</td>\n      <td>1.795653</td>\n    </tr>\n    <tr>\n      <th>966</th>\n      <td>ff18b749</td>\n      <td>-2.820076</td>\n      <td>0.937540</td>\n      <td>3.258458</td>\n      <td>0.417267</td>\n      <td>1.236652</td>\n      <td>3.341580</td>\n    </tr>\n    <tr>\n      <th>967</th>\n      <td>ffcd4dbd</td>\n      <td>-3.271800</td>\n      <td>0.827489</td>\n      <td>3.183395</td>\n      <td>0.629553</td>\n      <td>0.521227</td>\n      <td>2.665325</td>\n    </tr>\n    <tr>\n      <th>968</th>\n      <td>ffed1dd5</td>\n      <td>-3.359100</td>\n      <td>1.151259</td>\n      <td>2.489726</td>\n      <td>1.391600</td>\n      <td>-1.597793</td>\n      <td>2.344771</td>\n    </tr>\n  </tbody>\n</table>\n<p>969 rows × 7 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"train_data2 = train_data.merge(df_agg_train, how=\"left\", on=\"id\")\ntrain_data2.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T09:48:33.881536Z","iopub.execute_input":"2025-01-26T09:48:33.881821Z","iopub.status.idle":"2025-01-26T09:48:33.915584Z","shell.execute_reply.started":"2025-01-26T09:48:33.881788Z","shell.execute_reply":"2025-01-26T09:48:33.914500Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"         id Basic_Demos-Enroll_Season  Basic_Demos-Age  Basic_Demos-Sex  \\\n0  00008ff9                      Fall                5                0   \n1  000fd460                    Summer                9                0   \n2  00105258                    Summer               10                1   \n3  00115b9f                    Winter                9                0   \n4  0016bb22                    Spring               18                1   \n\n  CGAS-Season  CGAS-CGAS_Score Physical-Season  Physical-BMI  Physical-Height  \\\n0      Winter             51.0            Fall     16.877316             46.0   \n1         NaN              NaN            Fall     14.035590             48.0   \n2        Fall             71.0            Fall     16.648696             56.5   \n3        Fall             71.0          Summer     18.292347             56.0   \n4      Summer              NaN             NaN           NaN              NaN   \n\n   Physical-Weight  ...  SDS-SDS_Total_T  PreInt_EduHx-Season  \\\n0             50.8  ...              NaN                 Fall   \n1             46.0  ...             64.0               Summer   \n2             75.6  ...             54.0               Summer   \n3             81.6  ...             45.0               Winter   \n4              NaN  ...              NaN                  NaN   \n\n   PreInt_EduHx-computerinternet_hoursday  sii enmo_std_mean  enmo_std_std  \\\n0                                     3.0  2.0           NaN           NaN   \n1                                     0.0  0.0           NaN           NaN   \n2                                     2.0  0.0           NaN           NaN   \n3                                     0.0  1.0     -4.000377           NaN   \n4                                     NaN  NaN           NaN           NaN   \n\n   anglez_std_mean  anglez_std_std light_std_mean  light_std_std  \n0              NaN             NaN            NaN            NaN  \n1              NaN             NaN            NaN            NaN  \n2              NaN             NaN            NaN            NaN  \n3         1.989905             NaN       0.051475            NaN  \n4              NaN             NaN            NaN            NaN  \n\n[5 rows x 88 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Basic_Demos-Enroll_Season</th>\n      <th>Basic_Demos-Age</th>\n      <th>Basic_Demos-Sex</th>\n      <th>CGAS-Season</th>\n      <th>CGAS-CGAS_Score</th>\n      <th>Physical-Season</th>\n      <th>Physical-BMI</th>\n      <th>Physical-Height</th>\n      <th>Physical-Weight</th>\n      <th>...</th>\n      <th>SDS-SDS_Total_T</th>\n      <th>PreInt_EduHx-Season</th>\n      <th>PreInt_EduHx-computerinternet_hoursday</th>\n      <th>sii</th>\n      <th>enmo_std_mean</th>\n      <th>enmo_std_std</th>\n      <th>anglez_std_mean</th>\n      <th>anglez_std_std</th>\n      <th>light_std_mean</th>\n      <th>light_std_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>Fall</td>\n      <td>5</td>\n      <td>0</td>\n      <td>Winter</td>\n      <td>51.0</td>\n      <td>Fall</td>\n      <td>16.877316</td>\n      <td>46.0</td>\n      <td>50.8</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>Fall</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>Summer</td>\n      <td>9</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Fall</td>\n      <td>14.035590</td>\n      <td>48.0</td>\n      <td>46.0</td>\n      <td>...</td>\n      <td>64.0</td>\n      <td>Summer</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>Summer</td>\n      <td>10</td>\n      <td>1</td>\n      <td>Fall</td>\n      <td>71.0</td>\n      <td>Fall</td>\n      <td>16.648696</td>\n      <td>56.5</td>\n      <td>75.6</td>\n      <td>...</td>\n      <td>54.0</td>\n      <td>Summer</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>Winter</td>\n      <td>9</td>\n      <td>0</td>\n      <td>Fall</td>\n      <td>71.0</td>\n      <td>Summer</td>\n      <td>18.292347</td>\n      <td>56.0</td>\n      <td>81.6</td>\n      <td>...</td>\n      <td>45.0</td>\n      <td>Winter</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-4.000377</td>\n      <td>NaN</td>\n      <td>1.989905</td>\n      <td>NaN</td>\n      <td>0.051475</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>Spring</td>\n      <td>18</td>\n      <td>1</td>\n      <td>Summer</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 88 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"files_test = glob(INPUT_DIR + \"series_test.parquet/*\")\n\n\nlist_df_test = []\nfor file in tqdm(files_test):\n    df_series = (\n        pl.read_parquet(file)\n        .with_columns(\n            (\n                (pl.col(\"relative_date_PCIAT\") - pl.col(\"relative_date_PCIAT\").min())*24\n                + (pl.col(\"time_of_day\") // int(1e9)) / 3600\n            ).floor().cast(int).alias(\"total_hours\")\n        )\n        .filter(pl.col(\"non-wear_flag\") != 1)\n        .filter(pl.col(\"step\").count().over(\"total_hours\") == 12 * 60)\n        .group_by(\"total_hours\").agg(\n            pl.col(\"enmo\").std().alias(\"enmo_std\"),\n            pl.col(\"anglez\").std().alias(\"anglez_std\"),\n            pl.col(\"light\").std().alias(\"light_std\")\n        )\n        .with_columns(\n            (pl.col(\"total_hours\") % 24).alias(\"hour\"),\n            pl.lit(file.split(\"/\")[-1][3:]).alias(\"id\")\n        )\n    )\n    list_df_test.append(df_series.to_pandas())\n\ndf_series = pd.concat(list_df_test)\ndf_series[\"enmo_std\"] = np.log(df_series[\"enmo_std\"] + 0.01)\ndf_series[\"anglez_std\"] = np.log(df_series[\"anglez_std\"] + 1)\ndf_series[\"light_std\"] = np.log(df_series[\"light_std\"] + 0.01)\n\ndf_agg_test = df_series.groupby(\"id\")[[\"enmo_std\", \"anglez_std\", \"light_std\"]].agg([\"mean\", \"std\"]).reset_index()\ndf_agg_test.columns = [cols[0] + \"_\" + cols[1] if cols[1] != \"\" else cols[0] for cols in df_agg_test.columns]\ndf_agg_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T09:50:37.625335Z","iopub.execute_input":"2025-01-26T09:50:37.625764Z","iopub.status.idle":"2025-01-26T09:50:37.775887Z","shell.execute_reply.started":"2025-01-26T09:50:37.625704Z","shell.execute_reply":"2025-01-26T09:50:37.774988Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2b52f4c691342a8bbc284aca8461442"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"         id  enmo_std_mean  enmo_std_std  anglez_std_mean  anglez_std_std  \\\n0  00115b9f      -4.000377           NaN         1.989905             NaN   \n1  001f3379      -3.514671      0.652348         3.236993        0.678888   \n\n   light_std_mean  light_std_std  \n0        0.051475            NaN  \n1        0.774591       2.945807  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>enmo_std_mean</th>\n      <th>enmo_std_std</th>\n      <th>anglez_std_mean</th>\n      <th>anglez_std_std</th>\n      <th>light_std_mean</th>\n      <th>light_std_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00115b9f</td>\n      <td>-4.000377</td>\n      <td>NaN</td>\n      <td>1.989905</td>\n      <td>NaN</td>\n      <td>0.051475</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001f3379</td>\n      <td>-3.514671</td>\n      <td>0.652348</td>\n      <td>3.236993</td>\n      <td>0.678888</td>\n      <td>0.774591</td>\n      <td>2.945807</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"test_data2 = test_data.merge(df_agg_test, how=\"left\", on=\"id\")\ntest_data2.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T09:50:39.703885Z","iopub.execute_input":"2025-01-26T09:50:39.704295Z","iopub.status.idle":"2025-01-26T09:50:39.733803Z","shell.execute_reply.started":"2025-01-26T09:50:39.704260Z","shell.execute_reply":"2025-01-26T09:50:39.732670Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"         id Basic_Demos-Enroll_Season  Basic_Demos-Age  Basic_Demos-Sex  \\\n0  00008ff9                      Fall                5                0   \n1  000fd460                    Summer                9                0   \n2  00105258                    Summer               10                1   \n3  00115b9f                    Winter                9                0   \n4  0016bb22                    Spring               18                1   \n\n  CGAS-Season  CGAS-CGAS_Score Physical-Season  Physical-BMI  Physical-Height  \\\n0      Winter             51.0            Fall     16.877316             46.0   \n1         NaN              NaN            Fall     14.035590             48.0   \n2        Fall             71.0            Fall     16.648696             56.5   \n3        Fall             71.0          Summer     18.292347             56.0   \n4      Summer              NaN             NaN           NaN              NaN   \n\n   Physical-Weight  ...  SDS-SDS_Total_Raw  SDS-SDS_Total_T  \\\n0             50.8  ...                NaN              NaN   \n1             46.0  ...               46.0             64.0   \n2             75.6  ...               38.0             54.0   \n3             81.6  ...               31.0             45.0   \n4              NaN  ...                NaN              NaN   \n\n   PreInt_EduHx-Season  PreInt_EduHx-computerinternet_hoursday enmo_std_mean  \\\n0                 Fall                                     3.0           NaN   \n1               Summer                                     0.0           NaN   \n2               Summer                                     2.0           NaN   \n3               Winter                                     0.0     -4.000377   \n4                  NaN                                     NaN           NaN   \n\n   enmo_std_std  anglez_std_mean  anglez_std_std light_std_mean  light_std_std  \n0           NaN              NaN             NaN            NaN            NaN  \n1           NaN              NaN             NaN            NaN            NaN  \n2           NaN              NaN             NaN            NaN            NaN  \n3           NaN         1.989905             NaN       0.051475            NaN  \n4           NaN              NaN             NaN            NaN            NaN  \n\n[5 rows x 65 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Basic_Demos-Enroll_Season</th>\n      <th>Basic_Demos-Age</th>\n      <th>Basic_Demos-Sex</th>\n      <th>CGAS-Season</th>\n      <th>CGAS-CGAS_Score</th>\n      <th>Physical-Season</th>\n      <th>Physical-BMI</th>\n      <th>Physical-Height</th>\n      <th>Physical-Weight</th>\n      <th>...</th>\n      <th>SDS-SDS_Total_Raw</th>\n      <th>SDS-SDS_Total_T</th>\n      <th>PreInt_EduHx-Season</th>\n      <th>PreInt_EduHx-computerinternet_hoursday</th>\n      <th>enmo_std_mean</th>\n      <th>enmo_std_std</th>\n      <th>anglez_std_mean</th>\n      <th>anglez_std_std</th>\n      <th>light_std_mean</th>\n      <th>light_std_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>Fall</td>\n      <td>5</td>\n      <td>0</td>\n      <td>Winter</td>\n      <td>51.0</td>\n      <td>Fall</td>\n      <td>16.877316</td>\n      <td>46.0</td>\n      <td>50.8</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Fall</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>Summer</td>\n      <td>9</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Fall</td>\n      <td>14.035590</td>\n      <td>48.0</td>\n      <td>46.0</td>\n      <td>...</td>\n      <td>46.0</td>\n      <td>64.0</td>\n      <td>Summer</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>Summer</td>\n      <td>10</td>\n      <td>1</td>\n      <td>Fall</td>\n      <td>71.0</td>\n      <td>Fall</td>\n      <td>16.648696</td>\n      <td>56.5</td>\n      <td>75.6</td>\n      <td>...</td>\n      <td>38.0</td>\n      <td>54.0</td>\n      <td>Summer</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>Winter</td>\n      <td>9</td>\n      <td>0</td>\n      <td>Fall</td>\n      <td>71.0</td>\n      <td>Summer</td>\n      <td>18.292347</td>\n      <td>56.0</td>\n      <td>81.6</td>\n      <td>...</td>\n      <td>31.0</td>\n      <td>45.0</td>\n      <td>Winter</td>\n      <td>0.0</td>\n      <td>-4.000377</td>\n      <td>NaN</td>\n      <td>1.989905</td>\n      <td>NaN</td>\n      <td>0.051475</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>Spring</td>\n      <td>18</td>\n      <td>1</td>\n      <td>Summer</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 65 columns</p>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"X_train = train_data2[['Basic_Demos-Age',\n                      'Basic_Demos-Sex',\n                      'CGAS-CGAS_Score',\n                      'Physical-BMI',\n                      'BIA-BIA_BMI',\n                      'Physical-Waist_Circumference',\n                      'Physical-Diastolic_BP',\n                      'Physical-HeartRate',\n                      'Physical-Systolic_BP',\n                      'Fitness_Endurance-Max_Stage',\n                      'Fitness_Endurance-Time_Mins',\n                      'Fitness_Endurance-Time_Sec',\n                      'FGC-FGC_CU_Zone',\n                      'FGC-FGC_GSND_Zone',\n                      'FGC-FGC_GSD_Zone',\n                      'FGC-FGC_PU_Zone',\n                      'FGC-FGC_SRL_Zone',\n                      'FGC-FGC_SRR_Zone',\n                      'FGC-FGC_TL_Zone',\n                      'BIA-BIA_Activity_Level_num',\n                      'BIA-BIA_BMC',\n                      'BIA-BIA_BMR',\n                      'BIA-BIA_DEE',\n                      'BIA-BIA_ECW',\n                      'BIA-BIA_FFM',\n                      'BIA-BIA_FFMI',\n                      'BIA-BIA_FMI',\n                      'BIA-BIA_Fat',\n                      'BIA-BIA_ICW',\n                      'BIA-BIA_LDM',\n                      'BIA-BIA_LST',\n                      'BIA-BIA_SMM',\n                      'BIA-BIA_TBW',\n                      'PAQ_A-PAQ_A_Total',\n                      'PAQ_C-PAQ_C_Total',\n                      'SDS-SDS_Total_T',\n                      'PreInt_EduHx-computerinternet_hoursday'\n                       ,\n                      'enmo_std_mean',\n                      'enmo_std_std',\n                      'anglez_std_mean',\n                      'anglez_std_std',\n                      'light_std_mean',\n                      'light_std_std'\n                      ]]\n\ny_train = train_data2['PCIAT-PCIAT_Total']\n\nX_test = test_data2[['Basic_Demos-Age',\n                      'Basic_Demos-Sex',\n                      'CGAS-CGAS_Score',\n                      'Physical-BMI',\n                      'BIA-BIA_BMI',\n                      'Physical-Waist_Circumference',\n                      'Physical-Diastolic_BP',\n                      'Physical-HeartRate',\n                      'Physical-Systolic_BP',\n                      'Fitness_Endurance-Max_Stage',\n                      'Fitness_Endurance-Time_Mins',\n                      'Fitness_Endurance-Time_Sec',\n                      'FGC-FGC_CU_Zone',\n                      'FGC-FGC_GSND_Zone',\n                      'FGC-FGC_GSD_Zone',\n                      'FGC-FGC_PU_Zone',\n                      'FGC-FGC_SRL_Zone',\n                      'FGC-FGC_SRR_Zone',\n                      'FGC-FGC_TL_Zone',\n                      'BIA-BIA_Activity_Level_num',\n                      'BIA-BIA_BMC',\n                      'BIA-BIA_BMR',\n                      'BIA-BIA_DEE',\n                      'BIA-BIA_ECW',\n                      'BIA-BIA_FFM',\n                      'BIA-BIA_FFMI',\n                      'BIA-BIA_FMI',\n                      'BIA-BIA_Fat',\n                      'BIA-BIA_ICW',\n                      'BIA-BIA_LDM',\n                      'BIA-BIA_LST',\n                      'BIA-BIA_SMM',\n                      'BIA-BIA_TBW',\n                      'PAQ_A-PAQ_A_Total',\n                      'PAQ_C-PAQ_C_Total',\n                      'SDS-SDS_Total_T',\n                      'PreInt_EduHx-computerinternet_hoursday'\n                      ,\n                      'enmo_std_mean',\n                      'enmo_std_std',\n                      'anglez_std_mean',\n                      'anglez_std_std',\n                      'light_std_mean',\n                      'light_std_std'\n                   ]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:46:49.690117Z","iopub.execute_input":"2025-01-26T05:46:49.690517Z","iopub.status.idle":"2025-01-26T05:46:49.702894Z","shell.execute_reply.started":"2025-01-26T05:46:49.690481Z","shell.execute_reply":"2025-01-26T05:46:49.701589Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Add this only if we are not interested in the actigraph data\n\nX_train = X_train.drop(columns=['enmo_std_mean',\n                      'enmo_std_std',\n                      'anglez_std_mean',\n                      'anglez_std_std',\n                      'light_std_mean',\n                      'light_std_std'])\n\nX_test = X_test.drop(columns=['enmo_std_mean',\n                      'enmo_std_std',\n                      'anglez_std_mean',\n                      'anglez_std_std',\n                      'light_std_mean',\n                      'light_std_std'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:46:51.529137Z","iopub.execute_input":"2025-01-26T05:46:51.529504Z","iopub.status.idle":"2025-01-26T05:46:51.538222Z","shell.execute_reply.started":"2025-01-26T05:46:51.529476Z","shell.execute_reply":"2025-01-26T05:46:51.537053Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Add calculated fields\nX_train['Physical-BMI_Calc'] = X_train.apply(lambda row: row['Physical-BMI'] if row['Physical-BMI']==row['Physical-BMI'] else row['BIA-BIA_BMI'],axis=1)\nX_train['Fitness_Endurance-Time_Sec_Calc'] = X_train.apply(lambda row: row['Fitness_Endurance-Time_Sec'] + (row['Fitness_Endurance-Time_Mins']*60), axis=1)\nX_train['PAQ_Total'] = X_train.apply(lambda row: row['PAQ_A-PAQ_A_Total'] if row['PAQ_A-PAQ_A_Total']==row['PAQ_A-PAQ_A_Total'] else row['PAQ_C-PAQ_C_Total'],axis=1)\n\n\n# Drop fields no longer needed\nX_train = X_train.drop(columns=['PAQ_A-PAQ_A_Total','PAQ_C-PAQ_C_Total',\n                     'Physical-BMI','BIA-BIA_BMI',\n                     'Fitness_Endurance-Time_Mins','Fitness_Endurance-Time_Sec'])\n\n# Remove outliers - may give warnings due to NaN value comparison\nX_train.loc[X_train['CGAS-CGAS_Score']>=100.0,'CGAS-CGAS_Score'] = np.nan\nX_train.loc[X_train['Physical-Systolic_BP']>=180.0,'Physical-Systolic_BP'] = np.nan\nX_train.loc[X_train['Physical-Diastolic_BP']>=120.0,'Physical-Diastolic_BP'] = np.nan\nX_train.loc[X_train['BIA-BIA_DEE']>=6000.0,'BIA-BIA_DEE'] = np.nan\nX_train.loc[(X_train['BIA-BIA_BMC']<=0.0) | (X_train['BIA-BIA_BMC']>=16.0),'BIA-BIA_BMC'] = np.nan\nX_train.loc[(X_train['BIA-BIA_BMR']<=0.0) | (X_train['BIA-BIA_BMR']>=2400.0),'BIA-BIA_BMR'] = np.nan\nX_train.loc[(X_train['BIA-BIA_ECW']<=0.0) | (X_train['BIA-BIA_ECW']>=60.0),'BIA-BIA_ECW'] = np.nan\nX_train.loc[(X_train['BIA-BIA_FFM']<=0.0) | (X_train['BIA-BIA_FFM']>=200.0),'BIA-BIA_FFM'] = np.nan\nX_train.loc[(X_train['BIA-BIA_FFMI']<=0.0) | (X_train['BIA-BIA_FFMI']>=25.0),'BIA-BIA_FFMI'] = np.nan\nX_train.loc[(X_train['BIA-BIA_FMI']<=0.0) | (X_train['BIA-BIA_FMI']>=25.0),'BIA-BIA_FMI'] = np.nan\nX_train.loc[(X_train['BIA-BIA_Fat']<=8.0) | (X_train['BIA-BIA_Fat']>=60.0),'BIA-BIA_Fat'] = np.nan\nX_train.loc[(X_train['BIA-BIA_ICW']<=0.0) | (X_train['BIA-BIA_ICW']>=80.0),'BIA-BIA_ICW'] = np.nan\nX_train.loc[(X_train['BIA-BIA_LDM']<=0.0) | (X_train['BIA-BIA_LDM']>=60.0),'BIA-BIA_LDM'] = np.nan\nX_train.loc[(X_train['BIA-BIA_LST']<=0.0) | (X_train['BIA-BIA_LST']>=150.0),'BIA-BIA_LST'] = np.nan\nX_train.loc[(X_train['BIA-BIA_SMM']<=0.0) | (X_train['BIA-BIA_SMM']>=100.0),'BIA-BIA_SMM'] = np.nan\nX_train.loc[(X_train['BIA-BIA_TBW']<=0.0) | (X_train['BIA-BIA_TBW']>=150.0),'BIA-BIA_TBW'] = np.nan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:46:55.092170Z","iopub.execute_input":"2025-01-26T05:46:55.092592Z","iopub.status.idle":"2025-01-26T05:46:55.266193Z","shell.execute_reply.started":"2025-01-26T05:46:55.092551Z","shell.execute_reply":"2025-01-26T05:46:55.264762Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Missing feature imputation","metadata":{}},{"cell_type":"code","source":"# Noting number of missing features\n# Different weight approaches\nfeatures_missing_labelled = X_train.loc[y_train.notna()].isnull().sum(axis=1)/X_train.shape[1]\nweights_labelled = 1 - features_missing_labelled\nweights_labelled.shape\n\nfeatures_missing_labelled2 = X_train.loc[y_train.notna()].isnull().sum(axis=1)\nweights_labelled2 = 1 * ((0.95)**features_missing_labelled2)\n\nweights_labelled3 = np.exp((-2)*features_missing_labelled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:46:59.186996Z","iopub.execute_input":"2025-01-26T05:46:59.187414Z","iopub.status.idle":"2025-01-26T05:46:59.205032Z","shell.execute_reply.started":"2025-01-26T05:46:59.187374Z","shell.execute_reply":"2025-01-26T05:46:59.203452Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"#MICE\niter_imputer = IterativeImputer(max_iter=10, random_state=42)\nX_train_fimpute = pd.DataFrame(iter_imputer.fit_transform(X_train), columns = X_train.columns)\nX_train_fimpute.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:47:00.514022Z","iopub.execute_input":"2025-01-26T05:47:00.514439Z","iopub.status.idle":"2025-01-26T05:47:01.508006Z","shell.execute_reply.started":"2025-01-26T05:47:00.514405Z","shell.execute_reply":"2025-01-26T05:47:01.506875Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"       Basic_Demos-Age  Basic_Demos-Sex  CGAS-CGAS_Score  \\\ncount      3960.000000      3960.000000      3960.000000   \nmean         10.433586         0.372727        64.818333   \nstd           3.574648         0.483591         9.805847   \nmin           5.000000         0.000000       -23.164848   \n25%           8.000000         0.000000        60.000000   \n50%          10.000000         0.000000        64.765647   \n75%          13.000000         1.000000        70.000000   \nmax          22.000000         1.000000        95.000000   \n\n       Physical-Waist_Circumference  Physical-Diastolic_BP  \\\ncount                   3960.000000            3960.000000   \nmean                      26.974426              69.403169   \nstd                        4.770198              11.011761   \nmin                        4.967968               0.000000   \n25%                       24.000000              64.000000   \n50%                       26.137767              69.000000   \n75%                       29.035486              73.000000   \nmax                       56.944995             119.000000   \n\n       Physical-HeartRate  Physical-Systolic_BP  Fitness_Endurance-Max_Stage  \\\ncount         3960.000000           3960.000000                  3960.000000   \nmean            81.519388            116.805724                     4.974030   \nstd             12.021297             14.228392                     0.935016   \nmin             27.000000              0.000000                     0.000000   \n25%             74.477905            109.000000                     4.865373   \n50%             81.485263            115.604541                     4.973003   \n75%             87.000000            122.000000                     5.081351   \nmax            138.000000            179.000000                    28.000000   \n\n       FGC-FGC_CU_Zone  FGC-FGC_GSND_Zone  ...  BIA-BIA_ICW  BIA-BIA_LDM  \\\ncount      3960.000000        3960.000000  ...  3960.000000  3960.000000   \nmean          0.480343           1.849340  ...    31.365620    18.066786   \nstd           0.389401           0.346190  ...     7.099089     5.021442   \nmin          -0.172314          -1.772461  ...    14.489000     4.635810   \n25%           0.000000           1.766934  ...    28.665877    16.394825   \n50%           0.485209           1.872826  ...    31.347850    18.058831   \n75%           1.000000           2.000000  ...    32.689980    18.232694   \nmax           1.129295           3.000000  ...    86.587576    52.527500   \n\n       BIA-BIA_LST  BIA-BIA_SMM  BIA-BIA_TBW  SDS-SDS_Total_T  \\\ncount  3960.000000  3960.000000  3960.000000      3960.000000   \nmean     63.749033    31.477562    50.138601        57.881433   \nstd      18.068723    10.250418    13.839926        10.737030   \nmin      23.620100     4.655730    20.589200        38.000000   \n25%      56.747700    27.190300    44.819225        51.000000   \n50%      63.734357    30.792761    50.135566        57.770114   \n75%      63.917531    33.007960    50.147145        60.000000   \nmax     188.145195   111.835760   146.075000       100.000000   \n\n       PreInt_EduHx-computerinternet_hoursday  Physical-BMI_Calc  \\\ncount                             3960.000000        3960.000000   \nmean                                 1.073783          19.374281   \nstd                                  1.022870           4.520383   \nmin                                 -0.278039           0.000000   \n25%                                  0.000000          16.485531   \n50%                                  1.000000          18.589876   \n75%                                  2.000000          21.015855   \nmax                                  3.019947          59.132048   \n\n       Fitness_Endurance-Time_Sec_Calc    PAQ_Total  \ncount                      3960.000000  3960.000000  \nmean                        468.721596     2.537644  \nstd                          89.856124     0.653839  \nmin                           5.000000     0.580000  \n25%                         450.588533     2.150000  \n50%                         469.207062     2.567600  \n75%                         487.416029     2.886524  \nmax                        2154.275208     4.790000  \n\n[8 rows x 34 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Basic_Demos-Age</th>\n      <th>Basic_Demos-Sex</th>\n      <th>CGAS-CGAS_Score</th>\n      <th>Physical-Waist_Circumference</th>\n      <th>Physical-Diastolic_BP</th>\n      <th>Physical-HeartRate</th>\n      <th>Physical-Systolic_BP</th>\n      <th>Fitness_Endurance-Max_Stage</th>\n      <th>FGC-FGC_CU_Zone</th>\n      <th>FGC-FGC_GSND_Zone</th>\n      <th>...</th>\n      <th>BIA-BIA_ICW</th>\n      <th>BIA-BIA_LDM</th>\n      <th>BIA-BIA_LST</th>\n      <th>BIA-BIA_SMM</th>\n      <th>BIA-BIA_TBW</th>\n      <th>SDS-SDS_Total_T</th>\n      <th>PreInt_EduHx-computerinternet_hoursday</th>\n      <th>Physical-BMI_Calc</th>\n      <th>Fitness_Endurance-Time_Sec_Calc</th>\n      <th>PAQ_Total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>...</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>10.433586</td>\n      <td>0.372727</td>\n      <td>64.818333</td>\n      <td>26.974426</td>\n      <td>69.403169</td>\n      <td>81.519388</td>\n      <td>116.805724</td>\n      <td>4.974030</td>\n      <td>0.480343</td>\n      <td>1.849340</td>\n      <td>...</td>\n      <td>31.365620</td>\n      <td>18.066786</td>\n      <td>63.749033</td>\n      <td>31.477562</td>\n      <td>50.138601</td>\n      <td>57.881433</td>\n      <td>1.073783</td>\n      <td>19.374281</td>\n      <td>468.721596</td>\n      <td>2.537644</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3.574648</td>\n      <td>0.483591</td>\n      <td>9.805847</td>\n      <td>4.770198</td>\n      <td>11.011761</td>\n      <td>12.021297</td>\n      <td>14.228392</td>\n      <td>0.935016</td>\n      <td>0.389401</td>\n      <td>0.346190</td>\n      <td>...</td>\n      <td>7.099089</td>\n      <td>5.021442</td>\n      <td>18.068723</td>\n      <td>10.250418</td>\n      <td>13.839926</td>\n      <td>10.737030</td>\n      <td>1.022870</td>\n      <td>4.520383</td>\n      <td>89.856124</td>\n      <td>0.653839</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>5.000000</td>\n      <td>0.000000</td>\n      <td>-23.164848</td>\n      <td>4.967968</td>\n      <td>0.000000</td>\n      <td>27.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.172314</td>\n      <td>-1.772461</td>\n      <td>...</td>\n      <td>14.489000</td>\n      <td>4.635810</td>\n      <td>23.620100</td>\n      <td>4.655730</td>\n      <td>20.589200</td>\n      <td>38.000000</td>\n      <td>-0.278039</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>8.000000</td>\n      <td>0.000000</td>\n      <td>60.000000</td>\n      <td>24.000000</td>\n      <td>64.000000</td>\n      <td>74.477905</td>\n      <td>109.000000</td>\n      <td>4.865373</td>\n      <td>0.000000</td>\n      <td>1.766934</td>\n      <td>...</td>\n      <td>28.665877</td>\n      <td>16.394825</td>\n      <td>56.747700</td>\n      <td>27.190300</td>\n      <td>44.819225</td>\n      <td>51.000000</td>\n      <td>0.000000</td>\n      <td>16.485531</td>\n      <td>450.588533</td>\n      <td>2.150000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>10.000000</td>\n      <td>0.000000</td>\n      <td>64.765647</td>\n      <td>26.137767</td>\n      <td>69.000000</td>\n      <td>81.485263</td>\n      <td>115.604541</td>\n      <td>4.973003</td>\n      <td>0.485209</td>\n      <td>1.872826</td>\n      <td>...</td>\n      <td>31.347850</td>\n      <td>18.058831</td>\n      <td>63.734357</td>\n      <td>30.792761</td>\n      <td>50.135566</td>\n      <td>57.770114</td>\n      <td>1.000000</td>\n      <td>18.589876</td>\n      <td>469.207062</td>\n      <td>2.567600</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>13.000000</td>\n      <td>1.000000</td>\n      <td>70.000000</td>\n      <td>29.035486</td>\n      <td>73.000000</td>\n      <td>87.000000</td>\n      <td>122.000000</td>\n      <td>5.081351</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>...</td>\n      <td>32.689980</td>\n      <td>18.232694</td>\n      <td>63.917531</td>\n      <td>33.007960</td>\n      <td>50.147145</td>\n      <td>60.000000</td>\n      <td>2.000000</td>\n      <td>21.015855</td>\n      <td>487.416029</td>\n      <td>2.886524</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>22.000000</td>\n      <td>1.000000</td>\n      <td>95.000000</td>\n      <td>56.944995</td>\n      <td>119.000000</td>\n      <td>138.000000</td>\n      <td>179.000000</td>\n      <td>28.000000</td>\n      <td>1.129295</td>\n      <td>3.000000</td>\n      <td>...</td>\n      <td>86.587576</td>\n      <td>52.527500</td>\n      <td>188.145195</td>\n      <td>111.835760</td>\n      <td>146.075000</td>\n      <td>100.000000</td>\n      <td>3.019947</td>\n      <td>59.132048</td>\n      <td>2154.275208</td>\n      <td>4.790000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 34 columns</p>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Clip imputed values to original max and min\nfor column in X_train_fimpute.columns:\n    max_val = np.max(X_train[column])\n    min_val = np.min(X_train[column])\n    X_train_fimpute.loc[X_train_fimpute[column]>max_val,column] = max_val\n    X_train_fimpute.loc[X_train_fimpute[column]<min_val, column] = min_val\n\nX_train_fimpute.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:47:02.984543Z","iopub.execute_input":"2025-01-26T05:47:02.984936Z","iopub.status.idle":"2025-01-26T05:47:03.099092Z","shell.execute_reply.started":"2025-01-26T05:47:02.984907Z","shell.execute_reply":"2025-01-26T05:47:03.097849Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"       Basic_Demos-Age  Basic_Demos-Sex  CGAS-CGAS_Score  \\\ncount      3960.000000      3960.000000      3960.000000   \nmean         10.433586         0.372727        64.830496   \nstd           3.574648         0.483591         9.726236   \nmin           5.000000         0.000000        25.000000   \n25%           8.000000         0.000000        60.000000   \n50%          10.000000         0.000000        64.765647   \n75%          13.000000         1.000000        70.000000   \nmax          22.000000         1.000000        95.000000   \n\n       Physical-Waist_Circumference  Physical-Diastolic_BP  \\\ncount                   3960.000000            3960.000000   \nmean                      26.998653              69.403169   \nstd                        4.692416              11.011761   \nmin                       18.000000               0.000000   \n25%                       24.000000              64.000000   \n50%                       26.137767              69.000000   \n75%                       29.035486              73.000000   \nmax                       50.000000             119.000000   \n\n       Physical-HeartRate  Physical-Systolic_BP  Fitness_Endurance-Max_Stage  \\\ncount         3960.000000           3960.000000                  3960.000000   \nmean            81.519388            116.805724                     4.974030   \nstd             12.021297             14.228392                     0.935016   \nmin             27.000000              0.000000                     0.000000   \n25%             74.477905            109.000000                     4.865373   \n50%             81.485263            115.604541                     4.973003   \n75%             87.000000            122.000000                     5.081351   \nmax            138.000000            179.000000                    28.000000   \n\n       FGC-FGC_CU_Zone  FGC-FGC_GSND_Zone  ...  BIA-BIA_ICW  BIA-BIA_LDM  \\\ncount      3960.000000        3960.000000  ...  3960.000000  3960.000000   \nmean          0.480419           1.850041  ...    31.363446    18.066786   \nstd           0.389164           0.341634  ...     7.083396     5.021442   \nmin           0.000000           1.000000  ...    14.489000     4.635810   \n25%           0.000000           1.766934  ...    28.665877    16.394825   \n50%           0.485209           1.872826  ...    31.347850    18.058831   \n75%           1.000000           2.000000  ...    32.689980    18.232694   \nmax           1.000000           3.000000  ...    79.473800    52.527500   \n\n       BIA-BIA_LST  BIA-BIA_SMM  BIA-BIA_TBW  SDS-SDS_Total_T  \\\ncount  3960.000000  3960.000000  3960.000000      3960.000000   \nmean     63.672368    31.469455    50.138601        57.881433   \nstd      17.644465    10.193244    13.839926        10.737030   \nmin      23.620100     4.655730    20.589200        38.000000   \n25%      56.747700    27.190300    44.819225        51.000000   \n50%      63.734357    30.792761    50.135566        57.770114   \n75%      63.917531    33.007960    50.147145        60.000000   \nmax     149.830000    97.923100   146.075000       100.000000   \n\n       PreInt_EduHx-computerinternet_hoursday  Physical-BMI_Calc  \\\ncount                             3960.000000        3960.000000   \nmean                                 1.073848          19.374281   \nstd                                  1.022777           4.520383   \nmin                                  0.000000           0.000000   \n25%                                  0.000000          16.485531   \n50%                                  1.000000          18.589876   \n75%                                  2.000000          21.015855   \nmax                                  3.000000          59.132048   \n\n       Fitness_Endurance-Time_Sec_Calc    PAQ_Total  \ncount                      3960.000000  3960.000000  \nmean                        468.480617     2.537644  \nstd                          86.553527     0.653839  \nmin                           5.000000     0.580000  \n25%                         450.588533     2.150000  \n50%                         469.207062     2.567600  \n75%                         487.416029     2.886524  \nmax                        1200.000000     4.790000  \n\n[8 rows x 34 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Basic_Demos-Age</th>\n      <th>Basic_Demos-Sex</th>\n      <th>CGAS-CGAS_Score</th>\n      <th>Physical-Waist_Circumference</th>\n      <th>Physical-Diastolic_BP</th>\n      <th>Physical-HeartRate</th>\n      <th>Physical-Systolic_BP</th>\n      <th>Fitness_Endurance-Max_Stage</th>\n      <th>FGC-FGC_CU_Zone</th>\n      <th>FGC-FGC_GSND_Zone</th>\n      <th>...</th>\n      <th>BIA-BIA_ICW</th>\n      <th>BIA-BIA_LDM</th>\n      <th>BIA-BIA_LST</th>\n      <th>BIA-BIA_SMM</th>\n      <th>BIA-BIA_TBW</th>\n      <th>SDS-SDS_Total_T</th>\n      <th>PreInt_EduHx-computerinternet_hoursday</th>\n      <th>Physical-BMI_Calc</th>\n      <th>Fitness_Endurance-Time_Sec_Calc</th>\n      <th>PAQ_Total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>...</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>10.433586</td>\n      <td>0.372727</td>\n      <td>64.830496</td>\n      <td>26.998653</td>\n      <td>69.403169</td>\n      <td>81.519388</td>\n      <td>116.805724</td>\n      <td>4.974030</td>\n      <td>0.480419</td>\n      <td>1.850041</td>\n      <td>...</td>\n      <td>31.363446</td>\n      <td>18.066786</td>\n      <td>63.672368</td>\n      <td>31.469455</td>\n      <td>50.138601</td>\n      <td>57.881433</td>\n      <td>1.073848</td>\n      <td>19.374281</td>\n      <td>468.480617</td>\n      <td>2.537644</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3.574648</td>\n      <td>0.483591</td>\n      <td>9.726236</td>\n      <td>4.692416</td>\n      <td>11.011761</td>\n      <td>12.021297</td>\n      <td>14.228392</td>\n      <td>0.935016</td>\n      <td>0.389164</td>\n      <td>0.341634</td>\n      <td>...</td>\n      <td>7.083396</td>\n      <td>5.021442</td>\n      <td>17.644465</td>\n      <td>10.193244</td>\n      <td>13.839926</td>\n      <td>10.737030</td>\n      <td>1.022777</td>\n      <td>4.520383</td>\n      <td>86.553527</td>\n      <td>0.653839</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>5.000000</td>\n      <td>0.000000</td>\n      <td>25.000000</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n      <td>27.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>14.489000</td>\n      <td>4.635810</td>\n      <td>23.620100</td>\n      <td>4.655730</td>\n      <td>20.589200</td>\n      <td>38.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>8.000000</td>\n      <td>0.000000</td>\n      <td>60.000000</td>\n      <td>24.000000</td>\n      <td>64.000000</td>\n      <td>74.477905</td>\n      <td>109.000000</td>\n      <td>4.865373</td>\n      <td>0.000000</td>\n      <td>1.766934</td>\n      <td>...</td>\n      <td>28.665877</td>\n      <td>16.394825</td>\n      <td>56.747700</td>\n      <td>27.190300</td>\n      <td>44.819225</td>\n      <td>51.000000</td>\n      <td>0.000000</td>\n      <td>16.485531</td>\n      <td>450.588533</td>\n      <td>2.150000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>10.000000</td>\n      <td>0.000000</td>\n      <td>64.765647</td>\n      <td>26.137767</td>\n      <td>69.000000</td>\n      <td>81.485263</td>\n      <td>115.604541</td>\n      <td>4.973003</td>\n      <td>0.485209</td>\n      <td>1.872826</td>\n      <td>...</td>\n      <td>31.347850</td>\n      <td>18.058831</td>\n      <td>63.734357</td>\n      <td>30.792761</td>\n      <td>50.135566</td>\n      <td>57.770114</td>\n      <td>1.000000</td>\n      <td>18.589876</td>\n      <td>469.207062</td>\n      <td>2.567600</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>13.000000</td>\n      <td>1.000000</td>\n      <td>70.000000</td>\n      <td>29.035486</td>\n      <td>73.000000</td>\n      <td>87.000000</td>\n      <td>122.000000</td>\n      <td>5.081351</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>...</td>\n      <td>32.689980</td>\n      <td>18.232694</td>\n      <td>63.917531</td>\n      <td>33.007960</td>\n      <td>50.147145</td>\n      <td>60.000000</td>\n      <td>2.000000</td>\n      <td>21.015855</td>\n      <td>487.416029</td>\n      <td>2.886524</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>22.000000</td>\n      <td>1.000000</td>\n      <td>95.000000</td>\n      <td>50.000000</td>\n      <td>119.000000</td>\n      <td>138.000000</td>\n      <td>179.000000</td>\n      <td>28.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>...</td>\n      <td>79.473800</td>\n      <td>52.527500</td>\n      <td>149.830000</td>\n      <td>97.923100</td>\n      <td>146.075000</td>\n      <td>100.000000</td>\n      <td>3.000000</td>\n      <td>59.132048</td>\n      <td>1200.000000</td>\n      <td>4.790000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 34 columns</p>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# standardise data\nscaler = StandardScaler()                  \n\nX_train_fimpute[X_train_fimpute.columns] = scaler.fit_transform(X_train_fimpute[X_train_fimpute.columns])\nX_train_fimpute.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:47:04.541269Z","iopub.execute_input":"2025-01-26T05:47:04.541715Z","iopub.status.idle":"2025-01-26T05:47:04.655751Z","shell.execute_reply.started":"2025-01-26T05:47:04.541681Z","shell.execute_reply":"2025-01-26T05:47:04.654538Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"       Basic_Demos-Age  Basic_Demos-Sex  CGAS-CGAS_Score  \\\ncount     3.960000e+03     3.960000e+03     3.960000e+03   \nmean     -1.928872e-16    -5.831474e-17     8.343494e-16   \nstd       1.000126e+00     1.000126e+00     1.000126e+00   \nmin      -1.520226e+00    -7.708456e-01    -4.095678e+00   \n25%      -6.808763e-01    -7.708456e-01    -4.967087e-01   \n50%      -1.213100e-01    -7.708456e-01    -6.668295e-03   \n75%       7.180394e-01     1.297277e+00     5.315681e-01   \nmax       3.236088e+00     1.297277e+00     3.102260e+00   \n\n       Physical-Waist_Circumference  Physical-Diastolic_BP  \\\ncount                  3.960000e+03           3.960000e+03   \nmean                   5.921189e-17           2.866394e-16   \nstd                    1.000126e+00           1.000126e+00   \nmin                   -1.917944e+00          -6.303437e+00   \n25%                   -6.391231e-01          -4.907346e-01   \n50%                   -1.834866e-01          -3.661725e-02   \n75%                    4.341239e-01           3.266766e-01   \nmax                    4.902432e+00           4.504556e+00   \n\n       Physical-HeartRate  Physical-Systolic_BP  Fitness_Endurance-Max_Stage  \\\ncount        3.960000e+03          3.960000e+03                 3.960000e+03   \nmean         4.893953e-16         -3.545985e-16                 6.948426e-16   \nstd          1.000126e+00          1.000126e+00                 1.000126e+00   \nmin         -4.535806e+00         -8.210378e+00                -5.320400e+00   \n25%         -5.858247e-01         -5.486713e-01                -1.162227e-01   \n50%         -2.839099e-03         -8.443218e-02                -1.098129e-03   \n75%          4.559661e-01          3.651103e-01                 1.147947e-01   \nmax          4.698973e+00          4.371691e+00                 2.462940e+01   \n\n       FGC-FGC_CU_Zone  FGC-FGC_GSND_Zone  ...   BIA-BIA_ICW   BIA-BIA_LDM  \\\ncount     3.960000e+03       3.960000e+03  ...  3.960000e+03  3.960000e+03   \nmean     -1.282924e-16       1.058637e-16  ... -3.184882e-16 -1.004808e-16   \nstd       1.000126e+00       1.000126e+00  ...  1.000126e+00  1.000126e+00   \nmin      -1.234646e+00      -2.488480e+00  ... -2.382555e+00 -2.675063e+00   \n25%      -1.234646e+00      -2.432958e-01  ... -3.808780e-01 -3.330064e-01   \n50%       1.230976e-02       6.670247e-02  ... -2.202046e-03 -1.584385e-03   \n75%       1.335289e+00       4.390017e-01  ...  1.872974e-01  3.304415e-02   \nmax       1.335289e+00       3.366483e+00  ...  6.792848e+00  6.863580e+00   \n\n        BIA-BIA_LST   BIA-BIA_SMM   BIA-BIA_TBW  SDS-SDS_Total_T  \\\ncount  3.960000e+03  3.960000e+03  3.960000e+03     3.960000e+03   \nmean   1.040694e-16 -4.135861e-16  1.964758e-16    -1.821214e-16   \nstd    1.000126e+00  1.000126e+00  1.000126e+00     1.000126e+00   \nmin   -2.270249e+00 -2.630871e+00 -2.135353e+00    -1.851904e+00   \n25%   -3.925051e-01 -4.198561e-01 -3.843986e-01    -6.409875e-01   \n50%    3.513687e-03 -6.639494e-02 -2.193295e-04    -1.036914e-02   \n75%    1.389639e-02  1.509529e-01  6.173593e-04     1.973390e-01   \nmax    4.883600e+00  6.520205e+00  6.932733e+00     3.923235e+00   \n\n       PreInt_EduHx-computerinternet_hoursday  Physical-BMI_Calc  \\\ncount                            3.960000e+03       3.960000e+03   \nmean                            -5.741759e-17       9.796877e-16   \nstd                              1.000126e+00       1.000126e+00   \nmin                             -1.050066e+00      -4.286523e+00   \n25%                             -1.050066e+00      -6.391307e-01   \n50%                             -7.221268e-02      -1.735482e-01   \n75%                              9.056408e-01       3.631950e-01   \nmax                              1.883494e+00       8.796331e+00   \n\n       Fitness_Endurance-Time_Sec_Calc     PAQ_Total  \ncount                     3.960000e+03  3.960000e+03  \nmean                      4.831152e-16 -1.578984e-16  \nstd                       1.000126e+00  1.000126e+00  \nmin                      -5.355520e+00 -2.994455e+00  \n25%                      -2.067431e-01 -5.929487e-01  \n50%                       8.394070e-03  4.582149e-02  \n75%                       2.187987e-01  5.336544e-01  \nmax                       8.452709e+00  3.445252e+00  \n\n[8 rows x 34 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Basic_Demos-Age</th>\n      <th>Basic_Demos-Sex</th>\n      <th>CGAS-CGAS_Score</th>\n      <th>Physical-Waist_Circumference</th>\n      <th>Physical-Diastolic_BP</th>\n      <th>Physical-HeartRate</th>\n      <th>Physical-Systolic_BP</th>\n      <th>Fitness_Endurance-Max_Stage</th>\n      <th>FGC-FGC_CU_Zone</th>\n      <th>FGC-FGC_GSND_Zone</th>\n      <th>...</th>\n      <th>BIA-BIA_ICW</th>\n      <th>BIA-BIA_LDM</th>\n      <th>BIA-BIA_LST</th>\n      <th>BIA-BIA_SMM</th>\n      <th>BIA-BIA_TBW</th>\n      <th>SDS-SDS_Total_T</th>\n      <th>PreInt_EduHx-computerinternet_hoursday</th>\n      <th>Physical-BMI_Calc</th>\n      <th>Fitness_Endurance-Time_Sec_Calc</th>\n      <th>PAQ_Total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>...</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-1.928872e-16</td>\n      <td>-5.831474e-17</td>\n      <td>8.343494e-16</td>\n      <td>5.921189e-17</td>\n      <td>2.866394e-16</td>\n      <td>4.893953e-16</td>\n      <td>-3.545985e-16</td>\n      <td>6.948426e-16</td>\n      <td>-1.282924e-16</td>\n      <td>1.058637e-16</td>\n      <td>...</td>\n      <td>-3.184882e-16</td>\n      <td>-1.004808e-16</td>\n      <td>1.040694e-16</td>\n      <td>-4.135861e-16</td>\n      <td>1.964758e-16</td>\n      <td>-1.821214e-16</td>\n      <td>-5.741759e-17</td>\n      <td>9.796877e-16</td>\n      <td>4.831152e-16</td>\n      <td>-1.578984e-16</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>...</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-1.520226e+00</td>\n      <td>-7.708456e-01</td>\n      <td>-4.095678e+00</td>\n      <td>-1.917944e+00</td>\n      <td>-6.303437e+00</td>\n      <td>-4.535806e+00</td>\n      <td>-8.210378e+00</td>\n      <td>-5.320400e+00</td>\n      <td>-1.234646e+00</td>\n      <td>-2.488480e+00</td>\n      <td>...</td>\n      <td>-2.382555e+00</td>\n      <td>-2.675063e+00</td>\n      <td>-2.270249e+00</td>\n      <td>-2.630871e+00</td>\n      <td>-2.135353e+00</td>\n      <td>-1.851904e+00</td>\n      <td>-1.050066e+00</td>\n      <td>-4.286523e+00</td>\n      <td>-5.355520e+00</td>\n      <td>-2.994455e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-6.808763e-01</td>\n      <td>-7.708456e-01</td>\n      <td>-4.967087e-01</td>\n      <td>-6.391231e-01</td>\n      <td>-4.907346e-01</td>\n      <td>-5.858247e-01</td>\n      <td>-5.486713e-01</td>\n      <td>-1.162227e-01</td>\n      <td>-1.234646e+00</td>\n      <td>-2.432958e-01</td>\n      <td>...</td>\n      <td>-3.808780e-01</td>\n      <td>-3.330064e-01</td>\n      <td>-3.925051e-01</td>\n      <td>-4.198561e-01</td>\n      <td>-3.843986e-01</td>\n      <td>-6.409875e-01</td>\n      <td>-1.050066e+00</td>\n      <td>-6.391307e-01</td>\n      <td>-2.067431e-01</td>\n      <td>-5.929487e-01</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-1.213100e-01</td>\n      <td>-7.708456e-01</td>\n      <td>-6.668295e-03</td>\n      <td>-1.834866e-01</td>\n      <td>-3.661725e-02</td>\n      <td>-2.839099e-03</td>\n      <td>-8.443218e-02</td>\n      <td>-1.098129e-03</td>\n      <td>1.230976e-02</td>\n      <td>6.670247e-02</td>\n      <td>...</td>\n      <td>-2.202046e-03</td>\n      <td>-1.584385e-03</td>\n      <td>3.513687e-03</td>\n      <td>-6.639494e-02</td>\n      <td>-2.193295e-04</td>\n      <td>-1.036914e-02</td>\n      <td>-7.221268e-02</td>\n      <td>-1.735482e-01</td>\n      <td>8.394070e-03</td>\n      <td>4.582149e-02</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>7.180394e-01</td>\n      <td>1.297277e+00</td>\n      <td>5.315681e-01</td>\n      <td>4.341239e-01</td>\n      <td>3.266766e-01</td>\n      <td>4.559661e-01</td>\n      <td>3.651103e-01</td>\n      <td>1.147947e-01</td>\n      <td>1.335289e+00</td>\n      <td>4.390017e-01</td>\n      <td>...</td>\n      <td>1.872974e-01</td>\n      <td>3.304415e-02</td>\n      <td>1.389639e-02</td>\n      <td>1.509529e-01</td>\n      <td>6.173593e-04</td>\n      <td>1.973390e-01</td>\n      <td>9.056408e-01</td>\n      <td>3.631950e-01</td>\n      <td>2.187987e-01</td>\n      <td>5.336544e-01</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3.236088e+00</td>\n      <td>1.297277e+00</td>\n      <td>3.102260e+00</td>\n      <td>4.902432e+00</td>\n      <td>4.504556e+00</td>\n      <td>4.698973e+00</td>\n      <td>4.371691e+00</td>\n      <td>2.462940e+01</td>\n      <td>1.335289e+00</td>\n      <td>3.366483e+00</td>\n      <td>...</td>\n      <td>6.792848e+00</td>\n      <td>6.863580e+00</td>\n      <td>4.883600e+00</td>\n      <td>6.520205e+00</td>\n      <td>6.932733e+00</td>\n      <td>3.923235e+00</td>\n      <td>1.883494e+00</td>\n      <td>8.796331e+00</td>\n      <td>8.452709e+00</td>\n      <td>3.445252e+00</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 34 columns</p>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Repeat the above for X_test\n\n# Add calculated fields\nX_test['Physical-BMI_Calc'] = X_test.apply(lambda row: row['Physical-BMI'] if row['Physical-BMI']==row['Physical-BMI'] else row['BIA-BIA_BMI'],axis=1)\nX_test['Fitness_Endurance-Time_Sec_Calc'] = X_test.apply(lambda row: row['Fitness_Endurance-Time_Sec'] + (row['Fitness_Endurance-Time_Mins']*60), axis=1)\nX_test['PAQ_Total'] = X_test.apply(lambda row: row['PAQ_A-PAQ_A_Total'] if row['PAQ_A-PAQ_A_Total']==row['PAQ_A-PAQ_A_Total'] else row['PAQ_C-PAQ_C_Total'],axis=1)\n\n# Drop fields no longer needed\nX_test = X_test.drop(columns=['PAQ_A-PAQ_A_Total','PAQ_C-PAQ_C_Total',\n                     'Physical-BMI','BIA-BIA_BMI',\n                     'Fitness_Endurance-Time_Mins','Fitness_Endurance-Time_Sec'])\n\n# Remove outliers\nX_test.loc[X_test['CGAS-CGAS_Score']>=100.0,'CGAS-CGAS_Score'] = np.nan\nX_test.loc[X_test['Physical-Systolic_BP']>=180.0,'Physical-Systolic_BP'] = np.nan\nX_test.loc[X_test['Physical-Diastolic_BP']>=120.0,'Physical-Diastolic_BP'] = np.nan\nX_test.loc[X_test['BIA-BIA_DEE']>=6000.0,'BIA-BIA_DEE'] = np.nan\nX_test.loc[(X_test['BIA-BIA_BMC']<=0.0) | (X_test['BIA-BIA_BMC']>=16.0),'BIA-BIA_BMC'] = np.nan\nX_test.loc[(X_test['BIA-BIA_BMR']<=0.0) | (X_test['BIA-BIA_BMR']>=2400.0),'BIA-BIA_BMR'] = np.nan\nX_test.loc[(X_test['BIA-BIA_ECW']<=0.0) | (X_test['BIA-BIA_ECW']>=60.0),'BIA-BIA_ECW'] = np.nan\nX_test.loc[(X_test['BIA-BIA_FFM']<=0.0) | (X_test['BIA-BIA_FFM']>=200.0),'BIA-BIA_FFM'] = np.nan\nX_test.loc[(X_test['BIA-BIA_FFMI']<=0.0) | (X_test['BIA-BIA_FFMI']>=25.0),'BIA-BIA_FFMI'] = np.nan\nX_test.loc[(X_test['BIA-BIA_FMI']<=0.0) | (X_test['BIA-BIA_FMI']>=25.0),'BIA-BIA_FMI'] = np.nan\nX_test.loc[(X_test['BIA-BIA_Fat']<=8.0) | (X_test['BIA-BIA_Fat']>=60.0),'BIA-BIA_Fat'] = np.nan\nX_test.loc[(X_test['BIA-BIA_ICW']<=0.0) | (X_test['BIA-BIA_ICW']>=80.0),'BIA-BIA_ICW'] = np.nan\nX_test.loc[(X_test['BIA-BIA_LDM']<=0.0) | (X_test['BIA-BIA_LDM']>=60.0),'BIA-BIA_LDM'] = np.nan\nX_test.loc[(X_test['BIA-BIA_LST']<=0.0) | (X_test['BIA-BIA_LST']>=150.0),'BIA-BIA_LST'] = np.nan\nX_test.loc[(X_test['BIA-BIA_SMM']<=0.0) | (X_test['BIA-BIA_SMM']>=100.0),'BIA-BIA_SMM'] = np.nan\nX_test.loc[(X_test['BIA-BIA_TBW']<=0.0) | (X_test['BIA-BIA_TBW']>=150.0),'BIA-BIA_TBW'] = np.nan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:47:06.106854Z","iopub.execute_input":"2025-01-26T05:47:06.107205Z","iopub.status.idle":"2025-01-26T05:47:06.142513Z","shell.execute_reply.started":"2025-01-26T05:47:06.107178Z","shell.execute_reply":"2025-01-26T05:47:06.141360Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Imputation\nX_test_fimpute = pd.DataFrame(iter_imputer.transform(X_test), columns = X_test.columns)\n\n# Clipping\nfor column in X_test_fimpute.columns:\n    max_val = np.max(X_train[column])\n    min_val = np.min(X_train[column])\n    X_test_fimpute.loc[X_test_fimpute[column]>max_val,column] = max_val\n    X_test_fimpute.loc[X_test_fimpute[column]<min_val, column] = min_val\n\n# Scaling\nX_test_fimpute[X_test_fimpute.columns] = scaler.transform(X_test_fimpute[X_test_fimpute.columns])\n\nX_test_fimpute.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:47:09.346040Z","iopub.execute_input":"2025-01-26T05:47:09.346403Z","iopub.status.idle":"2025-01-26T05:47:09.473294Z","shell.execute_reply.started":"2025-01-26T05:47:09.346376Z","shell.execute_reply":"2025-01-26T05:47:09.472071Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"       Basic_Demos-Age  Basic_Demos-Sex  CGAS-CGAS_Score  \\\ncount        20.000000        20.000000        20.000000   \nmean          0.088527         0.056403        -0.045878   \nstd           1.042416         1.039489         0.801382   \nmin          -1.520226        -0.770846        -1.524985   \n25%          -0.401093        -0.770846        -0.398350   \n50%          -0.121310        -0.770846         0.053470   \n75%           0.508202         1.297277         0.442934   \nmax           2.396738         1.297277         1.559845   \n\n       Physical-Waist_Circumference  Physical-Diastolic_BP  \\\ncount                     20.000000              20.000000   \nmean                       0.062078              -0.054532   \nstd                        0.834814               0.728397   \nmin                       -1.917944              -1.126499   \n25%                       -0.639123              -0.626970   \n50%                        0.186887               0.013374   \n75%                        0.601291               0.157655   \nmax                        1.827302               2.099974   \n\n       Physical-HeartRate  Physical-Systolic_BP  Fitness_Endurance-Max_Stage  \\\ncount           20.000000             20.000000                    20.000000   \nmean            -0.024661              0.096746                    -0.099940   \nstd              0.630397              1.095310                     0.609633   \nmin             -0.958369             -1.532744                    -1.982619   \n25%             -0.479991             -0.323267                    -0.117459   \n50%             -0.096654              0.013656                    -0.010525   \n75%              0.349697              0.359988                     0.109218   \nmax              1.287928              3.247037                     1.097415   \n\n       FGC-FGC_CU_Zone  FGC-FGC_GSND_Zone  ...  BIA-BIA_ICW  BIA-BIA_LDM  \\\ncount        20.000000          20.000000  ...    20.000000    20.000000   \nmean         -0.092855          -0.200220  ...    -0.201746    -0.212473   \nstd           1.064675           0.889571  ...     0.502229     0.550313   \nmin          -1.234646          -2.488480  ...    -1.458277    -1.826683   \n25%          -1.234646          -0.271331  ...    -0.353243    -0.152373   \n50%          -0.226210          -0.084494  ...    -0.172488    -0.012274   \n75%           1.335289           0.310673  ...     0.143634     0.007697   \nmax           1.335289           1.002942  ...     0.662725     0.564693   \n\n       BIA-BIA_LST  BIA-BIA_SMM  BIA-BIA_TBW  SDS-SDS_Total_T  \\\ncount    20.000000    20.000000    20.000000        20.000000   \nmean     -0.149911    -0.186412    -0.143597        -0.248715   \nstd       0.534173     0.543306     0.553989         0.532311   \nmin      -1.403148    -1.575630    -1.668095        -1.665609   \n25%      -0.072266    -0.321185    -0.053025        -0.384832   \n50%       0.001130    -0.057763    -0.000024        -0.036684   \n75%       0.007959     0.164499     0.000693         0.043558   \nmax       0.908379     0.466421     0.938555         0.569929   \n\n       PreInt_EduHx-computerinternet_hoursday  Physical-BMI_Calc  \\\ncount                               20.000000          20.000000   \nmean                                 0.397885           0.159731   \nstd                                  1.029493           0.885501   \nmin                                 -1.050066          -1.181175   \n25%                                 -0.316676          -0.484882   \n50%                                  0.905641           0.059206   \n75%                                  0.934083           0.438107   \nmax                                  1.883494           2.371861   \n\n       Fitness_Endurance-Time_Sec_Calc  PAQ_Total  \ncount                        20.000000  20.000000  \nmean                         -0.138408  -0.321770  \nstd                           0.532544   1.215554  \nmin                          -1.577034  -2.290829  \n25%                          -0.194965  -0.793832  \n50%                          -0.060168  -0.247980  \n75%                           0.054260   0.197347  \nmax                           1.253942   2.405110  \n\n[8 rows x 34 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Basic_Demos-Age</th>\n      <th>Basic_Demos-Sex</th>\n      <th>CGAS-CGAS_Score</th>\n      <th>Physical-Waist_Circumference</th>\n      <th>Physical-Diastolic_BP</th>\n      <th>Physical-HeartRate</th>\n      <th>Physical-Systolic_BP</th>\n      <th>Fitness_Endurance-Max_Stage</th>\n      <th>FGC-FGC_CU_Zone</th>\n      <th>FGC-FGC_GSND_Zone</th>\n      <th>...</th>\n      <th>BIA-BIA_ICW</th>\n      <th>BIA-BIA_LDM</th>\n      <th>BIA-BIA_LST</th>\n      <th>BIA-BIA_SMM</th>\n      <th>BIA-BIA_TBW</th>\n      <th>SDS-SDS_Total_T</th>\n      <th>PreInt_EduHx-computerinternet_hoursday</th>\n      <th>Physical-BMI_Calc</th>\n      <th>Fitness_Endurance-Time_Sec_Calc</th>\n      <th>PAQ_Total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>...</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.088527</td>\n      <td>0.056403</td>\n      <td>-0.045878</td>\n      <td>0.062078</td>\n      <td>-0.054532</td>\n      <td>-0.024661</td>\n      <td>0.096746</td>\n      <td>-0.099940</td>\n      <td>-0.092855</td>\n      <td>-0.200220</td>\n      <td>...</td>\n      <td>-0.201746</td>\n      <td>-0.212473</td>\n      <td>-0.149911</td>\n      <td>-0.186412</td>\n      <td>-0.143597</td>\n      <td>-0.248715</td>\n      <td>0.397885</td>\n      <td>0.159731</td>\n      <td>-0.138408</td>\n      <td>-0.321770</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.042416</td>\n      <td>1.039489</td>\n      <td>0.801382</td>\n      <td>0.834814</td>\n      <td>0.728397</td>\n      <td>0.630397</td>\n      <td>1.095310</td>\n      <td>0.609633</td>\n      <td>1.064675</td>\n      <td>0.889571</td>\n      <td>...</td>\n      <td>0.502229</td>\n      <td>0.550313</td>\n      <td>0.534173</td>\n      <td>0.543306</td>\n      <td>0.553989</td>\n      <td>0.532311</td>\n      <td>1.029493</td>\n      <td>0.885501</td>\n      <td>0.532544</td>\n      <td>1.215554</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-1.520226</td>\n      <td>-0.770846</td>\n      <td>-1.524985</td>\n      <td>-1.917944</td>\n      <td>-1.126499</td>\n      <td>-0.958369</td>\n      <td>-1.532744</td>\n      <td>-1.982619</td>\n      <td>-1.234646</td>\n      <td>-2.488480</td>\n      <td>...</td>\n      <td>-1.458277</td>\n      <td>-1.826683</td>\n      <td>-1.403148</td>\n      <td>-1.575630</td>\n      <td>-1.668095</td>\n      <td>-1.665609</td>\n      <td>-1.050066</td>\n      <td>-1.181175</td>\n      <td>-1.577034</td>\n      <td>-2.290829</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.401093</td>\n      <td>-0.770846</td>\n      <td>-0.398350</td>\n      <td>-0.639123</td>\n      <td>-0.626970</td>\n      <td>-0.479991</td>\n      <td>-0.323267</td>\n      <td>-0.117459</td>\n      <td>-1.234646</td>\n      <td>-0.271331</td>\n      <td>...</td>\n      <td>-0.353243</td>\n      <td>-0.152373</td>\n      <td>-0.072266</td>\n      <td>-0.321185</td>\n      <td>-0.053025</td>\n      <td>-0.384832</td>\n      <td>-0.316676</td>\n      <td>-0.484882</td>\n      <td>-0.194965</td>\n      <td>-0.793832</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-0.121310</td>\n      <td>-0.770846</td>\n      <td>0.053470</td>\n      <td>0.186887</td>\n      <td>0.013374</td>\n      <td>-0.096654</td>\n      <td>0.013656</td>\n      <td>-0.010525</td>\n      <td>-0.226210</td>\n      <td>-0.084494</td>\n      <td>...</td>\n      <td>-0.172488</td>\n      <td>-0.012274</td>\n      <td>0.001130</td>\n      <td>-0.057763</td>\n      <td>-0.000024</td>\n      <td>-0.036684</td>\n      <td>0.905641</td>\n      <td>0.059206</td>\n      <td>-0.060168</td>\n      <td>-0.247980</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.508202</td>\n      <td>1.297277</td>\n      <td>0.442934</td>\n      <td>0.601291</td>\n      <td>0.157655</td>\n      <td>0.349697</td>\n      <td>0.359988</td>\n      <td>0.109218</td>\n      <td>1.335289</td>\n      <td>0.310673</td>\n      <td>...</td>\n      <td>0.143634</td>\n      <td>0.007697</td>\n      <td>0.007959</td>\n      <td>0.164499</td>\n      <td>0.000693</td>\n      <td>0.043558</td>\n      <td>0.934083</td>\n      <td>0.438107</td>\n      <td>0.054260</td>\n      <td>0.197347</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.396738</td>\n      <td>1.297277</td>\n      <td>1.559845</td>\n      <td>1.827302</td>\n      <td>2.099974</td>\n      <td>1.287928</td>\n      <td>3.247037</td>\n      <td>1.097415</td>\n      <td>1.335289</td>\n      <td>1.002942</td>\n      <td>...</td>\n      <td>0.662725</td>\n      <td>0.564693</td>\n      <td>0.908379</td>\n      <td>0.466421</td>\n      <td>0.938555</td>\n      <td>0.569929</td>\n      <td>1.883494</td>\n      <td>2.371861</td>\n      <td>1.253942</td>\n      <td>2.405110</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 34 columns</p>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"# Prepare datasets and functions for models\n* Labelled data set\n* Augmented data sets - X, y, weights\n* PCA\n* sii target\n* QWK custom metrics\n* QWK custom loss/objective functions","metadata":{}},{"cell_type":"markdown","source":"## Labelled data set","metadata":{}},{"cell_type":"code","source":"X_train_labelled = X_train_fimpute.loc[y_train.notna()]\ny_train_labelled = y_train[y_train.notna()]\nprint(\"Size of labelled train data set is: \", (X_train_labelled.shape, y_train_labelled.shape))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:47:28.610918Z","iopub.execute_input":"2025-01-26T05:47:28.611262Z","iopub.status.idle":"2025-01-26T05:47:28.622722Z","shell.execute_reply.started":"2025-01-26T05:47:28.611236Z","shell.execute_reply":"2025-01-26T05:47:28.621538Z"}},"outputs":[{"name":"stdout","text":"Size of labelled train data set is:  ((2736, 34), (2736,))\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Since we are not using the unlabelled data in this notebook, we can drop the indices\n# This will help us prevent any accidents when doing cross-validation when we split by index, don't have to think about loc vs iloc etc.\n\nX_train_labelled = X_train_labelled.reset_index(drop=True)\ny_train_labelled = y_train_labelled.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:48:06.882841Z","iopub.execute_input":"2025-01-26T05:48:06.883886Z","iopub.status.idle":"2025-01-26T05:48:06.894502Z","shell.execute_reply.started":"2025-01-26T05:48:06.883826Z","shell.execute_reply":"2025-01-26T05:48:06.893235Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"weights_labelled = weights_labelled.reset_index(drop=True)\nweights_labelled2 = weights_labelled2.reset_index(drop=True)\nweights_labelled3 = weights_labelled3.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:48:07.785412Z","iopub.execute_input":"2025-01-26T05:48:07.785810Z","iopub.status.idle":"2025-01-26T05:48:07.791203Z","shell.execute_reply.started":"2025-01-26T05:48:07.785769Z","shell.execute_reply":"2025-01-26T05:48:07.789981Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Augmented data","metadata":{}},{"cell_type":"code","source":"# X datasets - X_train_labelled, X_train_labelled_aug1a, X_train_labelled_aug1b, X_train_labelled_aug2a, X_train_labelled_aug2b\n# aug1/aug2 - augment once or twice\n# a/b - 0.1 noise multiplier, 0.15 noise multiplier\n\n# Augmented data\n\n# Get standard deviations of each column in X and y\nstd_X = np.std(X_train_labelled, axis=0)\nstd_y = np.std(y_train_labelled, axis=0)\n\n# Create augmented datasets\n\nX_noise_multiplier=0.1\ny_noise_multiplier=0.1\n\n# Deliberately not doing this in a for loop because we will not augment more than twice\n# For ease of understanding the datasets being created\n# And we may choose to add different noisiness to each augmentation\nX_train_labelled_noisy = X_train_labelled + (X_noise_multiplier * np.random.normal(0, std_X, X_train_labelled.shape))\ny_train_labelled_noisy = y_train_labelled + (y_noise_multiplier * np.random.normal(0, std_y, y_train_labelled.shape))\n\nX_train_labelled_aug1a = pd.concat([X_train_labelled,X_train_labelled_noisy], ignore_index=True)\ny_train_labelled_aug1a = pd.concat([y_train_labelled,y_train_labelled_noisy], ignore_index=True)\nprint(X_train_labelled_aug1a.shape, y_train_labelled_aug1a.shape)\n\n#repeat\nX_train_labelled_noisy = X_train_labelled + (X_noise_multiplier * np.random.normal(0, std_X, X_train_labelled.shape))\ny_train_labelled_noisy = y_train_labelled + (y_noise_multiplier * np.random.normal(0, std_y, y_train_labelled.shape))\n\nX_train_labelled_aug2a = pd.concat([X_train_labelled_aug1a,X_train_labelled_noisy], ignore_index=True)\ny_train_labelled_aug2a = pd.concat([y_train_labelled_aug1a,y_train_labelled_noisy], ignore_index=True)\nprint(X_train_labelled_aug2a.shape, y_train_labelled_aug2a.shape)\n\n\n# Increased noise multiplier\nX_noise_multiplier=0.15\ny_noise_multiplier=0.15\n\nX_train_labelled_noisy = X_train_labelled + (X_noise_multiplier * np.random.normal(0, std_X, X_train_labelled.shape))\ny_train_labelled_noisy = y_train_labelled + (y_noise_multiplier * np.random.normal(0, std_y, y_train_labelled.shape))\n\nX_train_labelled_aug1b = pd.concat([X_train_labelled,X_train_labelled_noisy], ignore_index=True)\ny_train_labelled_aug1b = pd.concat([y_train_labelled,y_train_labelled_noisy], ignore_index=True)\nprint(X_train_labelled_aug1b.shape, y_train_labelled_aug1b.shape)\n\n#repeat\nX_train_labelled_noisy = X_train_labelled + (X_noise_multiplier * np.random.normal(0, std_X, X_train_labelled.shape))\ny_train_labelled_noisy = y_train_labelled + (y_noise_multiplier * np.random.normal(0, std_y, y_train_labelled.shape))\n\nX_train_labelled_aug2b = pd.concat([X_train_labelled_aug1b,X_train_labelled_noisy], ignore_index=True)\ny_train_labelled_aug2b = pd.concat([y_train_labelled_aug1b,y_train_labelled_noisy], ignore_index=True)\nprint(X_train_labelled_aug2b.shape, y_train_labelled_aug2b.shape)\n\n\nX_noise_multiplier=0.2\ny_noise_multiplier=0.2\n\nX_train_labelled_noisy = X_train_labelled + (X_noise_multiplier * np.random.normal(0, std_X, X_train_labelled.shape))\ny_train_labelled_noisy = y_train_labelled + (y_noise_multiplier * np.random.normal(0, std_y, y_train_labelled.shape))\n\nX_train_labelled_aug1c = pd.concat([X_train_labelled,X_train_labelled_noisy], ignore_index=True)\ny_train_labelled_aug1c = pd.concat([y_train_labelled,y_train_labelled_noisy], ignore_index=True)\nprint(X_train_labelled_aug1c.shape, y_train_labelled_aug1c.shape)\n\n#repeat\nX_train_labelled_noisy = X_train_labelled + (X_noise_multiplier * np.random.normal(0, std_X, X_train_labelled.shape))\ny_train_labelled_noisy = y_train_labelled + (y_noise_multiplier * np.random.normal(0, std_y, y_train_labelled.shape))\n\nX_train_labelled_aug2c = pd.concat([X_train_labelled_aug1c,X_train_labelled_noisy], ignore_index=True)\ny_train_labelled_aug2c = pd.concat([y_train_labelled_aug1c,y_train_labelled_noisy], ignore_index=True)\nprint(X_train_labelled_aug2c.shape, y_train_labelled_aug2c.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:48:09.871726Z","iopub.execute_input":"2025-01-26T05:48:09.872143Z","iopub.status.idle":"2025-01-26T05:48:09.960102Z","shell.execute_reply.started":"2025-01-26T05:48:09.872113Z","shell.execute_reply":"2025-01-26T05:48:09.958640Z"}},"outputs":[{"name":"stdout","text":"(5472, 34) (5472,)\n(8208, 34) (8208,)\n(5472, 34) (5472,)\n(8208, 34) (8208,)\n(5472, 34) (5472,)\n(8208, 34) (8208,)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Weights. We already have defined weights_labelled, weights_labelled2, weights_labelled3\n# Just need to augment\n\nweights_labelled_aug1 = pd.concat([weights_labelled, weights_labelled], ignore_index=True)\nweights_labelled_aug2 = pd.concat([weights_labelled_aug1, weights_labelled], ignore_index=True)\nprint(weights_labelled_aug1.shape, weights_labelled_aug2.shape)\n\nweights_labelled2_aug1 = pd.concat([weights_labelled2, weights_labelled2], ignore_index=True)\nweights_labelled2_aug2 = pd.concat([weights_labelled2_aug1, weights_labelled2], ignore_index=True)\nprint(weights_labelled2_aug1.shape, weights_labelled2_aug2.shape)\n\nweights_labelled3_aug1 = pd.concat([weights_labelled3, weights_labelled3], ignore_index=True)\nweights_labelled3_aug2 = pd.concat([weights_labelled3_aug1, weights_labelled3], ignore_index=True)\nprint(weights_labelled3_aug1.shape, weights_labelled3_aug2.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:48:12.066467Z","iopub.execute_input":"2025-01-26T05:48:12.066851Z","iopub.status.idle":"2025-01-26T05:48:12.077874Z","shell.execute_reply.started":"2025-01-26T05:48:12.066822Z","shell.execute_reply":"2025-01-26T05:48:12.076441Z"}},"outputs":[{"name":"stdout","text":"(5472,) (8208,)\n(5472,) (8208,)\n(5472,) (8208,)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## PCA","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=25)\n\nX_train_labelled_pca = pd.DataFrame(pca.fit_transform(X_train_labelled))\nX_test_pca = pd.DataFrame(pca.transform(X_test_fimpute))\nX_train_labelled_pca_aug1b = pd.DataFrame(pca.transform(X_train_labelled_aug1b))\nX_train_labelled_pca_aug2b = pd.DataFrame(pca.transform(X_train_labelled_aug2b))\n\nprint(X_train_labelled_pca.shape, X_test_pca.shape, X_train_labelled_pca_aug1b.shape, X_train_labelled_pca_aug2b.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:54:42.560939Z","iopub.execute_input":"2025-01-26T05:54:42.561394Z","iopub.status.idle":"2025-01-26T05:54:42.616112Z","shell.execute_reply.started":"2025-01-26T05:54:42.561326Z","shell.execute_reply":"2025-01-26T05:54:42.615054Z"}},"outputs":[{"name":"stdout","text":"(2736, 25) (20, 25) (5472, 25) (8208, 25)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## sii target data","metadata":{}},{"cell_type":"code","source":"y_train_labelled_sii = y_train_labelled.copy()\ny_train_labelled_sii.name='sii'\ny_train_labelled_sii = y_train_labelled_sii.apply(lambda row: 0 if row<=30 else \n                             (1 if row<50 else (\n                                2 if row<80 else (3)\n                            )))\n\ny_train_labelled_sii_aug1 = pd.concat([y_train_labelled_sii, y_train_labelled_sii], ignore_index=True)\ny_train_labelled_sii_aug2 = pd.concat([y_train_labelled_sii_aug1, y_train_labelled_sii], ignore_index=True)\n\n# Not using our alternative binned target, as it did not improve results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:54:47.534902Z","iopub.execute_input":"2025-01-26T05:54:47.535287Z","iopub.status.idle":"2025-01-26T05:54:47.543748Z","shell.execute_reply.started":"2025-01-26T05:54:47.535254Z","shell.execute_reply":"2025-01-26T05:54:47.542585Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## XGBoost custom QWK","metadata":{}},{"cell_type":"code","source":"# Custom qwk metric and loss function for pciat_sii\n#Modified from https://medium.com/@nlztrk/quadratic-weighted-kappa-qwk-metric-and-how-to-optimize-it-062cc9121baa\ny = np.array([0,1,2,1,1,2,3,3,2,3,2,1,2,1,0,1,0])\n\nc = 1.47\nd = 0.95\n\ng = np.zeros(4)\nfor i in range(4):\n    g[i] = ((y - i)**2).mean()\n    #g[i] = i\n\nprint(g)\nh = [(x-c)**2 + d for x in [0,1,2,3]]\nprint(h)\n\nplt.plot([0,1,2,3], g, marker=\".\", label=\"actual\")\nplt.plot([0,1,2,3], [(x-c)**2 + d for x in [0,1,2,3]], label=\"fitting\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:54:53.890014Z","iopub.execute_input":"2025-01-26T05:54:53.890433Z","iopub.status.idle":"2025-01-26T05:54:54.170515Z","shell.execute_reply.started":"2025-01-26T05:54:53.890398Z","shell.execute_reply":"2025-01-26T05:54:54.169245Z"}},"outputs":[{"name":"stdout","text":"[3.11764706 1.17647059 1.23529412 3.29411765]\n[3.1109, 1.1709, 1.2309, 3.2908999999999997]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABPiUlEQVR4nO3deXhU9d3+8feZSTJZJyFsCRB2RZFVZFXZN7FWqlakWrXVukErdanS9mlr26f0+VlbrcpmVaxWQUHQCknYQfY1yqLIvkjCnn2fOb8/ApEogSQk+c5yv65rLsnMGeae0+nk5jtnzseybdtGRERExBCH6QAiIiIS3FRGRERExCiVERERETFKZURERESMUhkRERERo1RGRERExCiVERERETFKZURERESMCjEdoCq8Xi9Hjx4lJiYGy7JMxxEREZEqsG2bnJwcmjVrhsNR+fqHX5SRo0ePkpSUZDqGiIiI1MDhw4dp0aJFpbf7RRmJiYkByp6M2+02nEZERESqIjs7m6SkpPLf45XxizJy7qMZt9utMiIiIuJnLnWIhQ5gFREREaNURkRERMQolRERERExSmVEREREjFIZEREREaNURkRERMQolRERERExSmVEREREjFIZEREREaNURkRERMQolREREZEglp5VwJq9J0nPKjCWwS9m04iIiEjtm7XxEBM/3IbXBocFk27rzJieLes9h1ZGREREglB6VgETP9xGS9Jxk4vXhl9/uN3IConKiIiISBDafzIPrw2TQl5njesXDHVsxmPbHDiZX+9Z9DGNiIhIEGrTKIpu1h76OndSYjvZ7m2N07Jo3Siy3rNoZURERCQINYxy8ZhrAQAfea/nhNWIv9zWicTYiHrPopURERGRILR0zVqG2+vBgla3PMOqDj2MFBFQGREREQk6tm1TsuqfOCybA/E30LPXDUbz6GMaERGRILP6sy8YXrQEgEYjf2U4jcqIiIhI0Dm2+J+4rBK+jrqG6Cv6m46jMiIiIhJMPtt7hCE5HwMQOegJsCzDiVRGREREgsqu5MnEWXmcCGtBg2t/YDoOoDIiIiISNPZlnOH6E7MA8Pb9OTichhOVURkREREJEhvnv0Fz6yRZjgY0veF+03HKqYyIiIgEgePZBXQ9NAOA7G4PQmi42UDnURkREREJAiuSZ3GVdYgCK4IWQ8eZjlOByoiIiEiAyy0qpdUX0wHIaH8XVmQDw4kqUhkREREJcIsXJ9OLHZTipNWoJ03H+Q6VERERkQBW4vESs3kyAIeb34yjQZLhRN+lMiIiIhLAlq5ey0DPWgCajTJ/6vcLURkREREJULZtU7zqZZyWzcH463E172w60gWpjIiIiASo1Z9/ybCixQA0HOGbqyKgMiIiIhKwMhb9k3CrhK+jOhJ95QDTcSqlMiIiIhKAPtt7hKE5HwG+MxCvMiojIiIiAWhX8pSzA/Ga0+Da20zHuSiVERERkQCz/1gm15+YCYCnj+8MxKuMyoiIiEiA2TD/9bMD8eJIuPF+03EuSWVEREQkgJzILqTLwbcAyO76AIRGGE50aUFdRtKzCliz9yTpWQWmo4iIiNSK5cmzuNo6SIEVToth403HqZIQ0wFMmbXxEBM/3IbXBocFk27rzJieLU3HEhERqbG8olKSvngNgGPtxtA6Mt5woqoJypWR9KyC8iIC4LXh1x9u1wqJiIj4tUVLUunDNjw4SLr5KdNxqiwoy8j+k3nYtpfhjo38O3QSERTisW0OnMw3HU1ERKRGSjxeoje9CsChZqNwNvCf1f6gLCNtGkURann5bcg79Hdu40fOpTgsaN0o0nQ0ERGRGlmyZj2DPGsAaDbqGcNpqicoy0hibAR/uq07kz2jAXg45BNu6diAxFjfP+JYRETk22zbpvjTswPxGvTD1aKL6UjVEpRlBGBMz5Y8/uT/kBWWQBMrk+b7Z1NY4jEdS0REpNrKBuItAqDh8KcNp6m+oC0jAInxsUQPeRKAezxzmbVuj+FEIiIi1Zex6GUirGKORl5N9FWDTMeptqAuIwDOa+8l39WEZtZpji5/XasjIiLiVz7fd5TBZwfihQ/8pU8PxKtM0JcRQsMJGzABgHtKPmT2xv1m84iIiFTDF8mTibdyORnajPjr7jAdp0ZURoCQ635CQVg8SY4THFz6JsWlXtORRERELunA8Sz6HT83EG+8zw/Eq4zKCEBYJCE3Pg7Aj4o/YN7mA2bziIiIVMG6T94gyTpBtiOOpv1/ajpOjamMnBXa60EKQ+No4zjGriVvUerR6oiIiPiuE9mFdD44A4CsLj/xi4F4lVEZOccVjaPfOADGFr7PR1sPGw4kIiJSueUpH3CNdYBCXLQY9nPTcS6Lysh5wvo+QlFIDO0dR9m+6G0854bXiIiI+JC8olJa7JwOQEb7O7GiGhpOdHlURs4X7oY+jwEwpmAm8z//2nAgERGR71q0ZBF9+bxsIN4o/xmIVxmVkW9xXf8oxc4ornIcZuvCd/BqdURERHxIicdL5NmBeIcTR+KMb202UC1QGfm2iAZ4ez0MwO2575G6Pd1wIBERkW8sXbuBwZ7VACTe/KzhNLWjWmVkypQpdOnSBbfbjdvtpm/fviQnJ1/0Ph988AFXXXUV4eHhdO7cmQULFlxW4PoQfsN4ih0RdHIcYH3qe9i2VkdERMQ827YpXPkyIZaXgw364mrR1XSkWlGtMtKiRQv++te/snnzZjZt2sTgwYO59dZb2bFjxwW3X7NmDWPHjuWBBx5g69atjB49mtGjR7N9+/ZaCV9nohrive4BAEbn/IclO48ZDiQiIgJrtu1ieNFCwD8H4lXGsi/zn/3x8fE8//zzPPDAA9+5bcyYMeTl5fHJJ5+UX9enTx+6devG1KlTq/wY2dnZxMbGkpWVhdvtvpy4VZd7nJK/dyLUW8Tv3H/iuV/+HMsPz/cvIiKBY/YL47kj522ORl5Fs6fX+fwcmqr+/q7xMSMej4eZM2eSl5dH3759L7jN2rVrGTp0aIXrRowYwdq1ay/6dxcVFZGdnV3hUu+im1Da/X4Absl8mxW7jtd/BhERkbM+35/O4Oy5gP8OxKtMtcvItm3biI6OxuVy8cgjjzB37lw6dux4wW0zMjJo2rRpheuaNm1KRkbGRR9j0qRJxMbGll+SkpKqG7NWRAz4JaVWGD0dX7Es9UMdOyIiIsZ8seC8gXg9/HMgXmWqXUY6dOhAWloa69ev59FHH+W+++5j586dtRpq4sSJZGVllV8OHzZ0NlR3IsVd7gFgxMl/s3bvKTM5REQkqB04nkXf4+8BUNr7MXCGGE5Uu6pdRsLCwmjfvj09evRg0qRJdO3alZdeeumC2yYkJHDsWMWDP48dO0ZCQsJFH8PlcpV/Y+fcxZTIwU9SaoXQz7mTlOS5xnKIiEjwWjf/TVpaJ8hxxJLQ/7vHaPq7yz7PiNfrpaio6IK39e3blyVLllS4btGiRZUeY+KTYltQfM1dAAw9/hYb9p82HEhERILJyZxCOh2YAUBm559AWKTZQHWgWmVk4sSJrFy5kgMHDrBt2zYmTpzI8uXLufvuuwG49957mThxYvn2jz/+OCkpKbzwwgt8+eWX/OEPf2DTpk2MHz++dp9FHYsc8jQenPR3bmNB8sem44iISBBZlvwBnaz9ZQPxhv/CdJw6Ua0ycvz4ce699146dOjAkCFD2LhxI6mpqQwbNgyAQ4cOkZ7+zRlL+/Xrx7vvvsv06dPp2rUrs2fPZt68eXTq1Kl2n0Vda9CawqvLDha6MX0GWw6dMRxIRESCQV5RKc13vgZARrsf+v1AvMpc9nlG6oOR84x826m9eF++Dgde/pAwmT88creZHCIiEjTmJSczev1deHDAL7b63RyaOj/PSNBp2I78K0cD0OfrN9l2JMtsHhERCWglHi+RGycDcDhxhN8VkepQGamG6GHP4sVipHMjHyanmI4jIiIBbNm6TQz2rAIgcdQzhtPULZWR6mjcgbx2NwPQ49AbfJFu4MywIiIS8GzbJn/FPwmxvBxq0BtXUnfTkeqUykg1xQwr+7bQKMd6PkhZcomtRUREqm/N9t3lA/Hih/3KcJq6pzJSXQmdyGk9Aodl02Xfa+w5nmM6kYiIBJijC18m0ioiPfJKoq8eYjpOnVMZqYGY4WWrI7c41jAzZbnZMCIiElC2HcgoH4jnGvBEQA3Eq4zKSE0060520mCclk2H3a+x/2Se6UQiIhIgdsyfQkMrh1OhCcRf90PTceqFykgNuUf8BoAfOD7lvdSVhtOIiEggOHA8m77H3wWgpPe4gBuIVxmVkZpqcR3ZzW4kxPLS5svXOHw633QiERHxc2sXvEkr6zg5DndADsSrjMrIZTi3OnK7Yzn/WbjGcBoREfFnJ3MK6bx/BgCZne6HsCijeeqTysjlaNWX7IQ+hFkemu+YxtHMAtOJRETETy1LmUMnax9FATwQrzIqI5fJPfzXANzpWMY7i9cbTiMiIv4or6iUxB3TAchoeztWdGPDieqXysjlatOf7MY9cFklNP58GsezC00nEhERP7No2RJuIA0PDlrcHPgnOfs2lZHLZVnEnF0ductazNtLNhkOJCIi/qTE4yV8w6sAHE4cjrNhG8OJ6p/KSC2w2g8hO74LEVYx7q1TOZlbZDqSiIj4iWXrNzHU8ykAiTcF36oIqIzUDssiZkTZ6siPrIW8s3SL4UAiIuIPbNsmb8UrhFheDsf1wtWyh+lIRqiM1BLrypFkx3UkyioifPM0zuQVm44kIiI+bs32PQwvTAGgwbCnDKcxR2WktlgWMcOfBeBHpPCfFZ8bDiQiIr7uyKKXibKKyIi4guiOw03HMUZlpBZZV91CjvsK3FYB1vppZBWUmI4kIiI+atuBDAZnlQ3ECxvwy6AYiFcZlZHa5HAQNaxsou89fMK7K7YbDiQiIr5q+4KpNLayOR2aQHzPO03HMUplpJY5rhlNbnQbYq18itdNJ6dQqyMiIlLRwRPZ9D1WNhCvuOej4Aw1nMgslZHa5nASObTs2JF77P/y3qovDAcSERFfs3b+W7S2jpHriCFh4M9MxzFOZaQOODrfQW5kEg2tHPJWTye/uNR0JBER8RGncgrpuP9NAM4E2UC8yqiM1AVnCBFDyk5cc4/3Y2au/spwIBER8RVLUufSxdpLEWG0GP646Tg+QWWkjji7jSUvohmNrSxOffoahSUe05FERMSw/OJSmm2fBkBG2zuCbiBeZVRG6oozlPBBZSew+bFnLh+s3W04kIiImLZo6RJuYGvZQLxRT5uO4zNURuqQ89p7yAtvSoJ1hvQVr1NUqtUREZFgVeLxErZhMgCHE4bhbNTWcCLfoTJSl0JchA14AoAflX7InA37DQcSERFTlq3fHPQD8SqjMlLHQq+7j/ywRrSwTnJo6euUeLymI4mISD2zbZvcFS8Tank4HNsTV6vrTEfyKSojdS00gtD+ZUdLjy2ezbxNBw0HEhGR+rZ2x15GnBuINzx4B+JVRmWkHoT2eoCC0Aa0chznqyVvUqrVERGRoHJ44bmBeO2J7jjCdByfozJSH8KicN7wcwDuKnyfj9MOGw4kIiL1ZfuBYwzO+hDQQLzKqIzUk7A+D1EYEks7Rzo7F72Fx2ubjiQiIvVg27mBeCFNie85xnQcn6QyUl9cMVj9HgPgzvyZLPj8a8OBRESkrh06kUOfjLMD8XppIF5lVEbqkavfoxQ5o7nS8TVpC/+NV6sjIiIBbfWCt2jjyCgbiDdAA/EqozJSn8JjsXs/DMDtuTNZuCPdcCAREakrp3IK6bjvDQDOXHMvuKINJ/JdKiP1LPyG8RQ7IunoOMiG1Hexba2OiIgEosWp8+hq7aWYUFqMmGA6jk9TGalvkfF4ej4IwOjsd1j6xTHDgUREpLblF5eSeHYgXnqb27GimxhO5NtURgyI6P84xY5wujj2syplplZHREQCzMJly+jPFrxYtLhZp36/FJURE6IaUXrtTwD4XuY7rPzqhOFAIiJSW0o9XsLWvwrA4aZDcTZqZziR71MZMSRywC8pscLo4djN8uQPtDoiIhIglmzYyjDPSgASRj1jOI1/UBkxJaYpJV1/DMDI0/9m7b5ThgOJiMjlsm2b3OVlA/GOxPbA1aqn6Uh+QWXEoMhBT1JqhdLb8SULF3xoOo6IiFymtTv3MaIwGYC4YRqIV1UqIybFNqeo01gAhhx/i40HThsOJCIil+Nw6itEW4UcC29L9DU3mY7jN1RGDIsa8jQenNzo3M6CBR+ZjiMiIjW04+AxBp0diBfaf4IG4lWDyohpcS0p6Fg2OKl/+ptsPXTGcCAREamJzxZMp4mVyZmQJsT3/pHpOH5FZcQHRA8tWx0Z5PyM/ybPNx1HRESq6dDJXPqk/weAouse0UC8alIZ8QXxbcnv8AMA+h55g+1fZxkOJCIi1bFq/r9p60gnz4omYdBDpuP4HZURHxEz7Fm8WAxzbmZucorpOCIiUkUVB+L9GFwxhhP5H5URX9HoCvLafx+Aaw/+iy8zsg0HEhGRqli88GO6WbspJpTmGohXIyojPiRm2LMA3OzcwOzkRYbTiIjIpeQXl5KwbSoA6a1/gBWTYDiRf1IZ8SVNO5LddhQAnff9iz3HcwwHEhGRi1m0fDkD2IwXi+ajnjYdx2+pjPgY97CJAHzPsZb3U5YZTiMiIpUp9XgJKR+IN5iQJlcaTuS/VEZ8TWIXslsOxWnZdNg9nQMn80wnEhGRC1i6MY1hpSsASLhJA/Euh8qID3KP+DUAtzpW817qSsNpRETk22zbJmf5y4RZHo7EXourdW/Tkfyayogvat6D7OYDCLG8tP1yGodP55tOJCIi51m3cz/DC8oG4sUO1UC8y6Uy4qPcI34DwG2Olby3aLXhNCIicr6Dqa8QYxVwLLwtMZ1GmY7j91RGfFXL3mQlXk+o5aH59qkczSwwnUhERIAdh44zOGsOACE3Pq6BeLVAZcSHxZ49duQOx3LeXbTOcBoREQFIm39uIF5jGmogXq1QGfFlrW8gq0lPXFYpjbdN5Xh2oelEIiJB7fCpXPqkvwOcHYgXEmY4UWBQGfFx544dGWMt4T9LNhpOIyIS3D795G3aOdLJs6JIGPSw6TgBQ2XEx1ltB5LVsBvhVgnurdM4mVtkOpKISFA6nVfMVWcH4p3ueK8G4tUilRFfZ1nl5x0Zay3kP0u3GA4kIhKcFqV+xLXWV5QQQouRE0zHCSgqI37AumI42Q06EWkVEbF5Kmfyik1HEhEJKgXFHpp+XjYQ76gG4tU6lRF/YFnEDC+bWTOWVN5bnmY2j4hIkFm4YgUD2VQ2EO8mDcSrbSojfsK66mayYzsQYxVgbZhGVkGJ6UgiIkGh1OPFue4VAI40GURI0w6GEwUelRF/YVlEDys7duRuFvDeyu2GA4mIBIclGz9neOlyAJpoIF6dUBnxI46O3yc7ph1uK5/SdVPJLSo1HUlEJKCVDcT7J2GWh6/d3Qlv08d0pIBUrTIyadIkevbsSUxMDE2aNGH06NHs2rXroveZMWMGlmVVuISHh19W6KDlcBA99FkA7vZ+wsxPdxoOJCIS2NbtPMDwggWABuLVpWqVkRUrVjBu3DjWrVvHokWLKCkpYfjw4eTl5V30fm63m/T09PLLwYMHLyt0MHN0vp2cqNY0sHLJWz2N/GKtjoiI1JUDC1/BbRVwPLwN0RqIV2dCqrNxSkpKhZ9nzJhBkyZN2Lx5M/3796/0fpZlkZCgr0HVCoeTyCG/go8f427vx8xa8xg/GXiN6VQiIgFnx6HjDMqcAxY4b3gcHDqyoa5c1p7NysoCID4+/qLb5ebm0qpVK5KSkrj11lvZsWPHRbcvKioiOzu7wkW+4ex6J7kRzWlkZXNm5XQKSzymI4mIBJy0Bf8iwTpDZkgjGva523ScgFbjMuL1epkwYQLXX389nTp1qnS7Dh068MYbb/DRRx/xzjvv4PV66devH0eOHKn0PpMmTSI2Nrb8kpSUVNOYgckZSvjgXwFwt2ceH6zbYziQiEhgOXwql15nB+IV9nhYA/HqmGXbtl2TOz766KMkJyezatUqWrRoUeX7lZSUcPXVVzN27Fj+9Kc/XXCboqIiioq+mcGSnZ1NUlISWVlZuN3umsQNPKXF5P6tC9GF6Tzv/Bm/mPh/uEKcplOJiASEd/89jR/t+xX5ViSRz+yCcP3uqYns7GxiY2Mv+fu7Risj48eP55NPPmHZsmXVKiIAoaGhdO/enT17Kv/XvMvlwu12V7jIt4SE4Rr4BAB3l87hww37DAcSEQkMp/OKuXLv62V/7vhjFZF6UK0yYts248ePZ+7cuSxdupQ2bdpU+wE9Hg/btm0jMTGx2veVikJ73EueqzHNrNMcXvY6JR6v6UgiIn5vcerHXGftooQQmo+YYDpOUKhWGRk3bhzvvPMO7777LjExMWRkZJCRkUFBQUH5Nvfeey8TJ04s//mPf/wjCxcuZN++fWzZsoV77rmHgwcP8uCDD9beswhWoeGE9f8lAGOLZjNv8wGzeURE/FxBsYdGn08D4GirW7HczQwnCg7VKiNTpkwhKyuLgQMHkpiYWH6ZNWtW+TaHDh0iPT29/OczZ87ws5/9jKuvvppRo0aRnZ3NmjVr6NixY+09iyAW2vMn5IfGk+Q4wd7Fb1Cq1RERkRpLXbGSgfZGAJqP+pXhNMGjxgew1qeqHgATrIpWvohr6e/Z723KZ7cuZHSP1qYjiYj4nVKPl+S//JBbPIs51GQQLR+bZzqS36vTA1jFt7h6P0hBSCxtHMfYuWgGHq/P90sREZ+zdNM3A/GajtSqSH1SGQkErmgc/cYDcGf+LJK3VX4OFxER+S7btsla9jIuq5Sv3V1xte1nOlJQURkJEK5+j1DojKG94yifpfwbr1ZHRESqbP0XBxhRMB8A9xANxKtvKiOBItwNfR4F4La891i4I/0SdxARkXP2p76K2yrghKsVMZ2/ZzpO0FEZCSDhNzxGkTOKqx2H2ZT6Nn5wbLKIiHE7D59gUOZsQAPxTNEeDyQRDfD2fAiA0dnvsuzLY4YDiYj4vq3zXyPBOkOWsyHxfe8xHScoqYwEmIgbf06xI4JOjgOsSn5PqyMiIhdx+FQuPY+WDcQr6PEQhLgMJwpOKiOBJqohpdf+FIDvZ77Np1+dMBxIRMR3fbrgXa50fE2+FUnC4EdNxwlaKiMBKHLgBEosF90ce1mRMkurIyIiF3A6r5gr9pwdiHf1PRAeazhR8FIZCUTRTSjufh8AI0/9m3V7TxkOJCLiexal/pee1pdlA/FG/tJ0nKCmMhKgogY+QYkVRk/HVyxOnm06joiITyko9tD486kAHG31fQ3EM0xlJFC5EynqcjcAQ46/xaYDpw0HEhHxHQtXfvrNQLybdOp301RGAlj04KcotULo59xJyoJ5puOIiPiEUo8Xa+0rOCybQ40HEpJwtelIQU9lJJDFtqCw4xgA+qe/QdrhTLN5RER8wLLN2xhRugzQQDxfoTIS4KKH/goPTvo7t/HJgv+ajiMiYpRt25xZ9gouq5SjMV1wtbvedCRBZSTwNWhN/lW3A9DnyOts/zrLcCAREXPWf3mQkfmfABCjgXg+Q2UkCMQMexYvDoY6t/JR8gLTcUREjNmXOhm3lc8JV0tiutxiOo6cpTISDBq2I/eKWwHocfB1dmXkGA4kIlL/dh4+wcAzHwAaiOdr9L9EkHAPn4gXi5HOjcxJTjUdR0Sk3m1Z8DrNrNNkOeOJ7/tj03HkPCojwaJxB3Lb3gxAl32vsed4ruFAIiL15/CpPHp9/TYABddqIJ6vURkJIu7hEwEY5VjP7JTFhtOIiNSflQve5UrHEQo0EM8nqYwEk4ROZLUagcOy6bD7NQ6czDOdSESkzp3JK+aKPW8AcOqqH0FEnNlA8h0qI0EmdkTZ6sj3Hat5P3WZ4TQiInVv4cL59LJ2aiCeD1MZCTbNupOVNBinZdPmy+kcPp1vOpGISJ0pLPHQ8LOygXjpLb+HFdvCcCK5EJWRIBQ74jcAjHasYtbCTw2nERGpOykrVzPYXg9AMw3E81kqI8GoxXVkJd5AqOWhxY5ppGcVmE4kIlLrPF67fCDe4Ub9CUm8xnQkqYTKSJCKHflbAG5zLOe9RWsNpxERqX1LN21nZMlSABprIJ5PUxkJVq36ktW0N2GWh8afT+V4TqHpRCIitca2bU4vexmXVcLRmM6Et7vBdCS5CJWRIOYe8WsA7rSW8t7iDYbTiIjUnvVfHmJkftmk8pjBT4BlGU4kF6MyEsSsNgPIbNQDl1VC7NapnMotMh1JRKRW7E2dQqyVz0lXEjFdbzUdRy5BZSSYWRaxZ1dHxliLeHfZZsOBREQu384jpxh45n0ArH6/AIfTcCK5FJWRIGe1H0JWfGcirGIiN00hM7/YdCQRkcuyZf6/aG6dItvZgIb97jUdR6pAZSTYWRbus+cduYtU3l2eZjaPiMhlOHI6j55nB+Lld/8ZhIYbTiRVoTIiWFeOJCuuI1FWEc4NU8guLDEdSUSkRlYseI8OjsMUWBEkDHnMdBypIpURAcsiZtizAIy1k3lvxTbDgUREqu9MXjHtd78OwKkOYyGigeFEUlUqIwKA4+pbyHZfgdsqwLNuCrlFpaYjiYhUy6JF8+lt7aQUJ81HPmE6jlSDyoiUcTiIHlY20fdH3vnMWrXDcCARkaorLPEQn1Y2EO9o0vew4pIMJ5LqUBmRco5rRpMd1YY4K4+CVVPJL9bqiIj4h5SVa84biPe04TRSXSoj8g2Hk8ihzwAw1vtf3l+zy3AgEZFL83htKB+IdwMhzTqbjiTVpDIiFYR0+SE5kUk0tHLIXDmNwhKP6UgiIhe1dNN2bipZAkDjERqI549URqQiZwgRg8v+z3y3Zx5z1u82HEhEpHK2bXNq2Su4rBLSo68hvH1/05GkBlRG5DtCuo8lN7wZja0sji2bTlGpVkdExDet3/XNQLzowU9qIJ6fUhmR73KG4hr0FAA/Kv2QuRv2GQ4kInJhe1KmEGfllQ3E6zbadBypIZURuaDQHveQ62pKgnWGI8teo8TjNR1JRKSCL46cYuCZDwCw+v1cA/H8mMqIXFiIi7ABZScNuqt4Dh9tPmA2j4jIt2xa8AYtrJNnB+LdZzqOXAaVEalUWM/7yAtrRAvrJPsW/4tSrY6IiI84cjqP6478G9BAvECgMiKVC40g5MbHARhT+AHz0w4bDiQiUmbFgllc7ThEoRWugXgBQGVELsrV+wEKQuNo5TjOl4vewOu1TUcSkSCXmV9Mu7MD8U5eqYF4gUBlRC4uLArH9b8A4I78WSRv+9pwIBEJdqkLk+ljbS8biHeTBuIFApURuSRX34coCImlnSOdbalvanVERIwpLPHQ4LOygXjpSTdjxbU0nEhqg8qIXJorBqvPowD8IHcmi3amGw4kIsEq5dO1DPGuBSDhJp36PVCojEiVhF//KIXOaDo4jrA55d/YtlZHRKR+ebw29ppXcFo2RxpeT6gG4gUMlRGpmog47F4PAzA6+12Wf3nccCARCTbLtuzkppLFADQaqVWRQKIyIlUWceN4ihyRdHQcZE3yO1odEZF6Y9s2J5e8TLhVQkZ0R8LbDzAdSWqRyohUXWQ8nuseBOB7me+wavcJw4FEJFhs2HWEEWcH4kUN0kC8QKMyItUSOeBxih3hdHXs49PkmabjiEiQ2J06hQZWLqfCmhPT/Qem40gtUxmR6olqREm3+wEYcerfrNt70mweEQl4X3x9moGnZ5X9oIF4AUllRKotatAvKbHC6OHYzZIFH5iOIyIBbuP8soF4Oc44Gl5/v+k4UgdURqT6YhIo6vpjAIaemMHmg6cNBxKRQHXkdB49zg7Ey+v2AIRGGE4kdUFlRGoketCTlFqh9HZ8Ser8D03HEZEAtTz5A65xHKTQcpEwZLzpOFJHVEakZmKbU3DNWABuTH+Tzw5nms0jIgEnM7+Ytl/9Czg7EC8y3nAiqSsqI1JjMUOfxoOTG53bmb/gI9NxRCTApC5KpZ+1DQ8Omo/UQLxApjIiNRfXkryrfwhA3yOvs+NoluFAIhIoCks8NEibDMDRFqOwGrQynEjqksqIXBb3sGfw4GCQ8zM+XjDfdBwRCRCpq9ZpIF4QURmRyxPflrwry05A1OPgv9iVkWM4kIj4O4/Xxrv63EC8foQ272o6ktQxlRG5bO5hz+LFYrhzM3OTU0zHERE/t3zLTkaeG4g34mnDaaQ+qIzI5Wt8JTntbgGgy77p7D2RaziQiPgr27Y5vuQVIqxiMqKuIvyKQaYjST1QGZFaETt8IgAjHRuZnbzIcBoR8VdlA/E+BiBSA/GCRrXKyKRJk+jZsycxMTE0adKE0aNHs2vXrkve74MPPuCqq64iPDyczp07s2DBghoHFh/VtCNZrUfisGyu3j2dg6fyTCcSET/0VeoU4q1cToc1w939NtNxpJ5Uq4ysWLGCcePGsW7dOhYtWkRJSQnDhw8nL6/yXzxr1qxh7NixPPDAA2zdupXRo0czevRotm/fftnhxbfEjvgNADc71vJ+yjLDaUTE33zx9WkGnnofALvveHCGGE4k9cWybduu6Z1PnDhBkyZNWLFiBf3797/gNmPGjCEvL49PPvmk/Lo+ffrQrVs3pk6dWqXHyc7OJjY2lqysLNxud03jSj3IfP124g4vZo6nP72fmEWLBpGmI4mIn/j39Be49+gfyXHGEvPMlxCm9w9/V9Xf35d1zEhWVtlJruLjKz9F79q1axk6dGiF60aMGMHatWsrvU9RURHZ2dkVLuIf4kb+GoBbHauYtXCl4TQi4i++PpP/zUC8rg+oiASZGpcRr9fLhAkTuP766+nUqVOl22VkZNC0adMK1zVt2pSMjIxK7zNp0iRiY2PLL0lJSTWNKfWteQ8ymw0gxPKStGMaGVmFphOJiB9Ylvw+1zgOlA3EG/pz03GkntW4jIwbN47t27czc+bM2swDwMSJE8nKyiq/HD58uNYfQ+pO3MiyY0dGWyt4b+Eqw2lExNdl5ZfQZtfrAJy84k4NxAtCNSoj48eP55NPPmHZsmW0aNHiotsmJCRw7NixCtcdO3aMhISESu/jcrlwu90VLuJHWvYms2lfwiwPTbdN5XiOVkdEpHIpi1O53vq8bCDeTU+ZjiMGVKuM2LbN+PHjmTt3LkuXLqVNmzaXvE/fvn1ZsmRJhesWLVpE3759q5dU/Ers2dWR261lzFyy3nAaEfFVhSUeYrdOAeBoi5uwGrQ2G0iMqFYZGTduHO+88w7vvvsuMTExZGRkkJGRQUFBQfk29957LxMnTiz/+fHHHyclJYUXXniBL7/8kj/84Q9s2rSJ8ePH196zEJ9jtbmRzMbX4bJKidsyhVO5RaYjiYgPSl21nqHeNQAk3PSM4TRiSrXKyJQpU8jKymLgwIEkJiaWX2bNmlW+zaFDh0hPTy//uV+/frz77rtMnz6drl27Mnv2bObNm3fRg14lMJw778id1mJmLt1kOI2I+BqP18az5hVCLC9H4vtqIF4Qu6zzjNQXnWfET9k2mS8PJO50Gm/at/CDZ94gLjLMdCoR8RGLN+3k+v8OIMIqpmDsh0R0GGI6ktSyejnPiMhFWRaxZ887MoaFzFy+1XAgEfEVtm1zfGnZQLxjUVcRceVg05HEIJURqVPWFcPJjLuGSKsI54bJZBeWmI4kIj5g4+6vGZFXNhAvYtATGogX5FRGpG5ZFu4RZasjd9mpzFrxmeFAIuILvkyZSkMrh9Nhibi73246jhimMiJ1znHVzWS5OxBjFeBZO5W8olLTkUTEoC+PnmbAybIvPth9NBBPVEakPlgW0cPLvu79I3s+76/SxGaRYLZ+/gxaOY6T63DT8Iafmo4jPkBlROqFs+OtZEW3w23lU7B6KgXFHtORRMSAo2fyufZw2UC8XA3Ek7NURqR+OBxEDS07odFYz3/5YM0XhgOJiAlLk2fT2bGfIg3Ek/OojEi9CelyB9mRrWhg5ZK1ciqFJVodEQkmWfkltN71LwBOtL8TohoaTiS+QmVE6o/DSeSQstWRuzwf8eH6rwwHEpH6lLxkETdYn50diPek6TjiQ1RGpF6FdLuTnIjmNLayOb5sGsWlXtORRKQeFJZ4cG85OxCv+Qis+EsPWpXgoTIi9csZSvigshHhY0vnMnfjXsOBRKQ+pKzeyHDvKkAD8eS7VEak3oVeew+5rgSaWpl8vXQaJR6tjogEMo/XpnR12UC8r+N7E9qiu+lI4mNURqT+hYQRNrDs8+K7iufw380HzOYRkTq1fOsXjCpeCED88KcNpxFfpDIiRoRddy95YY1oZp1m/5LX8Hh9fni0iNSAbdtkLHmVSKuIY5FXEtFhqOlI4oNURsSM0HBC+j8BwJ0FHzA/7aDhQCJSFzbuPsrIvI8AiBiogXhyYSojYoyr10/ID40nyXGCXQvfwKvVEZGAc24g3pnQBNw9fmg6jvgolRExJywSxw1lZ2C8I38mqduOGA4kIrVp19FMBpycCYC3zzgNxJNKqYyIUeF9HqIgJJY2jmN8nvqmVkdEAsi6BecNxLvxAdNxxIepjIhZrmisvuMBuC13Jot3HjUcSERqQ3pmPtcemgFAbtefQFiU2UDi01RGxLjw6x+h0BnDFY6v2Zryb2xbqyMi/m7xgjl0duynmDAShv7CdBzxcSojYl64G2/vRwD4fvZ/WL7rmOFAInI5zh+Id/yKH0JUI8OJxNepjIhPiLxxHEWOSK52HGZd8ttaHRHxY8lLFnOjlVY2EG/kU6bjiB9QGRHfENGA0p4/A+CWM++wevdJw4FEpCbKBuJNBiC92XCshm0NJxJ/oDIiPiOq/+MUOyLo5DjAp8n/MR1HRGogdc35A/F+ZTiN+AuVEfEdUQ0p7v5TAEaeept1e7U6IuJPPF6bklVnB+I16EVIUg/TkcRPqIyIT4keNIFiy0V3xx6WJ88yHUdEqmF52i5u0kA8qQGVEfEt0U0o6novAIOPv8XmA6cNBxKRqrBtm/TFrxJlFXE8sj0RVw0zHUn8iMqI+JyYwU9SYoXRy7GLRclzTMcRkSrYtCedEXnzAAjXQDypJpUR8T3uRAo7jQWg/9E3+PxIptk8InJJX6RMo7GVfXYg3p2m44ifURkRnxQz5GlKCaGfcycL5s8zHUdELmLX0UxuPPEeAN4+j4Ez1HAi8TcqI+Kb4pLI7zgGgL5HXmfn0WzDgUSkMmsXvEUbx7GzA/EeNB1H/JDKiPgs99Cn8eBggPNz/pv8sek4InIB6Zn5dD/0FgA5Xe7XQDypEZUR8V3xbcjtcAcA1x34F18dyzEcSES+bXHyh3R17KWYMBI1EE9qSGVEfFrs8Gfx4mCIcyvzFiSbjiMi58kqKKHll2UD8U60vwOiGxtOJP5KZUR8W8N2ZLf/PgBd901n34lcw4FE5JzkJYsZYG3Fg4NmN2kgntScyoj4vLjhE/FiMcK5kTnJC03HERHKBuJFb54KQHriMKyG7QwnEn+mMiK+r8lVZLcZBcDVu6dz6FS+4UAisnDNZkZ4PwUgYZQG4snlURkRvxA34tcAjHKs44OUxYbTiAQ3r9emcNUrhFoejjboSUjSdaYjiZ9TGRH/kNCJMy2H47Bs2u2axteZBaYTiQStZZ99xajiVAAaDNNAPLl8KiPiNxqMLFsducVazfupKwynEQlOtm1zdPGrRFuFnIhsT8TVw01HkgCgMiL+o1l3zjQfiNOySdoxhYysQtOJRILOpr0ZjMydB0DYgF9qIJ7UCpUR8StxI38DwK3Wp8xctMpwGpHgszN5Go2tLDJDmxB73RjTcSRAqIyIX7GSenEm4XpCLQ8Jn0/hRE6R6UgiQeOr9ExuODETgNLeGogntUdlRPzOudWR26xlzFq81nAakeCxZsHbtHOkk+eIodGNPzMdRwKIyoj4Hav19Zxp0pswy0ODtMmczis2HUkk4KVn5tPt4AwAcjrfB65os4EkoKiMiF+KO/vNmjtYysylGwynEQl8i1Pm0c2xh2JCSRj2uOk4EmBURsQvWW0GcKZhd1xWCVGbJpOVX2I6kkjAyioooeUXrwFwot3tEN3EcCIJNCoj4p8si9gRvwXgThYxc/lmw4FEAlfKkqUMsLbgxaLZTTrJmdQ+lRHxW44rhpDZoDMRVjEhGyaTU6jVEZHaVlTqIWrzZACOJg7FatTecCIJRCoj4r8si5jhZceO3GWnMGvlZ4YDiQSelNXfDMRrOlID8aRuqIyIX3NedROZsVcRZRVhr5lMXlGp6UgiAcPrtSla9Sqhlof0uB6EtuplOpIEKJUR8W+WRcywstWRMfYCPli13XAgkcCx7LPdjCpOASBOA/GkDqmMiN9zdryFzJgrcFsFFK6eTEGxx3QkEb9XYSBeRDsiOo40HUkCmMqI+D+Hg+hhzwJwl+cTZq/ZaTiQiP/bvDeDEWcH4rkGTNBAPKlTKiMSEEI6/YCsqNbEWXlkfzqFwhKtjohcjh0p02liZZIZ2gT3dXeZjiMBTmVEAoPDSeSQZwC4q/Rj5q7/ynAgEf+1OyOLG46/B0Bpr0cgJMxwIgl0KiMSMEK73kl2RBINrRxOLptCcanXdCQRv7R6ftlAvHxHFI36P2Q6jgQBlREJHM4QIgaXHfF/V+k8Ptq4x3AgEf+TkVlA17MD8bI73QeuGLOBJCiojEhACb32R+SEJ9LYyuLo0mmUerQ6IlIdi1Ln0d2xmxJCSRg2wXQcCRIqIxJYnKG4BpatjtxZ/CH/3XzAbB4RP5JVUEKLnWUD8Y63/QHENDWcSIKFyogEnLDr7iHX1ZRE6zQHlkzD47VNRxLxC8lLlzPI2owXi8SbdOp3qT8qIxJ4QlyE9p8AwA8LZzM/7YDROCL+oKjUQ+SmswPxEobgaHyF4UQSTFRGJCC5ev2EvNCGtLBOsmfRv/BqdUTkolLXbmGkdwUATW96xnAaCTYqIxKYQiNw3vg4ALfnvc/CbUcMBxLxXV6vTf7KVwmzPKTHXauBeFLvVEYkYIX3eZD8kDhaOY6zPfV1bFurIyIXsvzzPdx8diBe7NCnDKeRYKQyIoErLArr+p8D8IPcmSzekW44kIhvOrJ4MjFWASci2hLZ8SbTcSQIqYxIQIvo9zAFIW7aOdJJS31TqyMi37J5bzojcj4EwNV/Ajj0a0Hqn151EthcMdi9HwXg+1nvsmLXMcOBRHzL58n/oqmVSVZoY9w9x5qOI0Gq2mVk5cqV3HLLLTRr1gzLspg3b95Ft1++fDmWZX3nkpGRUdPMItUSecNjFDqj6eA4woYFb2l1ROSsPceyuPH4uwCU9NRAPDGn2mUkLy+Prl278uqrr1brfrt27SI9Pb380qRJk+o+tEjNRMTh6Vk27Ot7mf9hzZ6ThgOJ+IZPP3mH9o6jGognxoVU9w433XQTN91U/QOcmjRpQlxcXLXvJ1Ibovr/nKINU+nIQf7fgre5/vEnTEcSMSojq5DOB98CB2R3upfIcLfpSBLE6u2YkW7dupGYmMiwYcNYvXr1RbctKioiOzu7wkXkskTGU9LjAQBGnHqb9Xu1OiLBbWHKPK5z7NJAPPEJdV5GEhMTmTp1KnPmzGHOnDkkJSUxcOBAtmzZUul9Jk2aRGxsbPklKSmprmNKEIgeOIFiRzhdHftYkTzTdBwRY7ILS2hePhBvNMQkmA0kQc+yL+NoPsuymDt3LqNHj67W/QYMGEDLli15++23L3h7UVERRUVF5T9nZ2eTlJREVlYWbreWEqXmcj76FTFbp7HZewXWAwu5tlW86Ugi9W7mgsXcteF2vFjw2AYcTa40HUkCVHZ2NrGxsZf8/W3kq729evViz549ld7ucrlwu90VLiK1IWbwE5RYYfRw7Gbx/PdNxxGpd0WlHiI3ln0BIb3pIBUR8QlGykhaWhqJiYkmHlqCXUwCBZ3vAaB/xgy2HckyHEikfqWuTSsfiNdEA/HER1S7jOTm5pKWlkZaWhoA+/fvJy0tjUOHDgEwceJE7r333vLtX3zxRT766CP27NnD9u3bmTBhAkuXLmXcuHG18wxEqsk95ClKrVD6OL4gef4c03FE6o3Xa5P/6StlA/FiuxHauo/pSCJADcrIpk2b6N69O927dwfgiSeeoHv37vzud78DID09vbyYABQXF/Pkk0/SuXNnBgwYwGeffcbixYsZMmRILT0FkWqKbU5ex7sA6Hvkdb5I17e1JDis+Hwvo4qSAQ3EE99yWQew1peqHgAjUmWZh/C82A0nHv7W4hWeevDHphOJ1Ll//+0J7s19nZMRrWn09FbNoZE659MHsIoYF9eSnKvuAKDHwdfYfSzHcCCRurV5bzrDzw7EC7txgoqI+BS9GiVoxQ1/Fg8OBjk/46MFn5iOI1KnPk9+nQTrDFkhjXD3+pHpOCIVqIxI8IpvS3b70QB02fca+0/mmc0jUkf2HMvihrMD8Up7PgIhLsOJRCpSGZGg1mDERLxYDHdu5sPkFNNxROrEp/P/wxWOrymwomg4QAPxxPeojEhwa3wlWW1vAeDq3dM4fDrfcCCR2nUsu5BOB2YAkHXNPRAeazaQyAWojEjQazBiIgAjrQ3MTllkOI1I7UpN+Yiejl2UEkLC8F+ajiNyQSojIk07cqbVSByWTfsvp/J1ZoHpRCK1IruwhOY7ygbiHWszGtw687X4JpUREaDByN8AMMpay+zUZYbTiNSO5KUrGMQmABJHPm04jUjlQkwHqC1er5fi4mLTMQJeaGgoTqfTdIzal9iF0y2GEH9kCUk7pnAsezBN3eGmU4nUWFGpB9emKTgsm6+bDqJ506tMRxKpVECUkeLiYvbv34/X6zUdJSjExcWRkJCAZVmmo9SqBiN/A/9awvetVUxe+Cm/uGOY6UgiNZa67jNu8iwHC5qM/JXpOCIX5fdlxLZt0tPTcTqdJCUl4dBZBeuMbdvk5+dz/PhxgICbvGy16MHpxP7Ep68k4fPJnBzZn0bROh+D+B+v1yZ35Su4rFIyYruS0Kaf6UgiF+X3ZaS0tJT8/HyaNWtGZGSk6TgBLyIiAoDjx4/TpEmTgPvIpsFNv4E3VjLaWsFri9cwbvQg05FEqm35tn18rygZLIgd8qTpOCKX5PfLCB6PB4CwsDDDSYLHudJXUlJiOEnts1r24XSTPoRZHhpsnczpPB2HJP7n8KLJuK18ToW3IqLTLabjiFyS35eRcwLt+AVfFuj7usFNvwXgdpby/pL1htOIVM/mvRnlA/FCbnxcA/HEL+hVKvItVpsbOd3oOlxWKVGbJ5NVEHgrQBK4Pkt5nUTrNNkhDYntfY/pOCJVojIi33H//fczevRo0zGMihtRdt6RH7KI95dtMpxGpGr2HMvm+mNlA/GKNRBP/IjKiJ/6wx/+QLdu3UzHCFiO9oM43aAr4VYJoRteJadQqyPi+z5d8C4dHEcosCJpNOBh03FEqkxlRORCLIvYs2dl/aG9kPdXppnNI3IJx7ILuWb/DEAD8cT/qIycJz2rgDV7T5KeVT+zSVJSUrjhhhuIi4ujYcOGfO9732Pv3r3ltx85coSxY8cSHx9PVFQU1113HevXr2fGjBk899xzfPbZZ1iWhWVZzJgxgwMHDmBZFmlpaeV/R2ZmJpZlsXz5cqDs20cPPPAAbdq0ISIigg4dOvDSSy/Vy/P1N84rh3Mm9hqirCJYO5n84lLTkUQqlZryX3o5vqBEA/HED/n9eUa+zbZtCko81b7fnM1H+P3HO/Da4LDgue9fw+09WlTr74gIdVbrmyZ5eXk88cQTdOnShdzcXH73u9/xgx/8gLS0NPLz8xkwYADNmzfn448/JiEhgS1btuD1ehkzZgzbt28nJSWFxYsXAxAbG8uxY8cu+Zher5cWLVrwwQcf0LBhQ9asWcNDDz1EYmIid955Z7Web8CzLNwjfg3v382d3mRmf7qNe4d0N51K5DtyCktI3DEdLDjR+laauZuZjiRSLQFXRgpKPHT8Xepl/R1eG/7nox38z0c7qnW/nX8cQWRY1Xfp7bffXuHnN954g8aNG7Nz507WrFnDiRMn2LhxI/Hx8QC0b9++fNvo6GhCQkJISEioVsbQ0FCee+658p/btGnD2rVref/991VGLsB59c1kxlxJXM5XFK6aTGH/qYSHBtaJ3sT/zV/2KXeyEYCEmzQQT/yPPqYxaPfu3YwdO5a2bdvidrtp3bo1AIcOHSItLY3u3buXF5Ha9Oqrr9KjRw8aN25MdHQ006dP59ChQ7X+OAHBsogePhGAMd5PmL26egVVpK4VlXpwbZx8diDeQBxNrzYdSaTaAm5lJCLUyc4/jqjWfTKyChn69xV47W+uc1iw+IkBJMRWfXJrRDX/xXzLLbfQqlUrXnvtNZo1a4bX66VTp04UFxeXn3a9Os7N5bHtb57It8+SOnPmTJ566ileeOEF+vbtS0xMDM8//zzr1+vkXpUJuWY0mal/Ji53L7mfTqHohldwhWh1RHxD6rrPGXV2IF7jERqIJ/4p4FZGLMsiMiykWpe2jaOZdFtnnGeP93BaFpNu60zbxtHV+nuqc7zIqVOn2LVrF7/97W8ZMmQIV199NWfOnCm/vUuXLqSlpXH69OkL3j8sLKz8VPjnNG7cGID09PTy684/mBVg9erV9OvXj8cee4zu3bvTvn37CgfNygU4HEQNfQaAO0s/Zu66rwwHEinj9drkrHwVl1VChrsLYRqIJ34q4MpITY3p2ZJVzw7ivZ/1YdWzgxjTs2WdPl6DBg1o2LAh06dPZ8+ePSxdupQnnnii/PaxY8eSkJDA6NGjWb16Nfv27WPOnDmsXbsWgNatW7N//37S0tI4efIkRUVFRERE0KdPH/7617/yxRdfsGLFCn77299WeNwrrriCTZs2kZqayldffcX//M//sHHjxjp9roEgtMsdZEW2JN7K5dTyyRSXek1HEmHFtn18r2g+AO6hT0GAj2qQwKUycp7E2Aj6tmtIYmz1PyKpLofDwcyZM9m8eTOdOnXil7/8Jc8//3z57WFhYSxcuJAmTZowatQoOnfuzF//+tfyKbm33347I0eOZNCgQTRu3Jj33nsPKDsItrS0lB49ejBhwgT+/Oc/V3jchx9+mNtuu40xY8bQu3dvTp06xWOPPVbnz9fvOZxEDDm7OlIyj4837TYcSAQOLZ5CrJXPqfCWRGognvgxyz7/AAMflZ2dTWxsLFlZWbjd7gq3FRYWsn//ftq0aUN4eNWP75CaC9p97ikh+/muuAu/5uXQn/Losy8Q4lSfFzO27Msg4a2+NLNOkz3sBdzXP2g6ksh3XOz39/n0TipSVc5QXIOeAuDO4g/5ZMt+w4EkmKUlv1FWRELicffSQDzxbyojItXg6nEPOa4EmlqZHFw0FY/X5xcWJQDtOZbD9cf+A0DxdQ9BaBCtUEpAUhkRqY6QMEIHlB1ofEfRHBZ8dtBwIAlG5wbiFVoRNBrwqOk4IpdNZUSkmsJ73kduWCOaW6fYs3A6Xq2OSD06nl1Ix/1vApDZ8W6IiDMbSKQWqIyIVFdoOM4bywaR3ZH3Pou2HzYcSIJJcuon9HZ8QSlOEoY/cek7iPgBlRGRGojo/VPyQuNJcpxgR8rr+MGX0iQA7D6WQ8L2aQAcb/19iG1uOJFI7VAZEamJsEgc/X4OwOjcmSzdcdRwIAl0szYe4qEXZzHs7EC8tKQfG04kUnsCbjaNSH2J6PcQ+atfpG1pBvNS3mDwNb+t1kgAEa/XJq+4lOzCUrILSsouhaXkFH7z5+yCEjKyC/jk83T+EjIfh2WzxNOdny8uovt1BfVykkaRuqYyIlJTrmjsPuNg1V+4Jfs9Vn71EAM6NDWdSuqRx2uTW3S2SBSWkF1Qeva/55eKb64rKMjDm5+FXZiJoyib0JIsYux83FYebvJxW/m4ycNt5dGWitf/3ZVHmFU2j2pa6ffw2DYHTuarjEhAUBkxxLZtHn74YWbPns2ZM2eIjY3l/vvv58UXX6z239W6dWsmTJjAhAkTaj2nXFzUDY9SsPZlruBrPlowg/5X/kqrI36k1OMlp7C0vEjkFF64VJz7b15BPt78LCjMwlGURUhJ9tmykFf+31jycFv5tDvv+tiz/3VZFadoE1r9zIs817LBvgqnZdG6UWTt7AgRw1RGDElJSWHGjBksX76ctm3b4nA4iIj45l84FyoYM2bMYMKECWRmZlb4uzZu3EhUVFQ9JZcKwt14ej0Ca5/n5jNvs3bPT+h3RRPTqYJGcan3bIGouDpRWanIzS/EW5ANRZlYhVmEluZUKBNlpaGsTLT5Vrlwk0+kVfTNg1tAWPUz21h4wmKwXbEQHosjsgGOiFis8DgIj/3mElHx54935fHb5MNk2+E4LYu/3NZJqyISMFRGDNm7dy+JiYn063f5I78bN25cC4mkpqL7j6Nw/atczWH+tuBt+j3+pOlIfqOwxEN2YUnZ6kTBd0tF2W3n/Tm/iNLCbCjIwlGUjcuTU14evikNFcuE+7wyEWMVfPPgDmpUJgA8odF4XbFYEbE4IuJwVCgOcRVLxbfKhRUWQ4ij+t8d+H5T6Nm1MwdO5tO6UaSKiAQUDcoz4P777+ett94q/7lVq1a0bt2abt268eKLLzJw4EBWrFhR4T7Lli1j0KBBFa77/e9/zx/+8IfvrKJYlsVrr73G/PnzSU1NpXnz5rzwwgt8//vfL7/vxx9/zJNPPsnhw4fp27cv999/P/fffz9nzpwhLi7uovn9cZ/XtZz5vyNm40ts87am4P6l9Grb0HSkOmfbNoUl3vNWHip+pPHdUlFKdn4xJYXZUJCNVZRFhCf3O6sP3/5o4/yPQGIowGFd/luWJySybGUiIhZHRCyO8LjvrERUWi5cbnDq33EiVVHVQXmB9/8o24aSfDOPHRoJVThe4KWXXqJdu3ZMnz6djRs34nQ6+eEPf1h++4cffkjXrl156KGH+NnPfgZAfHw8L774Ir/73e/YtWsXANHR0ZU+xnPPPcf/+3//j+eff56XX36Zu+++m4MHDxIfH8/+/fu54447ePzxx3nwwQfZunUrTz311GU++eAWM/BxijZPpzMH+NuC/9Br/C9MR7ok27bJL/Zc4OOMi6xUFBRTXJiPXZCJoyiLSG9uhdWI84+baHOB693kE2J5ywKEUON3IK/Tdd7HHHHf/Yjjgh91xJWXCWdIDZdERKROBF4ZKcmHvzQz89i/Pgphlz52IzY2lpiYGJxOJwkJCd+5PT4+HqfTSUxMTIXbY2NjsSzrgvf5tvvvv5+xY8cC8Je//IV//vOfbNiwgZEjRzJt2jQ6dOjA888/D0CHDh3Yvn07//u//1vVZyrfFtWQom4/wbVlMkOOv8XWg/fQvVV8nT6k12uTW1xWFqr6MUd2YQkF+QXYhZk4i7KItvO+UybOHTfR+rzrY867/tw3Ompy8OU5tiMUr8sNEXE4wuOwIi62GhFXsVy43Dg0GE4koAReGREAunTpUv7nqKgo3G43x48fB2DXrl307Nmzwva9evWq13yByD34lxRvfZ3ujj385eP3KBg1hjaNoir9bN/jtck9+02OrEutSJxXKvIKCsq+zVF87muh+d86bqJsBaL1d64v+zn83Dc6LmNxwLacZcdMhJcdN2FdbCXiAh91WKEROPWtIxE5K/DKSGhk2QqFqcf2EaGhFf/ZalkWXq/XUJogEd2Egi73EvbZaww98RZ3vtYKC4vuLeOIjworLxV5BUV4C7NxFGdVOLfEt4+RaPWd68t+jjr3jY7LKRNY2C53WTGo8E2OuIt8zHFemQiLVpkQkVoTeGXEsqr0UYmvCwsLw+PxXPK6mujQoQMLFiyocN3GjRsv++8VKOw1jvC0GfRy7OLV0JcIw4M7I+/sNznOrVQUlH0t1HV5j+UNi8YKj8O6YGG40HUVv9Fh1eAbHSIidSHwykiAaN26NStXruSuu+7C5XLRqFEjWrduTW5uLkuWLKFr165ERkYSGVn91ZiHH36Yv//97zzzzDM88MADpKWlMWPGDACdsOsy7S2MYbdnIPeFLOJm54aLbmuHRpYVg28fE1HFb3Q49I0OEQkQejfzUX/84x95+OGHadeuHUVFRdi2Tb9+/XjkkUcYM2YMp06dKv9qb3W1adOG2bNn8+STT/LSSy/Rt29ffvOb3/Doo4/icl3mP9eDXJtGUTzsuYt0uyE2FllEkUsUz43pR8OGjSuUCst5GUeAiogEEJ1nRAD43//9X6ZOncrhw4cvua32+cXN2niIX3+4HY9tl58pc0zPlqZjiYjUu+A9z4hUyeTJk+nZsycNGzZk9erVPP/884wfP950rIAwpmdL+l/ZWGfKFBGpIpWRILV7927+/Oc/c/r0aVq2bMmTTz7JxIkTTccKGImxESohIiJVpDISpP7xj3/wj3/8w3QMERER9N0+ERERMUplRERERIwKmDLiB18KChg6k6uIiNQmvz9mJDQ0FMuyOHHiBI0bN9ZJu+qQbdsUFxdz4sQJHA4HYWGafCoiIpfP78uI0+mkRYsWHDlyhAMHDpiOExQiIyNp2bIlDp1OXEREaoHflxGA6OhorrjiCkpKSkxHCXhOp5OQkBCtQImISK0JiDICZb8knU6n6RgiIiJSTVpnFxEREaNURkRERMQolRERERExyi+OGTl3DpHs7GzDSURERKSqzv3evtS5wPyijOTk5ACQlJRkOImIiIhUV05ODrGxsZXebtl+cOpSr9fL0aNHiYmJqdWvlGZnZ5OUlMThw4dxu9219vcGIu2r6tH+qjrtq6rTvqo67auqq8t9Zds2OTk5NGvW7KLnpvKLlRGHw0GLFi3q7O93u916sVaR9lX1aH9VnfZV1WlfVZ32VdXV1b662IrIOTqAVURERIxSGRERERGjgrqMuFwufv/73+NyuUxH8XnaV9Wj/VV12ldVp31VddpXVecL+8ovDmAVERGRwBXUKyMiIiJinsqIiIiIGKUyIiIiIkapjIiIiIhRAV9GXn31VVq3bk14eDi9e/dmw4YNF93+gw8+4KqrriI8PJzOnTuzYMGCekpqXnX21YwZM7Asq8IlPDy8HtOas3LlSm655RaaNWuGZVnMmzfvkvdZvnw51157LS6Xi/bt2zNjxow6z+kLqruvli9f/p3XlWVZZGRk1E9ggyZNmkTPnj2JiYmhSZMmjB49ml27dl3yfsH4nlWTfRWs71lTpkyhS5cu5Sc069u3L8nJyRe9j4nXVECXkVmzZvHEE0/w+9//ni1bttC1a1dGjBjB8ePHL7j9mjVrGDt2LA888ABbt25l9OjRjB49mu3bt9dz8vpX3X0FZWfrS09PL78cPHiwHhObk5eXR9euXXn11VertP3+/fu5+eabGTRoEGlpaUyYMIEHH3yQ1NTUOk5qXnX31Tm7du2q8Npq0qRJHSX0HStWrGDcuHGsW7eORYsWUVJSwvDhw8nLy6v0PsH6nlWTfQXB+Z7VokUL/vrXv7J582Y2bdrE4MGDufXWW9mxY8cFtzf2mrIDWK9evexx48aV/+zxeOxmzZrZkyZNuuD2d955p33zzTdXuK537972ww8/XKc5fUF199Wbb75px8bG1lM63wXYc+fOveg2v/rVr+xrrrmmwnVjxoyxR4wYUYfJfE9V9tWyZctswD5z5ky9ZPJlx48ftwF7xYoVlW4TzO9Z56vKvtJ71jcaNGhg/+tf/7rgbaZeUwG7MlJcXMzmzZsZOnRo+XUOh4OhQ4eydu3aC95n7dq1FbYHGDFiRKXbB4qa7CuA3NxcWrVqRVJS0kWbdrAL1tfV5ejWrRuJiYkMGzaM1atXm45jRFZWFgDx8fGVbqPXVpmq7CvQe5bH42HmzJnk5eXRt2/fC25j6jUVsGXk5MmTeDwemjZtWuH6pk2bVvr5c0ZGRrW2DxQ12VcdOnTgjTfe4KOPPuKdd97B6/XSr18/jhw5Uh+R/Uplr6vs7GwKCgoMpfJNiYmJTJ06lTlz5jBnzhySkpIYOHAgW7ZsMR2tXnm9XiZMmMD1119Pp06dKt0uWN+zzlfVfRXM71nbtm0jOjoal8vFI488wty5c+nYseMFtzX1mvKLqb3ie/r27VuhWffr14+rr76aadOm8ac//clgMvFnHTp0oEOHDuU/9+vXj7179/KPf/yDt99+22Cy+jVu3Di2b9/OqlWrTEfxeVXdV8H8ntWhQwfS0tLIyspi9uzZ3HfffaxYsaLSQmJCwK6MNGrUCKfTybFjxypcf+zYMRISEi54n4SEhGptHyhqsq++LTQ0lO7du7Nnz566iOjXKntdud1uIiIiDKXyH7169Qqq19X48eP55JNPWLZsGS1atLjotsH6nnVOdfbVtwXTe1ZYWBjt27enR48eTJo0ia5du/LSSy9dcFtTr6mALSNhYWH06NGDJUuWlF/n9XpZsmRJpZ+V9e3bt8L2AIsWLap0+0BRk331bR6Ph23btpGYmFhXMf1WsL6uaktaWlpQvK5s22b8+PHMnTuXpUuX0qZNm0veJ1hfWzXZV98WzO9ZXq+XoqKiC95m7DVVp4fHGjZz5kzb5XLZM2bMsHfu3Gk/9NBDdlxcnJ2RkWHbtm3/+Mc/tp999tny7VevXm2HhITYf/vb3+wvvvjC/v3vf2+Hhoba27ZtM/UU6k1199Vzzz1np6am2nv37rU3b95s33XXXXZ4eLi9Y8cOU0+h3uTk5Nhbt261t27dagP23//+d3vr1q32wYMHbdu27Weffdb+8Y9/XL79vn377MjISPvpp5+2v/jiC/vVV1+1nU6nnZKSYuop1Jvq7qt//OMf9rx58+zdu3fb27Ztsx9//HHb4XDYixcvNvUU6s2jjz5qx8bG2suXL7fT09PLL/n5+eXb6D2rTE32VbC+Zz377LP2ihUr7P3799uff/65/eyzz9qWZdkLFy60bdt3XlMBXUZs27Zffvllu2XLlnZYWJjdq1cve926deW3DRgwwL7vvvsqbP/+++/bV155pR0WFmZfc8019vz58+s5sTnV2VcTJkwo37Zp06b2qFGj7C1bthhIXf/Off3025dz++e+++6zBwwY8J37dOvWzQ4LC7Pbtm1rv/nmm/We24Tq7qv/+7//s9u1a2eHh4fb8fHx9sCBA+2lS5eaCV/PLrSfgAqvFb1nlanJvgrW96yf/vSndqtWreywsDC7cePG9pAhQ8qLiG37zmvKsm3brtu1FxEREZHKBewxIyIiIuIfVEZERETEKJURERERMUplRERERIxSGRERERGjVEZERETEKJURERERMUplRERERIxSGRERERGjVEZERETEKJURERERMUplRERERIz6/1ws/jHx1SXFAAAAAElFTkSuQmCC\n"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# Metric for XGB - not used\n'''\ndef quadratic_weighted_kappa_sii(y_true, y_pred):\n    if isinstance(y_pred, xgb.QuantileDMatrix):\n        # XGB\n        y_true, y_pred = y_pred, y_true\n\n        y_true = (y_true.get_label() + c).round()\n        y_pred = (y_pred + c).clip(0, 3).round()\n        qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n        return 'QWK', qwk\n\n    else:\n        # For LightGBM\n        y_true = y_true + c\n        y_pred = (y_pred + c).clip(0, 3).round()\n        qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n        return 'QWK', qwk, True\n'''\n\n# Objective function for both XGB and LGB\ndef qwk_obj_sii(y_true, y_pred):\n    labels = y_true + c\n    preds = y_pred + c\n    preds = preds.clip(0, 3)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-c)**2+d)\n    df = preds - labels\n    dg = preds - c\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:54:56.503617Z","iopub.execute_input":"2025-01-26T05:54:56.504019Z","iopub.status.idle":"2025-01-26T05:54:56.511684Z","shell.execute_reply.started":"2025-01-26T05:54:56.503984Z","shell.execute_reply":"2025-01-26T05:54:56.510198Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## DNN custom QWK\n\nDNN y_true and y_pred are passed as tensors, unlike for XGBoost. So different definitions are required.","metadata":{}},{"cell_type":"code","source":"# Weight matrix for QWK calculation\nW = np.zeros((4, 4))\nfor i in range(len(W)):\n    for j in range(len(W)):\n        W[i][j] = float(((i-j)**2)/((4)-1)**2)\n\nprint(W)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:54:59.350773Z","iopub.execute_input":"2025-01-26T05:54:59.351142Z","iopub.status.idle":"2025-01-26T05:54:59.358783Z","shell.execute_reply.started":"2025-01-26T05:54:59.351111Z","shell.execute_reply":"2025-01-26T05:54:59.357348Z"}},"outputs":[{"name":"stdout","text":"[[0.         0.11111111 0.44444444 1.        ]\n [0.11111111 0.         0.11111111 0.44444444]\n [0.44444444 0.11111111 0.         0.11111111]\n [1.         0.44444444 0.11111111 0.        ]]\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Given a confusion matrix, compute the expected matrix from the outer product of the row and column frequencies\ndef compute_expected_matrix(conf_matrix):\n    row_sums = tf.reduce_sum(conf_matrix, axis=1, keepdims=True)\n    col_sums = tf.reduce_sum(conf_matrix, axis=0, keepdims=True)\n    \n    # Normalize the row and column sums by the total sum of the confusion matrix\n    total_sum = tf.reduce_sum(conf_matrix)\n    \n    # Compute the expected matrix by multiplying the row and column marginals and normalizing\n    expected_matrix = (row_sums @ col_sums) / total_sum\n    \n    return expected_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:55:02.017705Z","iopub.execute_input":"2025-01-26T05:55:02.018077Z","iopub.status.idle":"2025-01-26T05:55:02.023912Z","shell.execute_reply.started":"2025-01-26T05:55:02.018048Z","shell.execute_reply":"2025-01-26T05:55:02.022557Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Metric for tracking model as it is trained\n# Writing own QWK score function. Gradients not necessary as this is not the loss function\ndef qwk_sii(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred, 0, 3)\n    y_true = tf.squeeze(y_true)\n    y_pred = tf.squeeze(y_pred)\n\n    y_true = tf.round(y_true)\n    y_pred = tf.round(y_pred)\n\n    confusion_matrix = tf.math.confusion_matrix(tf.cast(y_true, tf.int32), tf.cast(y_pred, tf.int32), num_classes=4)    \n    expected_matrix = compute_expected_matrix(confusion_matrix)\n    #print(confusion_matrix)\n    confusion_matrix = confusion_matrix / tf.reduce_sum(confusion_matrix)\n    expected_matrix = expected_matrix / tf.reduce_sum(expected_matrix)\n    weight_matrix = tf.cast(tf.constant(W),tf.float32) # W defined for 4 classes\n    numerator = tf.reduce_sum(weight_matrix * tf.cast(confusion_matrix, tf.float32))\n    denominator = tf.reduce_sum(weight_matrix * tf.cast(expected_matrix, tf.float32))\n\n    weighted_kappa = 1 - (numerator / denominator)\n\n    #print(confusion_matrix)\n    #print(expected_matrix)\n    #print(weight_matrix)\n\n    #E = np.outer(W, confusion)\n\n    return weighted_kappa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:55:05.233655Z","iopub.execute_input":"2025-01-26T05:55:05.234189Z","iopub.status.idle":"2025-01-26T05:55:05.242183Z","shell.execute_reply.started":"2025-01-26T05:55:05.234149Z","shell.execute_reply":"2025-01-26T05:55:05.240459Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"@tf.custom_gradient\ndef custom_confusion_matrix_loss(y_true, y_pred):\n    # Clip predictions to the range [0, 3] to calculate confusion matrix\n    # Not overwriting y_true and y_pred so they can be passed as is to the gradient func\n    y_pred = tf.clip_by_value(y_pred, 0, 3)\n    y_true2 = tf.squeeze(y_true)  # Flatten to 1D, e.g., expecting (64,) instead of (64,1)\n    y_pred2 = tf.squeeze(y_pred)  # Flatten to 1D\n    \n    # Round the true and predicted values: 0, 1, 2, 3\n    y_true2 = tf.round(y_true2)\n    y_pred2 = tf.round(y_pred2)\n\n    # Compute the confusion matrix O\n    confusion_matrix = tf.math.confusion_matrix(tf.cast(y_true2, tf.int32), tf.cast(y_pred2, tf.int32), num_classes=4)\n\n    # Compute the expected matrix E\n    expected_matrix = compute_expected_matrix(confusion_matrix)\n\n    # Normalize confusion matrix and expected matrix\n    confusion_matrix = confusion_matrix / tf.reduce_sum(confusion_matrix)\n    expected_matrix = expected_matrix / tf.reduce_sum(expected_matrix)\n\n    # Multiply by weight matrix\n    numerator = tf.reduce_sum(W * tf.cast(confusion_matrix, tf.float32))\n    denominator = tf.reduce_sum(W * tf.cast(expected_matrix, tf.float32))\n\n    weighted_kappa = 1 - numerator / denominator\n    \n    # Forward pass (return the loss)\n    def grad(dy):\n        # Approximate gradient function using c and d determined previously\n        # Referring to: https://medium.com/@nlztrk/quadratic-weighted-kappa-qwk-metric-and-how-to-optimize-it-062cc9121baa\n        labels = y_true + c\n        preds = y_pred + c\n        #print(labels.shape, preds.shape)\n        preds = tf.clip_by_value(preds, 0, 3)\n        #print(labels.shape, preds.shape)\n        f = 1 / 2 * tf.reduce_sum((preds - labels) ** 2)\n        #print(\"f: \", f)\n        g = 1 / 2 * tf.reduce_sum((preds - c) ** 2 + d)\n        #print(\"g: \", g)\n\n        df = preds - labels\n        #print(\"df: \", df)\n        dg = preds - c\n        #print(\"dg: \", dg)\n        grad = (df / g - f * dg / g ** 2) * tf.cast(tf.size(labels), tf.float32)\n        #print(\"grad: \", grad)\n        #grad = (df/g - f*dg/g**2)*len()\n\n        grad_loss = dy * (grad) #Passing gradient multiplication\n\n        # Experienced problems with exploding gradients, so clipping\n        # Can clip all gradients to -1,1, or scale them by a consistent factor\n        #grad_loss = tf.clip_by_value(grad_loss, -1, 1)\n        clipping_factor = tf.reduce_max(tf.abs(grad_loss)) # Max gradient change\n        #print(\"clipping factor: \", clipping_factor)\n\n        #if clipping_factor>1:\n        #    grad_loss = grad_loss / clipping_factor\n\n        grad_loss = grad_loss / clipping_factor\n\n        #return grad, grad\n        #print(\"grad loss: \", grad_loss)\n        #print(\"shapes: \", (y_true.shape, y_pred.shape), (grad_loss.shape, grad_loss.shape))\n        return grad_loss, grad_loss  # Gradients for y_true, y_pred\n\n    return (1 - weighted_kappa), grad # (1 - weighted_kappa) to treat as a loss function\n    #return weighted_kappa, grad","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:55:05.929588Z","iopub.execute_input":"2025-01-26T05:55:05.929952Z","iopub.status.idle":"2025-01-26T05:55:05.939795Z","shell.execute_reply.started":"2025-01-26T05:55:05.929922Z","shell.execute_reply":"2025-01-26T05:55:05.938441Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def quadratic_weighted_kappa_sii(y_true, y_pred):\n    return custom_confusion_matrix_loss(y_true, y_pred)\n\nclass QWKLoss(tf.keras.losses.Loss):\n    def __init__(self, name=\"QWKLoss\"):\n        super().__init__(name=name)\n    \n    def call(self, y_true, y_pred):\n        return quadratic_weighted_kappa_sii(y_true, y_pred)\n        #return soft_quadratic_weighted_kappa(y_true, y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:55:18.963613Z","iopub.execute_input":"2025-01-26T05:55:18.963970Z","iopub.status.idle":"2025-01-26T05:55:18.969987Z","shell.execute_reply.started":"2025-01-26T05:55:18.963943Z","shell.execute_reply":"2025-01-26T05:55:18.968878Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"# QWK objective function\nxgb_model = XGBRegressor(random_state=42, objective=qwk_obj_sii, learning_rate=0.05, max_depth=3, min_child_weight=5, n_estimators=100, subsample=0.7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:55:55.616127Z","iopub.execute_input":"2025-01-26T05:55:55.616590Z","iopub.status.idle":"2025-01-26T05:55:55.622229Z","shell.execute_reply.started":"2025-01-26T05:55:55.616544Z","shell.execute_reply":"2025-01-26T05:55:55.620831Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Train on non-augmented data, with exponential weights\nxgb_model.fit(X_train_labelled, y_train_labelled_sii, sample_weight=weights_labelled3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:55:58.118000Z","iopub.execute_input":"2025-01-26T05:55:58.118359Z","iopub.status.idle":"2025-01-26T05:55:58.554765Z","shell.execute_reply.started":"2025-01-26T05:55:58.118315Z","shell.execute_reply":"2025-01-26T05:55:58.553810Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=3, max_leaves=None,\n             min_child_weight=5, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=100, n_jobs=None,\n             num_parallel_tree=None,\n             objective=<function qwk_obj_sii at 0x7bfec80b1c60>, ...)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=3, max_leaves=None,\n             min_child_weight=5, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=100, n_jobs=None,\n             num_parallel_tree=None,\n             objective=&lt;function qwk_obj_sii at 0x7bfec80b1c60&gt;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=3, max_leaves=None,\n             min_child_weight=5, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=100, n_jobs=None,\n             num_parallel_tree=None,\n             objective=&lt;function qwk_obj_sii at 0x7bfec80b1c60&gt;, ...)</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"## LightGBM","metadata":{}},{"cell_type":"code","source":"lgbm_model = lgb.LGBMRegressor(objective=qwk_obj_sii, metric='l2', boosting_type='gbdt',\n                              num_leaves=31, learning_rate=0.01, n_estimators=300,\n                              subsample=0.8, colsample_bytree=0.8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:56:01.485964Z","iopub.execute_input":"2025-01-26T05:56:01.486325Z","iopub.status.idle":"2025-01-26T05:56:01.491753Z","shell.execute_reply.started":"2025-01-26T05:56:01.486297Z","shell.execute_reply":"2025-01-26T05:56:01.490397Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"lgbm_model.fit(X_train_labelled_aug2b,y_train_labelled_sii_aug2, sample_weight=weights_labelled3_aug2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:58:17.401732Z","iopub.execute_input":"2025-01-26T05:58:17.402196Z","iopub.status.idle":"2025-01-26T05:58:18.642548Z","shell.execute_reply.started":"2025-01-26T05:58:17.402159Z","shell.execute_reply":"2025-01-26T05:58:18.641467Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Using self-defined objective function\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004381 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8670\n[LightGBM] [Info] Number of data points in the train set: 8208, number of used features: 34\n[LightGBM] [Info] Using self-defined objective function\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"LGBMRegressor(colsample_bytree=0.8, learning_rate=0.01, metric='l2',\n              n_estimators=300,\n              objective=<function qwk_obj_sii at 0x7bfec80b1c60>,\n              subsample=0.8)","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(colsample_bytree=0.8, learning_rate=0.01, metric=&#x27;l2&#x27;,\n              n_estimators=300,\n              objective=&lt;function qwk_obj_sii at 0x7bfec80b1c60&gt;,\n              subsample=0.8)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor(colsample_bytree=0.8, learning_rate=0.01, metric=&#x27;l2&#x27;,\n              n_estimators=300,\n              objective=&lt;function qwk_obj_sii at 0x7bfec80b1c60&gt;,\n              subsample=0.8)</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":33},{"cell_type":"markdown","source":"## DNN","metadata":{}},{"cell_type":"code","source":"k = 3  # Number of folds for cross-validation\nkf = KFold(n_splits=k, shuffle=True, random_state=42)  # Set shuffle=True to randomize data splits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:00:04.779967Z","iopub.execute_input":"2025-01-26T06:00:04.780437Z","iopub.status.idle":"2025-01-26T06:00:04.786183Z","shell.execute_reply.started":"2025-01-26T06:00:04.780396Z","shell.execute_reply":"2025-01-26T06:00:04.784648Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def create_dnn_model():\n    model = keras.models.Sequential([\n        keras.layers.Dense(24, input_shape=(X_train_labelled_pca.shape[1],), activation=\"selu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(20, activation=\"selu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(1, activation=\"linear\")\n    ])\n\n    model.compile(optimizer=Adam(learning_rate=0.005), loss='mean_squared_error', metrics=['mae','mse'])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:02:00.515046Z","iopub.execute_input":"2025-01-26T06:02:00.515784Z","iopub.status.idle":"2025-01-26T06:02:00.524299Z","shell.execute_reply.started":"2025-01-26T06:02:00.515720Z","shell.execute_reply":"2025-01-26T06:02:00.522872Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Prepare to store results\n#validation_qwk = []\nvalidation_losses = []\ndnn_models = []\n\n# Loop over each fold\nfor train_index, val_index in kf.split(X_train_labelled_pca_aug2b):\n    # Split the data into training and validation sets for the current fold\n    X_train_t, X_train_v = X_train_labelled_pca_aug2b.loc[train_index], X_train_labelled_pca_aug2b.loc[val_index]\n    y_train_t, y_train_v = y_train_labelled_aug2b.loc[train_index], y_train_labelled_aug2b.loc[val_index]\n    \n    # Build a new model for each fold\n    dnn_model = create_dnn_model()\n    \n    # Define early stopping to avoid overfitting\n    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n    \n    # Train the model on the training set and evaluate on the validation set\n    history = dnn_model.fit(\n        X_train_t, y_train_t,\n        validation_data=(X_train_v, y_train_v),\n        epochs=200,\n        batch_size=64,\n        callbacks=[early_stopping],\n        verbose=2\n    )\n    \n    # Evaluate the model on the validation set\n    val_loss, val_mae, val_mse = dnn_model.evaluate(X_train_v, y_train_v, verbose=2)\n    #val_qwk = model.evaluate(X_train_v, y_train_v, verbose=2)\n    #validation_qwk.append(val_qwk)\n    validation_losses.append(val_loss)\n    dnn_models.append(dnn_model)\n\n# Calculate the average validation loss across all folds\n#avg_val_qwk = np.mean(validation_qwk)\navg_val_loss = np.mean(validation_losses)\n#print(f\"Average Validation QWK: {avg_val_qwk}, Average Validation Loss: {avg_val_loss}\")\nprint(f\"Average Validation Loss: {avg_val_loss}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:05:18.238951Z","iopub.execute_input":"2025-01-26T06:05:18.239508Z","iopub.status.idle":"2025-01-26T06:08:02.530904Z","shell.execute_reply.started":"2025-01-26T06:05:18.239466Z","shell.execute_reply":"2025-01-26T06:08:02.529183Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/200\n86/86 - 2s - 28ms/step - loss: 639.5471 - mae: 19.7692 - mse: 639.5471 - val_loss: 378.8513 - val_mae: 15.3269 - val_mse: 378.8513\nEpoch 2/200\n86/86 - 0s - 4ms/step - loss: 409.9516 - mae: 15.9110 - mse: 409.9516 - val_loss: 329.9095 - val_mae: 14.3369 - val_mse: 329.9095\nEpoch 3/200\n86/86 - 0s - 4ms/step - loss: 374.6810 - mae: 15.2284 - mse: 374.6810 - val_loss: 322.0824 - val_mae: 14.1733 - val_mse: 322.0824\nEpoch 4/200\n86/86 - 0s - 3ms/step - loss: 355.7268 - mae: 14.9629 - mse: 355.7268 - val_loss: 311.0022 - val_mae: 13.9946 - val_mse: 311.0022\nEpoch 5/200\n86/86 - 0s - 4ms/step - loss: 340.9935 - mae: 14.5757 - mse: 340.9935 - val_loss: 311.4916 - val_mae: 14.0167 - val_mse: 311.4916\nEpoch 6/200\n86/86 - 0s - 4ms/step - loss: 339.3228 - mae: 14.5881 - mse: 339.3228 - val_loss: 307.4771 - val_mae: 13.9532 - val_mse: 307.4771\nEpoch 7/200\n86/86 - 0s - 4ms/step - loss: 334.8826 - mae: 14.4532 - mse: 334.8826 - val_loss: 304.5017 - val_mae: 13.9409 - val_mse: 304.5017\nEpoch 8/200\n86/86 - 0s - 4ms/step - loss: 333.6020 - mae: 14.4082 - mse: 333.6020 - val_loss: 302.3090 - val_mae: 13.9019 - val_mse: 302.3090\nEpoch 9/200\n86/86 - 0s - 4ms/step - loss: 324.5490 - mae: 14.2529 - mse: 324.5490 - val_loss: 298.1984 - val_mae: 13.8436 - val_mse: 298.1984\nEpoch 10/200\n86/86 - 0s - 4ms/step - loss: 322.1158 - mae: 14.2113 - mse: 322.1158 - val_loss: 303.3455 - val_mae: 13.9048 - val_mse: 303.3455\nEpoch 11/200\n86/86 - 0s - 4ms/step - loss: 326.0882 - mae: 14.1916 - mse: 326.0882 - val_loss: 300.3289 - val_mae: 13.8495 - val_mse: 300.3289\nEpoch 12/200\n86/86 - 0s - 3ms/step - loss: 321.3858 - mae: 14.2579 - mse: 321.3858 - val_loss: 295.2364 - val_mae: 13.7776 - val_mse: 295.2364\nEpoch 13/200\n86/86 - 0s - 3ms/step - loss: 317.6404 - mae: 14.1190 - mse: 317.6404 - val_loss: 300.5725 - val_mae: 13.8677 - val_mse: 300.5725\nEpoch 14/200\n86/86 - 0s - 4ms/step - loss: 318.2912 - mae: 14.1201 - mse: 318.2912 - val_loss: 298.3545 - val_mae: 13.8388 - val_mse: 298.3545\nEpoch 15/200\n86/86 - 0s - 4ms/step - loss: 322.3674 - mae: 14.2414 - mse: 322.3674 - val_loss: 297.2741 - val_mae: 13.8022 - val_mse: 297.2741\nEpoch 16/200\n86/86 - 0s - 3ms/step - loss: 316.1003 - mae: 14.1068 - mse: 316.1003 - val_loss: 294.2437 - val_mae: 13.7836 - val_mse: 294.2437\nEpoch 17/200\n86/86 - 0s - 3ms/step - loss: 312.3115 - mae: 14.0219 - mse: 312.3115 - val_loss: 292.4196 - val_mae: 13.7204 - val_mse: 292.4196\nEpoch 18/200\n86/86 - 0s - 4ms/step - loss: 315.4928 - mae: 14.0841 - mse: 315.4928 - val_loss: 294.5027 - val_mae: 13.7667 - val_mse: 294.5027\nEpoch 19/200\n86/86 - 0s - 4ms/step - loss: 311.3203 - mae: 14.0372 - mse: 311.3203 - val_loss: 294.8068 - val_mae: 13.7970 - val_mse: 294.8068\nEpoch 20/200\n86/86 - 0s - 3ms/step - loss: 307.3256 - mae: 13.9325 - mse: 307.3256 - val_loss: 294.0793 - val_mae: 13.7419 - val_mse: 294.0793\nEpoch 21/200\n86/86 - 0s - 3ms/step - loss: 312.1339 - mae: 13.9957 - mse: 312.1339 - val_loss: 294.4565 - val_mae: 13.7312 - val_mse: 294.4565\nEpoch 22/200\n86/86 - 0s - 3ms/step - loss: 309.0285 - mae: 13.9205 - mse: 309.0285 - val_loss: 293.5967 - val_mae: 13.7441 - val_mse: 293.5967\nEpoch 23/200\n86/86 - 0s - 3ms/step - loss: 308.0242 - mae: 13.9044 - mse: 308.0242 - val_loss: 292.6685 - val_mae: 13.7236 - val_mse: 292.6685\nEpoch 24/200\n86/86 - 0s - 3ms/step - loss: 304.0077 - mae: 13.8489 - mse: 304.0077 - val_loss: 292.8058 - val_mae: 13.6890 - val_mse: 292.8058\nEpoch 25/200\n86/86 - 0s - 4ms/step - loss: 305.8690 - mae: 13.8750 - mse: 305.8690 - val_loss: 291.4925 - val_mae: 13.7269 - val_mse: 291.4925\nEpoch 26/200\n86/86 - 0s - 3ms/step - loss: 309.3868 - mae: 13.9238 - mse: 309.3868 - val_loss: 289.2627 - val_mae: 13.6967 - val_mse: 289.2627\nEpoch 27/200\n86/86 - 0s - 3ms/step - loss: 303.6042 - mae: 13.7608 - mse: 303.6042 - val_loss: 287.4588 - val_mae: 13.6075 - val_mse: 287.4588\nEpoch 28/200\n86/86 - 0s - 3ms/step - loss: 302.9758 - mae: 13.7702 - mse: 302.9758 - val_loss: 289.1468 - val_mae: 13.6169 - val_mse: 289.1468\nEpoch 29/200\n86/86 - 0s - 4ms/step - loss: 299.5953 - mae: 13.7727 - mse: 299.5953 - val_loss: 292.2839 - val_mae: 13.7050 - val_mse: 292.2839\nEpoch 30/200\n86/86 - 0s - 3ms/step - loss: 300.9510 - mae: 13.7692 - mse: 300.9510 - val_loss: 286.6563 - val_mae: 13.5958 - val_mse: 286.6563\nEpoch 31/200\n86/86 - 0s - 3ms/step - loss: 298.8280 - mae: 13.7576 - mse: 298.8280 - val_loss: 289.6309 - val_mae: 13.6224 - val_mse: 289.6309\nEpoch 32/200\n86/86 - 0s - 3ms/step - loss: 301.6750 - mae: 13.7409 - mse: 301.6750 - val_loss: 286.4877 - val_mae: 13.5690 - val_mse: 286.4877\nEpoch 33/200\n86/86 - 0s - 3ms/step - loss: 299.9128 - mae: 13.8125 - mse: 299.9128 - val_loss: 284.6801 - val_mae: 13.5612 - val_mse: 284.6801\nEpoch 34/200\n86/86 - 0s - 3ms/step - loss: 301.8268 - mae: 13.7469 - mse: 301.8268 - val_loss: 287.6143 - val_mae: 13.6040 - val_mse: 287.6143\nEpoch 35/200\n86/86 - 0s - 3ms/step - loss: 300.3762 - mae: 13.7729 - mse: 300.3762 - val_loss: 283.8841 - val_mae: 13.5437 - val_mse: 283.8841\nEpoch 36/200\n86/86 - 0s - 3ms/step - loss: 293.3770 - mae: 13.5707 - mse: 293.3770 - val_loss: 282.8447 - val_mae: 13.5056 - val_mse: 282.8447\nEpoch 37/200\n86/86 - 0s - 3ms/step - loss: 295.4557 - mae: 13.6072 - mse: 295.4557 - val_loss: 283.7931 - val_mae: 13.5337 - val_mse: 283.7931\nEpoch 38/200\n86/86 - 0s - 3ms/step - loss: 302.2193 - mae: 13.7417 - mse: 302.2193 - val_loss: 284.7955 - val_mae: 13.5126 - val_mse: 284.7955\nEpoch 39/200\n86/86 - 0s - 3ms/step - loss: 296.5606 - mae: 13.6633 - mse: 296.5606 - val_loss: 285.1373 - val_mae: 13.5150 - val_mse: 285.1373\nEpoch 40/200\n86/86 - 0s - 3ms/step - loss: 296.7466 - mae: 13.6092 - mse: 296.7466 - val_loss: 280.2106 - val_mae: 13.4064 - val_mse: 280.2106\nEpoch 41/200\n86/86 - 0s - 3ms/step - loss: 290.5201 - mae: 13.4825 - mse: 290.5201 - val_loss: 283.8264 - val_mae: 13.5034 - val_mse: 283.8264\nEpoch 42/200\n86/86 - 0s - 3ms/step - loss: 295.5510 - mae: 13.6172 - mse: 295.5510 - val_loss: 284.8500 - val_mae: 13.5245 - val_mse: 284.8500\nEpoch 43/200\n86/86 - 0s - 3ms/step - loss: 290.0764 - mae: 13.5047 - mse: 290.0764 - val_loss: 281.8715 - val_mae: 13.4359 - val_mse: 281.8715\nEpoch 44/200\n86/86 - 0s - 4ms/step - loss: 298.8902 - mae: 13.7787 - mse: 298.8902 - val_loss: 284.8970 - val_mae: 13.5066 - val_mse: 284.8970\nEpoch 45/200\n86/86 - 0s - 4ms/step - loss: 291.4489 - mae: 13.5770 - mse: 291.4489 - val_loss: 280.2322 - val_mae: 13.4258 - val_mse: 280.2322\nEpoch 46/200\n86/86 - 0s - 4ms/step - loss: 291.2517 - mae: 13.4625 - mse: 291.2517 - val_loss: 285.8011 - val_mae: 13.5249 - val_mse: 285.8011\nEpoch 47/200\n86/86 - 0s - 4ms/step - loss: 286.6914 - mae: 13.3757 - mse: 286.6914 - val_loss: 283.1711 - val_mae: 13.4737 - val_mse: 283.1711\nEpoch 48/200\n86/86 - 0s - 4ms/step - loss: 286.3915 - mae: 13.3646 - mse: 286.3915 - val_loss: 280.1591 - val_mae: 13.4233 - val_mse: 280.1591\nEpoch 49/200\n86/86 - 0s - 4ms/step - loss: 286.4647 - mae: 13.4327 - mse: 286.4647 - val_loss: 281.6008 - val_mae: 13.4551 - val_mse: 281.6008\nEpoch 50/200\n86/86 - 0s - 4ms/step - loss: 289.2320 - mae: 13.4783 - mse: 289.2320 - val_loss: 280.3112 - val_mae: 13.4335 - val_mse: 280.3112\nEpoch 51/200\n86/86 - 0s - 4ms/step - loss: 291.4568 - mae: 13.5053 - mse: 291.4568 - val_loss: 279.8869 - val_mae: 13.4204 - val_mse: 279.8869\nEpoch 52/200\n86/86 - 0s - 4ms/step - loss: 284.3369 - mae: 13.3493 - mse: 284.3369 - val_loss: 280.0656 - val_mae: 13.4073 - val_mse: 280.0656\nEpoch 53/200\n86/86 - 0s - 3ms/step - loss: 286.1075 - mae: 13.3873 - mse: 286.1075 - val_loss: 279.4941 - val_mae: 13.3585 - val_mse: 279.4941\nEpoch 54/200\n86/86 - 0s - 4ms/step - loss: 283.8586 - mae: 13.2975 - mse: 283.8586 - val_loss: 279.2843 - val_mae: 13.3972 - val_mse: 279.2843\nEpoch 55/200\n86/86 - 0s - 4ms/step - loss: 287.5067 - mae: 13.3759 - mse: 287.5067 - val_loss: 277.0159 - val_mae: 13.3193 - val_mse: 277.0159\nEpoch 56/200\n86/86 - 0s - 4ms/step - loss: 287.0281 - mae: 13.4238 - mse: 287.0281 - val_loss: 278.5295 - val_mae: 13.3751 - val_mse: 278.5295\nEpoch 57/200\n86/86 - 0s - 4ms/step - loss: 280.1440 - mae: 13.2793 - mse: 280.1440 - val_loss: 277.3436 - val_mae: 13.3476 - val_mse: 277.3436\nEpoch 58/200\n86/86 - 0s - 4ms/step - loss: 282.8329 - mae: 13.3210 - mse: 282.8329 - val_loss: 279.3289 - val_mae: 13.3953 - val_mse: 279.3289\nEpoch 59/200\n86/86 - 0s - 4ms/step - loss: 280.9680 - mae: 13.2394 - mse: 280.9680 - val_loss: 277.7469 - val_mae: 13.3321 - val_mse: 277.7469\nEpoch 60/200\n86/86 - 0s - 4ms/step - loss: 275.6313 - mae: 13.1903 - mse: 275.6313 - val_loss: 276.4244 - val_mae: 13.3190 - val_mse: 276.4244\nEpoch 61/200\n86/86 - 0s - 4ms/step - loss: 284.2083 - mae: 13.3680 - mse: 284.2083 - val_loss: 276.9512 - val_mae: 13.3095 - val_mse: 276.9512\nEpoch 62/200\n86/86 - 0s - 4ms/step - loss: 280.3882 - mae: 13.2909 - mse: 280.3882 - val_loss: 279.1399 - val_mae: 13.3605 - val_mse: 279.1399\nEpoch 63/200\n86/86 - 0s - 4ms/step - loss: 277.7683 - mae: 13.2157 - mse: 277.7683 - val_loss: 278.9535 - val_mae: 13.3717 - val_mse: 278.9535\nEpoch 64/200\n86/86 - 0s - 4ms/step - loss: 278.2972 - mae: 13.1582 - mse: 278.2972 - val_loss: 277.1027 - val_mae: 13.3119 - val_mse: 277.1027\nEpoch 65/200\n86/86 - 0s - 4ms/step - loss: 280.8571 - mae: 13.3215 - mse: 280.8571 - val_loss: 277.8285 - val_mae: 13.4124 - val_mse: 277.8285\nEpoch 66/200\n86/86 - 0s - 5ms/step - loss: 282.3319 - mae: 13.3249 - mse: 282.3319 - val_loss: 276.7465 - val_mae: 13.3465 - val_mse: 276.7465\nEpoch 67/200\n86/86 - 0s - 4ms/step - loss: 278.1632 - mae: 13.1819 - mse: 278.1632 - val_loss: 274.8501 - val_mae: 13.3187 - val_mse: 274.8501\nEpoch 68/200\n86/86 - 0s - 3ms/step - loss: 277.5390 - mae: 13.2303 - mse: 277.5390 - val_loss: 273.2977 - val_mae: 13.2869 - val_mse: 273.2977\nEpoch 69/200\n86/86 - 0s - 4ms/step - loss: 277.4556 - mae: 13.2270 - mse: 277.4556 - val_loss: 276.2404 - val_mae: 13.3093 - val_mse: 276.2404\nEpoch 70/200\n86/86 - 0s - 3ms/step - loss: 278.2594 - mae: 13.1565 - mse: 278.2594 - val_loss: 271.5759 - val_mae: 13.2468 - val_mse: 271.5759\nEpoch 71/200\n86/86 - 0s - 3ms/step - loss: 276.4238 - mae: 13.2119 - mse: 276.4238 - val_loss: 273.1817 - val_mae: 13.2378 - val_mse: 273.1817\nEpoch 72/200\n86/86 - 0s - 3ms/step - loss: 273.2967 - mae: 13.0834 - mse: 273.2967 - val_loss: 270.8715 - val_mae: 13.2235 - val_mse: 270.8715\nEpoch 73/200\n86/86 - 0s - 4ms/step - loss: 278.3232 - mae: 13.1256 - mse: 278.3232 - val_loss: 270.7355 - val_mae: 13.2008 - val_mse: 270.7355\nEpoch 74/200\n86/86 - 0s - 4ms/step - loss: 275.0134 - mae: 13.1285 - mse: 275.0134 - val_loss: 268.7741 - val_mae: 13.1986 - val_mse: 268.7741\nEpoch 75/200\n86/86 - 0s - 4ms/step - loss: 273.3224 - mae: 13.1574 - mse: 273.3224 - val_loss: 270.6738 - val_mae: 13.1963 - val_mse: 270.6738\nEpoch 76/200\n86/86 - 0s - 3ms/step - loss: 274.3423 - mae: 13.1247 - mse: 274.3423 - val_loss: 271.6643 - val_mae: 13.1789 - val_mse: 271.6643\nEpoch 77/200\n86/86 - 0s - 3ms/step - loss: 277.4556 - mae: 13.2026 - mse: 277.4556 - val_loss: 270.1639 - val_mae: 13.1961 - val_mse: 270.1639\nEpoch 78/200\n86/86 - 0s - 3ms/step - loss: 274.6145 - mae: 13.1963 - mse: 274.6145 - val_loss: 272.3123 - val_mae: 13.2185 - val_mse: 272.3123\nEpoch 79/200\n86/86 - 0s - 3ms/step - loss: 274.3487 - mae: 13.1284 - mse: 274.3487 - val_loss: 268.8143 - val_mae: 13.1802 - val_mse: 268.8143\nEpoch 80/200\n86/86 - 0s - 3ms/step - loss: 272.3511 - mae: 13.0514 - mse: 272.3511 - val_loss: 269.9053 - val_mae: 13.2049 - val_mse: 269.9053\nEpoch 81/200\n86/86 - 0s - 3ms/step - loss: 273.6495 - mae: 13.0997 - mse: 273.6495 - val_loss: 271.0317 - val_mae: 13.2190 - val_mse: 271.0317\nEpoch 82/200\n86/86 - 0s - 3ms/step - loss: 273.9002 - mae: 13.0793 - mse: 273.9002 - val_loss: 270.6409 - val_mae: 13.1839 - val_mse: 270.6409\nEpoch 83/200\n86/86 - 0s - 4ms/step - loss: 275.6654 - mae: 13.1161 - mse: 275.6654 - val_loss: 269.7926 - val_mae: 13.1540 - val_mse: 269.7926\nEpoch 84/200\n86/86 - 0s - 3ms/step - loss: 274.9164 - mae: 13.1457 - mse: 274.9164 - val_loss: 269.6177 - val_mae: 13.1531 - val_mse: 269.6177\nEpoch 85/200\n86/86 - 0s - 4ms/step - loss: 275.4536 - mae: 13.1184 - mse: 275.4536 - val_loss: 269.3993 - val_mae: 13.1889 - val_mse: 269.3993\nEpoch 86/200\n86/86 - 0s - 4ms/step - loss: 276.1623 - mae: 13.1747 - mse: 276.1623 - val_loss: 270.0016 - val_mae: 13.1488 - val_mse: 270.0016\nEpoch 87/200\n86/86 - 0s - 4ms/step - loss: 271.8981 - mae: 13.0283 - mse: 271.8981 - val_loss: 268.4324 - val_mae: 13.1550 - val_mse: 268.4324\nEpoch 88/200\n86/86 - 0s - 3ms/step - loss: 273.8840 - mae: 13.1160 - mse: 273.8840 - val_loss: 269.3038 - val_mae: 13.1918 - val_mse: 269.3038\nEpoch 89/200\n86/86 - 0s - 4ms/step - loss: 274.3965 - mae: 13.1184 - mse: 274.3965 - val_loss: 267.8889 - val_mae: 13.1349 - val_mse: 267.8889\nEpoch 90/200\n86/86 - 0s - 3ms/step - loss: 270.3459 - mae: 13.0336 - mse: 270.3459 - val_loss: 267.0206 - val_mae: 13.0996 - val_mse: 267.0206\nEpoch 91/200\n86/86 - 0s - 3ms/step - loss: 272.1861 - mae: 12.9896 - mse: 272.1861 - val_loss: 268.7090 - val_mae: 13.1097 - val_mse: 268.7090\nEpoch 92/200\n86/86 - 0s - 3ms/step - loss: 268.0748 - mae: 12.9753 - mse: 268.0748 - val_loss: 268.4920 - val_mae: 13.1395 - val_mse: 268.4920\nEpoch 93/200\n86/86 - 0s - 3ms/step - loss: 269.8133 - mae: 12.9587 - mse: 269.8133 - val_loss: 266.2927 - val_mae: 13.1163 - val_mse: 266.2927\nEpoch 94/200\n86/86 - 0s - 4ms/step - loss: 269.3992 - mae: 12.9736 - mse: 269.3992 - val_loss: 264.8688 - val_mae: 13.0613 - val_mse: 264.8688\nEpoch 95/200\n86/86 - 0s - 3ms/step - loss: 271.2785 - mae: 13.1507 - mse: 271.2785 - val_loss: 267.9743 - val_mae: 13.1046 - val_mse: 267.9743\nEpoch 96/200\n86/86 - 0s - 3ms/step - loss: 270.8097 - mae: 13.0123 - mse: 270.8097 - val_loss: 267.4247 - val_mae: 13.0733 - val_mse: 267.4247\nEpoch 97/200\n86/86 - 0s - 3ms/step - loss: 270.1706 - mae: 12.9432 - mse: 270.1706 - val_loss: 264.7867 - val_mae: 12.9987 - val_mse: 264.7867\nEpoch 98/200\n86/86 - 0s - 3ms/step - loss: 266.5804 - mae: 12.9012 - mse: 266.5804 - val_loss: 264.3575 - val_mae: 12.9818 - val_mse: 264.3575\nEpoch 99/200\n86/86 - 0s - 4ms/step - loss: 265.2511 - mae: 12.9067 - mse: 265.2511 - val_loss: 265.8085 - val_mae: 13.0542 - val_mse: 265.8085\nEpoch 100/200\n86/86 - 0s - 3ms/step - loss: 267.5271 - mae: 12.9365 - mse: 267.5271 - val_loss: 262.1378 - val_mae: 12.9532 - val_mse: 262.1378\nEpoch 101/200\n86/86 - 0s - 3ms/step - loss: 264.3942 - mae: 12.9028 - mse: 264.3942 - val_loss: 264.3683 - val_mae: 13.0024 - val_mse: 264.3683\nEpoch 102/200\n86/86 - 0s - 3ms/step - loss: 266.4276 - mae: 13.0294 - mse: 266.4276 - val_loss: 264.2770 - val_mae: 12.9737 - val_mse: 264.2770\nEpoch 103/200\n86/86 - 0s - 3ms/step - loss: 270.6270 - mae: 12.9828 - mse: 270.6270 - val_loss: 265.5175 - val_mae: 13.0270 - val_mse: 265.5175\nEpoch 104/200\n86/86 - 0s - 3ms/step - loss: 268.8419 - mae: 12.9854 - mse: 268.8419 - val_loss: 263.7225 - val_mae: 12.9894 - val_mse: 263.7225\nEpoch 105/200\n86/86 - 0s - 3ms/step - loss: 272.6377 - mae: 13.0471 - mse: 272.6377 - val_loss: 267.4178 - val_mae: 13.0407 - val_mse: 267.4178\nEpoch 106/200\n86/86 - 0s - 3ms/step - loss: 265.5381 - mae: 12.8579 - mse: 265.5381 - val_loss: 266.6843 - val_mae: 13.0256 - val_mse: 266.6843\nEpoch 107/200\n86/86 - 0s - 5ms/step - loss: 268.7175 - mae: 13.0015 - mse: 268.7175 - val_loss: 263.9500 - val_mae: 13.0157 - val_mse: 263.9500\nEpoch 108/200\n86/86 - 0s - 4ms/step - loss: 264.2437 - mae: 12.8681 - mse: 264.2437 - val_loss: 262.5118 - val_mae: 12.8743 - val_mse: 262.5118\nEpoch 109/200\n86/86 - 0s - 4ms/step - loss: 265.8295 - mae: 12.9050 - mse: 265.8295 - val_loss: 262.7883 - val_mae: 12.9543 - val_mse: 262.7883\nEpoch 110/200\n86/86 - 0s - 4ms/step - loss: 267.5545 - mae: 12.9711 - mse: 267.5545 - val_loss: 267.7413 - val_mae: 13.0802 - val_mse: 267.7413\nEpoch 111/200\n86/86 - 0s - 4ms/step - loss: 264.2349 - mae: 12.8877 - mse: 264.2349 - val_loss: 263.9103 - val_mae: 12.9909 - val_mse: 263.9103\nEpoch 112/200\n86/86 - 0s - 4ms/step - loss: 265.3991 - mae: 12.9409 - mse: 265.3991 - val_loss: 266.8820 - val_mae: 13.0955 - val_mse: 266.8820\nEpoch 113/200\n86/86 - 0s - 4ms/step - loss: 266.1193 - mae: 12.9451 - mse: 266.1193 - val_loss: 264.4727 - val_mae: 13.0317 - val_mse: 264.4727\nEpoch 114/200\n86/86 - 0s - 3ms/step - loss: 259.1629 - mae: 12.7407 - mse: 259.1629 - val_loss: 264.4079 - val_mae: 13.0279 - val_mse: 264.4079\nEpoch 115/200\n86/86 - 0s - 4ms/step - loss: 261.5393 - mae: 12.8089 - mse: 261.5393 - val_loss: 262.9854 - val_mae: 13.0227 - val_mse: 262.9854\nEpoch 116/200\n86/86 - 0s - 3ms/step - loss: 262.0777 - mae: 12.8515 - mse: 262.0777 - val_loss: 261.8119 - val_mae: 12.9737 - val_mse: 261.8119\nEpoch 117/200\n86/86 - 0s - 3ms/step - loss: 260.3190 - mae: 12.8268 - mse: 260.3190 - val_loss: 261.9148 - val_mae: 12.9924 - val_mse: 261.9148\nEpoch 118/200\n86/86 - 0s - 3ms/step - loss: 271.5715 - mae: 13.0823 - mse: 271.5715 - val_loss: 265.3309 - val_mae: 13.0139 - val_mse: 265.3309\nEpoch 119/200\n86/86 - 0s - 3ms/step - loss: 261.2054 - mae: 12.7837 - mse: 261.2054 - val_loss: 263.5383 - val_mae: 12.9740 - val_mse: 263.5383\nEpoch 120/200\n86/86 - 0s - 3ms/step - loss: 266.9743 - mae: 12.9764 - mse: 266.9743 - val_loss: 264.2124 - val_mae: 13.0069 - val_mse: 264.2124\nEpoch 121/200\n86/86 - 0s - 4ms/step - loss: 261.6286 - mae: 12.8162 - mse: 261.6286 - val_loss: 264.6200 - val_mae: 13.0193 - val_mse: 264.6200\nEpoch 122/200\n86/86 - 0s - 4ms/step - loss: 262.9085 - mae: 12.8171 - mse: 262.9085 - val_loss: 264.0961 - val_mae: 13.0441 - val_mse: 264.0961\nEpoch 123/200\n86/86 - 0s - 3ms/step - loss: 259.6648 - mae: 12.7609 - mse: 259.6648 - val_loss: 263.9490 - val_mae: 12.9782 - val_mse: 263.9490\nEpoch 124/200\n86/86 - 0s - 4ms/step - loss: 264.7655 - mae: 12.9057 - mse: 264.7655 - val_loss: 263.8725 - val_mae: 13.0187 - val_mse: 263.8725\nEpoch 125/200\n86/86 - 0s - 3ms/step - loss: 262.9843 - mae: 12.9134 - mse: 262.9843 - val_loss: 261.4869 - val_mae: 12.9445 - val_mse: 261.4869\nEpoch 126/200\n86/86 - 0s - 4ms/step - loss: 266.7093 - mae: 12.9317 - mse: 266.7093 - val_loss: 263.7636 - val_mae: 13.0060 - val_mse: 263.7636\nEpoch 127/200\n86/86 - 0s - 3ms/step - loss: 270.7433 - mae: 13.0337 - mse: 270.7433 - val_loss: 262.5247 - val_mae: 13.0127 - val_mse: 262.5247\nEpoch 128/200\n86/86 - 0s - 3ms/step - loss: 259.2251 - mae: 12.7852 - mse: 259.2251 - val_loss: 260.9210 - val_mae: 12.9469 - val_mse: 260.9210\nEpoch 129/200\n86/86 - 0s - 4ms/step - loss: 258.9757 - mae: 12.7714 - mse: 258.9757 - val_loss: 262.9883 - val_mae: 12.9685 - val_mse: 262.9883\nEpoch 130/200\n86/86 - 0s - 3ms/step - loss: 260.0378 - mae: 12.8111 - mse: 260.0378 - val_loss: 264.7499 - val_mae: 13.0146 - val_mse: 264.7499\nEpoch 131/200\n86/86 - 0s - 4ms/step - loss: 262.1922 - mae: 12.7263 - mse: 262.1922 - val_loss: 261.4216 - val_mae: 12.9752 - val_mse: 261.4216\nEpoch 132/200\n86/86 - 0s - 3ms/step - loss: 263.5336 - mae: 12.9534 - mse: 263.5336 - val_loss: 262.0037 - val_mae: 12.9528 - val_mse: 262.0037\nEpoch 133/200\n86/86 - 0s - 4ms/step - loss: 258.5088 - mae: 12.7887 - mse: 258.5088 - val_loss: 260.4202 - val_mae: 12.8954 - val_mse: 260.4202\nEpoch 134/200\n86/86 - 0s - 4ms/step - loss: 265.9039 - mae: 12.9267 - mse: 265.9039 - val_loss: 261.0662 - val_mae: 12.9081 - val_mse: 261.0662\nEpoch 135/200\n86/86 - 0s - 4ms/step - loss: 258.7119 - mae: 12.7266 - mse: 258.7119 - val_loss: 261.5421 - val_mae: 12.9735 - val_mse: 261.5421\nEpoch 136/200\n86/86 - 0s - 3ms/step - loss: 260.7945 - mae: 12.8203 - mse: 260.7945 - val_loss: 261.5015 - val_mae: 12.9474 - val_mse: 261.5015\nEpoch 137/200\n86/86 - 0s - 3ms/step - loss: 258.8173 - mae: 12.8066 - mse: 258.8173 - val_loss: 261.1422 - val_mae: 12.9432 - val_mse: 261.1422\nEpoch 138/200\n86/86 - 0s - 3ms/step - loss: 263.4272 - mae: 12.8393 - mse: 263.4272 - val_loss: 262.2815 - val_mae: 12.9807 - val_mse: 262.2815\nEpoch 139/200\n86/86 - 0s - 4ms/step - loss: 260.0494 - mae: 12.8026 - mse: 260.0494 - val_loss: 259.6447 - val_mae: 12.8609 - val_mse: 259.6447\nEpoch 140/200\n86/86 - 0s - 4ms/step - loss: 263.1704 - mae: 12.8772 - mse: 263.1704 - val_loss: 260.9702 - val_mae: 12.8942 - val_mse: 260.9702\nEpoch 141/200\n86/86 - 0s - 3ms/step - loss: 259.3949 - mae: 12.8133 - mse: 259.3949 - val_loss: 261.2171 - val_mae: 12.9171 - val_mse: 261.2171\nEpoch 142/200\n86/86 - 0s - 4ms/step - loss: 258.1041 - mae: 12.6782 - mse: 258.1041 - val_loss: 262.3600 - val_mae: 12.9601 - val_mse: 262.3600\nEpoch 143/200\n86/86 - 0s - 3ms/step - loss: 257.5364 - mae: 12.7442 - mse: 257.5364 - val_loss: 261.2391 - val_mae: 12.9155 - val_mse: 261.2391\nEpoch 144/200\n86/86 - 0s - 4ms/step - loss: 259.6497 - mae: 12.7779 - mse: 259.6497 - val_loss: 262.1618 - val_mae: 12.9452 - val_mse: 262.1618\nEpoch 145/200\n86/86 - 0s - 3ms/step - loss: 262.7258 - mae: 12.8871 - mse: 262.7258 - val_loss: 262.5453 - val_mae: 12.9423 - val_mse: 262.5453\nEpoch 146/200\n86/86 - 0s - 4ms/step - loss: 259.1479 - mae: 12.7438 - mse: 259.1479 - val_loss: 261.4430 - val_mae: 12.9109 - val_mse: 261.4430\nEpoch 147/200\n86/86 - 0s - 3ms/step - loss: 258.8908 - mae: 12.8040 - mse: 258.8908 - val_loss: 265.0634 - val_mae: 13.0284 - val_mse: 265.0634\nEpoch 148/200\n86/86 - 0s - 3ms/step - loss: 261.5629 - mae: 12.8452 - mse: 261.5629 - val_loss: 262.0125 - val_mae: 12.9076 - val_mse: 262.0125\nEpoch 149/200\n86/86 - 0s - 3ms/step - loss: 259.5175 - mae: 12.7298 - mse: 259.5175 - val_loss: 263.0921 - val_mae: 12.9449 - val_mse: 263.0921\nEpoch 150/200\n86/86 - 0s - 3ms/step - loss: 259.1211 - mae: 12.7544 - mse: 259.1211 - val_loss: 266.4422 - val_mae: 13.0424 - val_mse: 266.4422\nEpoch 151/200\n86/86 - 0s - 3ms/step - loss: 260.9573 - mae: 12.8341 - mse: 260.9573 - val_loss: 262.5162 - val_mae: 12.9374 - val_mse: 262.5162\nEpoch 152/200\n86/86 - 0s - 3ms/step - loss: 258.9861 - mae: 12.7156 - mse: 258.9861 - val_loss: 262.4990 - val_mae: 12.9465 - val_mse: 262.4990\nEpoch 153/200\n86/86 - 0s - 3ms/step - loss: 260.0231 - mae: 12.7401 - mse: 260.0231 - val_loss: 261.8171 - val_mae: 12.9375 - val_mse: 261.8171\nEpoch 154/200\n86/86 - 0s - 4ms/step - loss: 259.0752 - mae: 12.7485 - mse: 259.0752 - val_loss: 263.2496 - val_mae: 12.9269 - val_mse: 263.2496\nEpoch 155/200\n86/86 - 0s - 4ms/step - loss: 253.3244 - mae: 12.5865 - mse: 253.3244 - val_loss: 263.0001 - val_mae: 12.9354 - val_mse: 263.0001\nEpoch 156/200\n86/86 - 0s - 3ms/step - loss: 257.9713 - mae: 12.6859 - mse: 257.9713 - val_loss: 263.9262 - val_mae: 12.9941 - val_mse: 263.9262\nEpoch 157/200\n86/86 - 0s - 3ms/step - loss: 254.8010 - mae: 12.6621 - mse: 254.8010 - val_loss: 262.5169 - val_mae: 12.9680 - val_mse: 262.5169\nEpoch 158/200\n86/86 - 0s - 4ms/step - loss: 254.5677 - mae: 12.6136 - mse: 254.5677 - val_loss: 262.2520 - val_mae: 12.9421 - val_mse: 262.2520\nEpoch 159/200\n86/86 - 0s - 4ms/step - loss: 259.3841 - mae: 12.7781 - mse: 259.3841 - val_loss: 261.7679 - val_mae: 12.9362 - val_mse: 261.7679\n86/86 - 0s - 2ms/step - loss: 259.6446 - mae: 12.8609 - mse: 259.6446\nEpoch 1/200\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"86/86 - 3s - 30ms/step - loss: 696.2159 - mae: 20.6342 - mse: 696.2159 - val_loss: 358.0237 - val_mae: 14.9816 - val_mse: 358.0237\nEpoch 2/200\n86/86 - 0s - 3ms/step - loss: 417.9039 - mae: 16.2125 - mse: 417.9039 - val_loss: 317.4366 - val_mae: 14.0662 - val_mse: 317.4366\nEpoch 3/200\n86/86 - 0s - 3ms/step - loss: 385.6168 - mae: 15.5588 - mse: 385.6168 - val_loss: 308.6385 - val_mae: 13.8719 - val_mse: 308.6385\nEpoch 4/200\n86/86 - 0s - 3ms/step - loss: 363.5802 - mae: 15.0934 - mse: 363.5802 - val_loss: 304.9879 - val_mae: 13.8152 - val_mse: 304.9879\nEpoch 5/200\n86/86 - 0s - 3ms/step - loss: 356.6616 - mae: 15.0322 - mse: 356.6616 - val_loss: 303.2782 - val_mae: 13.8231 - val_mse: 303.2782\nEpoch 6/200\n86/86 - 0s - 3ms/step - loss: 341.3903 - mae: 14.7101 - mse: 341.3903 - val_loss: 301.3611 - val_mae: 13.8043 - val_mse: 301.3611\nEpoch 7/200\n86/86 - 0s - 3ms/step - loss: 340.7696 - mae: 14.5680 - mse: 340.7696 - val_loss: 299.6920 - val_mae: 13.7788 - val_mse: 299.6920\nEpoch 8/200\n86/86 - 0s - 3ms/step - loss: 344.6859 - mae: 14.7032 - mse: 344.6859 - val_loss: 299.1218 - val_mae: 13.7886 - val_mse: 299.1218\nEpoch 9/200\n86/86 - 0s - 4ms/step - loss: 337.3810 - mae: 14.5834 - mse: 337.3810 - val_loss: 298.4910 - val_mae: 13.7510 - val_mse: 298.4910\nEpoch 10/200\n86/86 - 0s - 3ms/step - loss: 333.9950 - mae: 14.5224 - mse: 333.9950 - val_loss: 298.2865 - val_mae: 13.7846 - val_mse: 298.2865\nEpoch 11/200\n86/86 - 0s - 3ms/step - loss: 326.9030 - mae: 14.3461 - mse: 326.9030 - val_loss: 296.5339 - val_mae: 13.7039 - val_mse: 296.5339\nEpoch 12/200\n86/86 - 0s - 3ms/step - loss: 325.9288 - mae: 14.3019 - mse: 325.9288 - val_loss: 295.1537 - val_mae: 13.7208 - val_mse: 295.1537\nEpoch 13/200\n86/86 - 0s - 3ms/step - loss: 323.4455 - mae: 14.2937 - mse: 323.4455 - val_loss: 294.1165 - val_mae: 13.6624 - val_mse: 294.1165\nEpoch 14/200\n86/86 - 0s - 3ms/step - loss: 323.6895 - mae: 14.2815 - mse: 323.6895 - val_loss: 292.8712 - val_mae: 13.6536 - val_mse: 292.8712\nEpoch 15/200\n86/86 - 0s - 3ms/step - loss: 320.3051 - mae: 14.2694 - mse: 320.3051 - val_loss: 292.8812 - val_mae: 13.6975 - val_mse: 292.8812\nEpoch 16/200\n86/86 - 0s - 4ms/step - loss: 323.3633 - mae: 14.2956 - mse: 323.3633 - val_loss: 291.6262 - val_mae: 13.5675 - val_mse: 291.6262\nEpoch 17/200\n86/86 - 0s - 4ms/step - loss: 318.1030 - mae: 14.2102 - mse: 318.1030 - val_loss: 291.2447 - val_mae: 13.6417 - val_mse: 291.2447\nEpoch 18/200\n86/86 - 0s - 4ms/step - loss: 322.4247 - mae: 14.1995 - mse: 322.4247 - val_loss: 289.1802 - val_mae: 13.5706 - val_mse: 289.1802\nEpoch 19/200\n86/86 - 0s - 4ms/step - loss: 315.2063 - mae: 14.0969 - mse: 315.2063 - val_loss: 289.6639 - val_mae: 13.6264 - val_mse: 289.6639\nEpoch 20/200\n86/86 - 0s - 4ms/step - loss: 319.3555 - mae: 14.1210 - mse: 319.3555 - val_loss: 288.6606 - val_mae: 13.5610 - val_mse: 288.6606\nEpoch 21/200\n86/86 - 0s - 4ms/step - loss: 309.5195 - mae: 13.9798 - mse: 309.5195 - val_loss: 288.7569 - val_mae: 13.5762 - val_mse: 288.7569\nEpoch 22/200\n86/86 - 0s - 3ms/step - loss: 316.3434 - mae: 14.1626 - mse: 316.3434 - val_loss: 287.4587 - val_mae: 13.5363 - val_mse: 287.4587\nEpoch 23/200\n86/86 - 0s - 3ms/step - loss: 316.7485 - mae: 14.1636 - mse: 316.7485 - val_loss: 287.5460 - val_mae: 13.5775 - val_mse: 287.5460\nEpoch 24/200\n86/86 - 0s - 4ms/step - loss: 317.2104 - mae: 14.1757 - mse: 317.2104 - val_loss: 287.4659 - val_mae: 13.5572 - val_mse: 287.4659\nEpoch 25/200\n86/86 - 0s - 3ms/step - loss: 312.4107 - mae: 14.0021 - mse: 312.4107 - val_loss: 285.6288 - val_mae: 13.4922 - val_mse: 285.6288\nEpoch 26/200\n86/86 - 0s - 4ms/step - loss: 312.1251 - mae: 13.9799 - mse: 312.1251 - val_loss: 285.2106 - val_mae: 13.5163 - val_mse: 285.2106\nEpoch 27/200\n86/86 - 0s - 4ms/step - loss: 309.9510 - mae: 14.0310 - mse: 309.9510 - val_loss: 284.1018 - val_mae: 13.4317 - val_mse: 284.1018\nEpoch 28/200\n86/86 - 0s - 4ms/step - loss: 311.6004 - mae: 14.0048 - mse: 311.6004 - val_loss: 283.6580 - val_mae: 13.4478 - val_mse: 283.6580\nEpoch 29/200\n86/86 - 0s - 4ms/step - loss: 303.0751 - mae: 13.8431 - mse: 303.0751 - val_loss: 283.4479 - val_mae: 13.4218 - val_mse: 283.4479\nEpoch 30/200\n86/86 - 0s - 4ms/step - loss: 310.6436 - mae: 13.9466 - mse: 310.6436 - val_loss: 283.5451 - val_mae: 13.4629 - val_mse: 283.5451\nEpoch 31/200\n86/86 - 0s - 3ms/step - loss: 311.5437 - mae: 14.0108 - mse: 311.5437 - val_loss: 281.5910 - val_mae: 13.3933 - val_mse: 281.5910\nEpoch 32/200\n86/86 - 0s - 4ms/step - loss: 307.4075 - mae: 13.9158 - mse: 307.4075 - val_loss: 280.8373 - val_mae: 13.3695 - val_mse: 280.8373\nEpoch 33/200\n86/86 - 0s - 4ms/step - loss: 301.5300 - mae: 13.7486 - mse: 301.5300 - val_loss: 282.3559 - val_mae: 13.4238 - val_mse: 282.3559\nEpoch 34/200\n86/86 - 0s - 4ms/step - loss: 302.9063 - mae: 13.7562 - mse: 302.9063 - val_loss: 278.5768 - val_mae: 13.3067 - val_mse: 278.5768\nEpoch 35/200\n86/86 - 0s - 4ms/step - loss: 301.1116 - mae: 13.7165 - mse: 301.1116 - val_loss: 279.6751 - val_mae: 13.3331 - val_mse: 279.6751\nEpoch 36/200\n86/86 - 0s - 4ms/step - loss: 300.2466 - mae: 13.8067 - mse: 300.2466 - val_loss: 277.0676 - val_mae: 13.2515 - val_mse: 277.0676\nEpoch 37/200\n86/86 - 0s - 4ms/step - loss: 295.9000 - mae: 13.6488 - mse: 295.9000 - val_loss: 278.2850 - val_mae: 13.2959 - val_mse: 278.2850\nEpoch 38/200\n86/86 - 0s - 4ms/step - loss: 297.6611 - mae: 13.7020 - mse: 297.6611 - val_loss: 277.7308 - val_mae: 13.3119 - val_mse: 277.7308\nEpoch 39/200\n86/86 - 0s - 3ms/step - loss: 301.0926 - mae: 13.7921 - mse: 301.0926 - val_loss: 276.8611 - val_mae: 13.2972 - val_mse: 276.8611\nEpoch 40/200\n86/86 - 0s - 3ms/step - loss: 297.2070 - mae: 13.6580 - mse: 297.2070 - val_loss: 277.7458 - val_mae: 13.2820 - val_mse: 277.7458\nEpoch 41/200\n86/86 - 0s - 4ms/step - loss: 295.6072 - mae: 13.6025 - mse: 295.6072 - val_loss: 275.9192 - val_mae: 13.2555 - val_mse: 275.9192\nEpoch 42/200\n86/86 - 0s - 4ms/step - loss: 299.4055 - mae: 13.8134 - mse: 299.4055 - val_loss: 275.4725 - val_mae: 13.2631 - val_mse: 275.4725\nEpoch 43/200\n86/86 - 0s - 3ms/step - loss: 291.5551 - mae: 13.5518 - mse: 291.5551 - val_loss: 271.4612 - val_mae: 13.1013 - val_mse: 271.4612\nEpoch 44/200\n86/86 - 0s - 4ms/step - loss: 298.5399 - mae: 13.7321 - mse: 298.5399 - val_loss: 273.2267 - val_mae: 13.1877 - val_mse: 273.2267\nEpoch 45/200\n86/86 - 0s - 4ms/step - loss: 289.6627 - mae: 13.5224 - mse: 289.6627 - val_loss: 272.4440 - val_mae: 13.1649 - val_mse: 272.4440\nEpoch 46/200\n86/86 - 0s - 4ms/step - loss: 290.1798 - mae: 13.5327 - mse: 290.1798 - val_loss: 272.2318 - val_mae: 13.1870 - val_mse: 272.2318\nEpoch 47/200\n86/86 - 0s - 4ms/step - loss: 285.0755 - mae: 13.3921 - mse: 285.0755 - val_loss: 272.2247 - val_mae: 13.1319 - val_mse: 272.2247\nEpoch 48/200\n86/86 - 0s - 4ms/step - loss: 290.4583 - mae: 13.5144 - mse: 290.4583 - val_loss: 272.2736 - val_mae: 13.1387 - val_mse: 272.2736\nEpoch 49/200\n86/86 - 0s - 4ms/step - loss: 291.9966 - mae: 13.5334 - mse: 291.9966 - val_loss: 275.4846 - val_mae: 13.2478 - val_mse: 275.4846\nEpoch 50/200\n86/86 - 0s - 4ms/step - loss: 291.4597 - mae: 13.5219 - mse: 291.4597 - val_loss: 271.6601 - val_mae: 13.1253 - val_mse: 271.6601\nEpoch 51/200\n86/86 - 0s - 4ms/step - loss: 287.9819 - mae: 13.4878 - mse: 287.9819 - val_loss: 271.1610 - val_mae: 13.0919 - val_mse: 271.1610\nEpoch 52/200\n86/86 - 0s - 4ms/step - loss: 290.5554 - mae: 13.5279 - mse: 290.5554 - val_loss: 271.5035 - val_mae: 13.1353 - val_mse: 271.5035\nEpoch 53/200\n86/86 - 0s - 4ms/step - loss: 284.2187 - mae: 13.3591 - mse: 284.2187 - val_loss: 271.9641 - val_mae: 13.1320 - val_mse: 271.9641\nEpoch 54/200\n86/86 - 0s - 3ms/step - loss: 283.0233 - mae: 13.3852 - mse: 283.0233 - val_loss: 274.4480 - val_mae: 13.2376 - val_mse: 274.4480\nEpoch 55/200\n86/86 - 0s - 4ms/step - loss: 286.1928 - mae: 13.4092 - mse: 286.1928 - val_loss: 271.1894 - val_mae: 13.1175 - val_mse: 271.1894\nEpoch 56/200\n86/86 - 0s - 4ms/step - loss: 284.1731 - mae: 13.3752 - mse: 284.1731 - val_loss: 270.3615 - val_mae: 13.1004 - val_mse: 270.3615\nEpoch 57/200\n86/86 - 0s - 4ms/step - loss: 286.0171 - mae: 13.3869 - mse: 286.0171 - val_loss: 269.2424 - val_mae: 13.0945 - val_mse: 269.2424\nEpoch 58/200\n86/86 - 0s - 4ms/step - loss: 288.9308 - mae: 13.5052 - mse: 288.9308 - val_loss: 268.8320 - val_mae: 13.0699 - val_mse: 268.8320\nEpoch 59/200\n86/86 - 0s - 4ms/step - loss: 283.6042 - mae: 13.4024 - mse: 283.6042 - val_loss: 269.1672 - val_mae: 13.0716 - val_mse: 269.1672\nEpoch 60/200\n86/86 - 0s - 4ms/step - loss: 284.8114 - mae: 13.4233 - mse: 284.8114 - val_loss: 268.8968 - val_mae: 13.0812 - val_mse: 268.8968\nEpoch 61/200\n86/86 - 0s - 3ms/step - loss: 283.0134 - mae: 13.3799 - mse: 283.0134 - val_loss: 269.1353 - val_mae: 13.0728 - val_mse: 269.1353\nEpoch 62/200\n86/86 - 0s - 3ms/step - loss: 284.0132 - mae: 13.4016 - mse: 284.0132 - val_loss: 269.0443 - val_mae: 13.0634 - val_mse: 269.0443\nEpoch 63/200\n86/86 - 0s - 4ms/step - loss: 280.9391 - mae: 13.3068 - mse: 280.9391 - val_loss: 272.2329 - val_mae: 13.1209 - val_mse: 272.2329\nEpoch 64/200\n86/86 - 0s - 4ms/step - loss: 282.6917 - mae: 13.2857 - mse: 282.6917 - val_loss: 267.6216 - val_mae: 13.0480 - val_mse: 267.6216\nEpoch 65/200\n86/86 - 0s - 3ms/step - loss: 283.7861 - mae: 13.4163 - mse: 283.7861 - val_loss: 268.6453 - val_mae: 13.0966 - val_mse: 268.6453\nEpoch 66/200\n86/86 - 0s - 4ms/step - loss: 283.2494 - mae: 13.3797 - mse: 283.2494 - val_loss: 267.4508 - val_mae: 13.0467 - val_mse: 267.4508\nEpoch 67/200\n86/86 - 0s - 4ms/step - loss: 286.6376 - mae: 13.4694 - mse: 286.6376 - val_loss: 267.1636 - val_mae: 13.0345 - val_mse: 267.1636\nEpoch 68/200\n86/86 - 0s - 4ms/step - loss: 286.7575 - mae: 13.4326 - mse: 286.7575 - val_loss: 269.0687 - val_mae: 13.1123 - val_mse: 269.0687\nEpoch 69/200\n86/86 - 0s - 4ms/step - loss: 284.5297 - mae: 13.4175 - mse: 284.5297 - val_loss: 266.7299 - val_mae: 13.0529 - val_mse: 266.7299\nEpoch 70/200\n86/86 - 0s - 4ms/step - loss: 281.6133 - mae: 13.3793 - mse: 281.6133 - val_loss: 265.4255 - val_mae: 12.9742 - val_mse: 265.4255\nEpoch 71/200\n86/86 - 0s - 4ms/step - loss: 281.3947 - mae: 13.3901 - mse: 281.3947 - val_loss: 264.3056 - val_mae: 12.9691 - val_mse: 264.3056\nEpoch 72/200\n86/86 - 0s - 4ms/step - loss: 278.6936 - mae: 13.2620 - mse: 278.6936 - val_loss: 266.4751 - val_mae: 13.0449 - val_mse: 266.4751\nEpoch 73/200\n86/86 - 0s - 4ms/step - loss: 280.8784 - mae: 13.2847 - mse: 280.8784 - val_loss: 267.0381 - val_mae: 13.0362 - val_mse: 267.0381\nEpoch 74/200\n86/86 - 0s - 4ms/step - loss: 278.9701 - mae: 13.2999 - mse: 278.9701 - val_loss: 263.9377 - val_mae: 12.9610 - val_mse: 263.9377\nEpoch 75/200\n86/86 - 0s - 4ms/step - loss: 275.2991 - mae: 13.1558 - mse: 275.2991 - val_loss: 263.9295 - val_mae: 12.9541 - val_mse: 263.9295\nEpoch 76/200\n86/86 - 0s - 4ms/step - loss: 274.4194 - mae: 13.1740 - mse: 274.4194 - val_loss: 264.2745 - val_mae: 12.9503 - val_mse: 264.2745\nEpoch 77/200\n86/86 - 0s - 4ms/step - loss: 278.2323 - mae: 13.2406 - mse: 278.2323 - val_loss: 263.8148 - val_mae: 12.9494 - val_mse: 263.8148\nEpoch 78/200\n86/86 - 0s - 3ms/step - loss: 271.0854 - mae: 13.0655 - mse: 271.0854 - val_loss: 263.4214 - val_mae: 12.9210 - val_mse: 263.4214\nEpoch 79/200\n86/86 - 0s - 4ms/step - loss: 277.9617 - mae: 13.2528 - mse: 277.9617 - val_loss: 264.8315 - val_mae: 12.9565 - val_mse: 264.8315\nEpoch 80/200\n86/86 - 0s - 4ms/step - loss: 273.4855 - mae: 13.1156 - mse: 273.4855 - val_loss: 264.8254 - val_mae: 13.0104 - val_mse: 264.8254\nEpoch 81/200\n86/86 - 0s - 4ms/step - loss: 279.8731 - mae: 13.3784 - mse: 279.8731 - val_loss: 263.6411 - val_mae: 12.9122 - val_mse: 263.6411\nEpoch 82/200\n86/86 - 0s - 4ms/step - loss: 271.8524 - mae: 13.0887 - mse: 271.8524 - val_loss: 262.4570 - val_mae: 12.8980 - val_mse: 262.4570\nEpoch 83/200\n86/86 - 0s - 3ms/step - loss: 276.9671 - mae: 13.1898 - mse: 276.9671 - val_loss: 262.6852 - val_mae: 12.9325 - val_mse: 262.6852\nEpoch 84/200\n86/86 - 0s - 3ms/step - loss: 274.4594 - mae: 13.1430 - mse: 274.4594 - val_loss: 263.1710 - val_mae: 12.9398 - val_mse: 263.1710\nEpoch 85/200\n86/86 - 0s - 4ms/step - loss: 274.4162 - mae: 13.1439 - mse: 274.4162 - val_loss: 261.4839 - val_mae: 12.9224 - val_mse: 261.4839\nEpoch 86/200\n86/86 - 0s - 4ms/step - loss: 270.7079 - mae: 13.0617 - mse: 270.7079 - val_loss: 262.2306 - val_mae: 12.9126 - val_mse: 262.2306\nEpoch 87/200\n86/86 - 0s - 4ms/step - loss: 273.5977 - mae: 13.1638 - mse: 273.5977 - val_loss: 261.0666 - val_mae: 12.8839 - val_mse: 261.0666\nEpoch 88/200\n86/86 - 0s - 4ms/step - loss: 272.7336 - mae: 13.0481 - mse: 272.7336 - val_loss: 263.0463 - val_mae: 12.9568 - val_mse: 263.0463\nEpoch 89/200\n86/86 - 0s - 4ms/step - loss: 270.0951 - mae: 13.0134 - mse: 270.0951 - val_loss: 262.6111 - val_mae: 12.9309 - val_mse: 262.6111\nEpoch 90/200\n86/86 - 0s - 3ms/step - loss: 275.3649 - mae: 13.1614 - mse: 275.3649 - val_loss: 261.7020 - val_mae: 12.8719 - val_mse: 261.7020\nEpoch 91/200\n86/86 - 0s - 4ms/step - loss: 275.1274 - mae: 13.1730 - mse: 275.1274 - val_loss: 264.4291 - val_mae: 12.9803 - val_mse: 264.4291\nEpoch 92/200\n86/86 - 0s - 3ms/step - loss: 275.1903 - mae: 13.2336 - mse: 275.1903 - val_loss: 262.0033 - val_mae: 12.9059 - val_mse: 262.0033\nEpoch 93/200\n86/86 - 0s - 3ms/step - loss: 269.1030 - mae: 13.0354 - mse: 269.1030 - val_loss: 259.9644 - val_mae: 12.8285 - val_mse: 259.9644\nEpoch 94/200\n86/86 - 0s - 4ms/step - loss: 270.3136 - mae: 13.0846 - mse: 270.3136 - val_loss: 261.1283 - val_mae: 12.8975 - val_mse: 261.1283\nEpoch 95/200\n86/86 - 0s - 4ms/step - loss: 272.7849 - mae: 13.1158 - mse: 272.7849 - val_loss: 260.9801 - val_mae: 12.8813 - val_mse: 260.9801\nEpoch 96/200\n86/86 - 0s - 3ms/step - loss: 269.1652 - mae: 13.0511 - mse: 269.1652 - val_loss: 259.0057 - val_mae: 12.8567 - val_mse: 259.0057\nEpoch 97/200\n86/86 - 0s - 4ms/step - loss: 262.0398 - mae: 12.8753 - mse: 262.0398 - val_loss: 259.9182 - val_mae: 12.8986 - val_mse: 259.9182\nEpoch 98/200\n86/86 - 0s - 4ms/step - loss: 268.7796 - mae: 12.9861 - mse: 268.7796 - val_loss: 259.8969 - val_mae: 12.8973 - val_mse: 259.8969\nEpoch 99/200\n86/86 - 0s - 4ms/step - loss: 271.4027 - mae: 13.1584 - mse: 271.4027 - val_loss: 260.8014 - val_mae: 12.9251 - val_mse: 260.8014\nEpoch 100/200\n86/86 - 0s - 4ms/step - loss: 269.2113 - mae: 13.0010 - mse: 269.2113 - val_loss: 259.9071 - val_mae: 12.8742 - val_mse: 259.9071\nEpoch 101/200\n86/86 - 0s - 3ms/step - loss: 272.7162 - mae: 13.1817 - mse: 272.7162 - val_loss: 260.4650 - val_mae: 12.8958 - val_mse: 260.4650\nEpoch 102/200\n86/86 - 0s - 4ms/step - loss: 272.1461 - mae: 13.1527 - mse: 272.1461 - val_loss: 256.6237 - val_mae: 12.7662 - val_mse: 256.6237\nEpoch 103/200\n86/86 - 0s - 4ms/step - loss: 265.3118 - mae: 12.9776 - mse: 265.3118 - val_loss: 256.8866 - val_mae: 12.7994 - val_mse: 256.8866\nEpoch 104/200\n86/86 - 0s - 4ms/step - loss: 266.2802 - mae: 12.9782 - mse: 266.2802 - val_loss: 258.1301 - val_mae: 12.8439 - val_mse: 258.1301\nEpoch 105/200\n86/86 - 0s - 3ms/step - loss: 271.8734 - mae: 13.1348 - mse: 271.8734 - val_loss: 257.8272 - val_mae: 12.8519 - val_mse: 257.8272\nEpoch 106/200\n86/86 - 0s - 3ms/step - loss: 267.7682 - mae: 12.9793 - mse: 267.7682 - val_loss: 257.9862 - val_mae: 12.8583 - val_mse: 257.9862\nEpoch 107/200\n86/86 - 0s - 4ms/step - loss: 266.5026 - mae: 13.0059 - mse: 266.5026 - val_loss: 256.2635 - val_mae: 12.7882 - val_mse: 256.2635\nEpoch 108/200\n86/86 - 0s - 4ms/step - loss: 272.2362 - mae: 13.1335 - mse: 272.2362 - val_loss: 259.2776 - val_mae: 12.9091 - val_mse: 259.2776\nEpoch 109/200\n86/86 - 0s - 3ms/step - loss: 267.2792 - mae: 12.9234 - mse: 267.2792 - val_loss: 259.5524 - val_mae: 12.9170 - val_mse: 259.5524\nEpoch 110/200\n86/86 - 0s - 4ms/step - loss: 267.1640 - mae: 13.0528 - mse: 267.1640 - val_loss: 258.0451 - val_mae: 12.8586 - val_mse: 258.0451\nEpoch 111/200\n86/86 - 0s - 4ms/step - loss: 267.6519 - mae: 13.0044 - mse: 267.6519 - val_loss: 258.0159 - val_mae: 12.8601 - val_mse: 258.0159\nEpoch 112/200\n86/86 - 0s - 4ms/step - loss: 270.3855 - mae: 13.0369 - mse: 270.3855 - val_loss: 257.1213 - val_mae: 12.8136 - val_mse: 257.1213\nEpoch 113/200\n86/86 - 0s - 4ms/step - loss: 264.3403 - mae: 12.9049 - mse: 264.3403 - val_loss: 257.2363 - val_mae: 12.8289 - val_mse: 257.2363\nEpoch 114/200\n86/86 - 0s - 4ms/step - loss: 265.5100 - mae: 13.0180 - mse: 265.5100 - val_loss: 257.3346 - val_mae: 12.8430 - val_mse: 257.3346\nEpoch 115/200\n86/86 - 0s - 3ms/step - loss: 268.1171 - mae: 13.0416 - mse: 268.1171 - val_loss: 257.3818 - val_mae: 12.8483 - val_mse: 257.3818\nEpoch 116/200\n86/86 - 0s - 3ms/step - loss: 260.7893 - mae: 12.7886 - mse: 260.7893 - val_loss: 256.9779 - val_mae: 12.8193 - val_mse: 256.9779\nEpoch 117/200\n86/86 - 0s - 4ms/step - loss: 267.9761 - mae: 13.0051 - mse: 267.9761 - val_loss: 258.1886 - val_mae: 12.8831 - val_mse: 258.1886\nEpoch 118/200\n86/86 - 0s - 4ms/step - loss: 263.1011 - mae: 12.8726 - mse: 263.1011 - val_loss: 257.6654 - val_mae: 12.8474 - val_mse: 257.6654\nEpoch 119/200\n86/86 - 0s - 4ms/step - loss: 267.7139 - mae: 12.9918 - mse: 267.7139 - val_loss: 255.8679 - val_mae: 12.7872 - val_mse: 255.8679\nEpoch 120/200\n86/86 - 0s - 4ms/step - loss: 264.1200 - mae: 12.9343 - mse: 264.1200 - val_loss: 257.1324 - val_mae: 12.8457 - val_mse: 257.1324\nEpoch 121/200\n86/86 - 0s - 3ms/step - loss: 271.2261 - mae: 13.1401 - mse: 271.2261 - val_loss: 257.1429 - val_mae: 12.8162 - val_mse: 257.1429\nEpoch 122/200\n86/86 - 0s - 4ms/step - loss: 260.7398 - mae: 12.7719 - mse: 260.7398 - val_loss: 256.5659 - val_mae: 12.8259 - val_mse: 256.5659\nEpoch 123/200\n86/86 - 0s - 4ms/step - loss: 266.0197 - mae: 12.9722 - mse: 266.0197 - val_loss: 257.6417 - val_mae: 12.8571 - val_mse: 257.6417\nEpoch 124/200\n86/86 - 0s - 4ms/step - loss: 259.1856 - mae: 12.8373 - mse: 259.1856 - val_loss: 257.5299 - val_mae: 12.8561 - val_mse: 257.5299\nEpoch 125/200\n86/86 - 0s - 3ms/step - loss: 262.6479 - mae: 12.9208 - mse: 262.6479 - val_loss: 256.8653 - val_mae: 12.8500 - val_mse: 256.8653\nEpoch 126/200\n86/86 - 0s - 3ms/step - loss: 265.2917 - mae: 12.9780 - mse: 265.2917 - val_loss: 257.1684 - val_mae: 12.8577 - val_mse: 257.1684\nEpoch 127/200\n86/86 - 0s - 3ms/step - loss: 263.6478 - mae: 12.8608 - mse: 263.6478 - val_loss: 257.1994 - val_mae: 12.8412 - val_mse: 257.1994\nEpoch 128/200\n86/86 - 0s - 3ms/step - loss: 264.9593 - mae: 12.9440 - mse: 264.9593 - val_loss: 256.4149 - val_mae: 12.8160 - val_mse: 256.4149\nEpoch 129/200\n86/86 - 0s - 3ms/step - loss: 263.7823 - mae: 12.9831 - mse: 263.7823 - val_loss: 256.3759 - val_mae: 12.8562 - val_mse: 256.3759\nEpoch 130/200\n86/86 - 0s - 4ms/step - loss: 264.8212 - mae: 12.9703 - mse: 264.8212 - val_loss: 256.0016 - val_mae: 12.7983 - val_mse: 256.0016\nEpoch 131/200\n86/86 - 0s - 4ms/step - loss: 259.7936 - mae: 12.8166 - mse: 259.7936 - val_loss: 257.3995 - val_mae: 12.8744 - val_mse: 257.3995\nEpoch 132/200\n86/86 - 0s - 4ms/step - loss: 266.4110 - mae: 12.9700 - mse: 266.4110 - val_loss: 257.0915 - val_mae: 12.8652 - val_mse: 257.0915\nEpoch 133/200\n86/86 - 0s - 4ms/step - loss: 257.9198 - mae: 12.7562 - mse: 257.9198 - val_loss: 258.3745 - val_mae: 12.8654 - val_mse: 258.3745\nEpoch 134/200\n86/86 - 0s - 3ms/step - loss: 266.1476 - mae: 12.9558 - mse: 266.1476 - val_loss: 254.4698 - val_mae: 12.7868 - val_mse: 254.4698\nEpoch 135/200\n86/86 - 0s - 4ms/step - loss: 263.1534 - mae: 12.8795 - mse: 263.1534 - val_loss: 257.2028 - val_mae: 12.8584 - val_mse: 257.2028\nEpoch 136/200\n86/86 - 0s - 4ms/step - loss: 263.3145 - mae: 12.8868 - mse: 263.3145 - val_loss: 254.5567 - val_mae: 12.8245 - val_mse: 254.5567\nEpoch 137/200\n86/86 - 0s - 3ms/step - loss: 262.3929 - mae: 12.8751 - mse: 262.3929 - val_loss: 254.2525 - val_mae: 12.7981 - val_mse: 254.2525\nEpoch 138/200\n86/86 - 0s - 3ms/step - loss: 258.3621 - mae: 12.8038 - mse: 258.3621 - val_loss: 254.9014 - val_mae: 12.7925 - val_mse: 254.9014\nEpoch 139/200\n86/86 - 0s - 4ms/step - loss: 264.5805 - mae: 12.9743 - mse: 264.5805 - val_loss: 254.4535 - val_mae: 12.7792 - val_mse: 254.4535\nEpoch 140/200\n86/86 - 0s - 3ms/step - loss: 260.8421 - mae: 12.8564 - mse: 260.8421 - val_loss: 255.7860 - val_mae: 12.8440 - val_mse: 255.7860\nEpoch 141/200\n86/86 - 0s - 3ms/step - loss: 260.2424 - mae: 12.7843 - mse: 260.2424 - val_loss: 255.7119 - val_mae: 12.8536 - val_mse: 255.7119\nEpoch 142/200\n86/86 - 0s - 3ms/step - loss: 259.3887 - mae: 12.8345 - mse: 259.3887 - val_loss: 257.7969 - val_mae: 12.9335 - val_mse: 257.7969\nEpoch 143/200\n86/86 - 0s - 3ms/step - loss: 263.9910 - mae: 12.9106 - mse: 263.9910 - val_loss: 256.1185 - val_mae: 12.8451 - val_mse: 256.1185\nEpoch 144/200\n86/86 - 0s - 3ms/step - loss: 262.3792 - mae: 12.9059 - mse: 262.3792 - val_loss: 255.7974 - val_mae: 12.8325 - val_mse: 255.7974\nEpoch 145/200\n86/86 - 0s - 4ms/step - loss: 256.8242 - mae: 12.7248 - mse: 256.8242 - val_loss: 253.8341 - val_mae: 12.7731 - val_mse: 253.8341\nEpoch 146/200\n86/86 - 0s - 4ms/step - loss: 258.5707 - mae: 12.7927 - mse: 258.5707 - val_loss: 252.8184 - val_mae: 12.7606 - val_mse: 252.8184\nEpoch 147/200\n86/86 - 0s - 4ms/step - loss: 258.9450 - mae: 12.8520 - mse: 258.9450 - val_loss: 252.7194 - val_mae: 12.7730 - val_mse: 252.7194\nEpoch 148/200\n86/86 - 0s - 3ms/step - loss: 251.7207 - mae: 12.6448 - mse: 251.7207 - val_loss: 252.5507 - val_mae: 12.7742 - val_mse: 252.5507\nEpoch 149/200\n86/86 - 0s - 3ms/step - loss: 257.2378 - mae: 12.8470 - mse: 257.2378 - val_loss: 252.5815 - val_mae: 12.7922 - val_mse: 252.5815\nEpoch 150/200\n86/86 - 0s - 4ms/step - loss: 255.4695 - mae: 12.7219 - mse: 255.4695 - val_loss: 252.9164 - val_mae: 12.7957 - val_mse: 252.9164\nEpoch 151/200\n86/86 - 0s - 4ms/step - loss: 257.9992 - mae: 12.7987 - mse: 257.9992 - val_loss: 251.5208 - val_mae: 12.7104 - val_mse: 251.5208\nEpoch 152/200\n86/86 - 0s - 4ms/step - loss: 262.0111 - mae: 12.8826 - mse: 262.0111 - val_loss: 250.4759 - val_mae: 12.6970 - val_mse: 250.4759\nEpoch 153/200\n86/86 - 0s - 3ms/step - loss: 256.4466 - mae: 12.7704 - mse: 256.4466 - val_loss: 252.7145 - val_mae: 12.7652 - val_mse: 252.7145\nEpoch 154/200\n86/86 - 0s - 3ms/step - loss: 258.1026 - mae: 12.8834 - mse: 258.1026 - val_loss: 252.4451 - val_mae: 12.6983 - val_mse: 252.4451\nEpoch 155/200\n86/86 - 0s - 3ms/step - loss: 254.5040 - mae: 12.6949 - mse: 254.5040 - val_loss: 254.1902 - val_mae: 12.8236 - val_mse: 254.1902\nEpoch 156/200\n86/86 - 0s - 3ms/step - loss: 259.5505 - mae: 12.8241 - mse: 259.5505 - val_loss: 252.4048 - val_mae: 12.7451 - val_mse: 252.4048\nEpoch 157/200\n86/86 - 0s - 3ms/step - loss: 254.1534 - mae: 12.6957 - mse: 254.1534 - val_loss: 253.9649 - val_mae: 12.8082 - val_mse: 253.9649\nEpoch 158/200\n86/86 - 0s - 3ms/step - loss: 258.5153 - mae: 12.8480 - mse: 258.5153 - val_loss: 253.4441 - val_mae: 12.7081 - val_mse: 253.4441\nEpoch 159/200\n86/86 - 0s - 3ms/step - loss: 259.2552 - mae: 12.7380 - mse: 259.2552 - val_loss: 252.4665 - val_mae: 12.7089 - val_mse: 252.4665\nEpoch 160/200\n86/86 - 0s - 4ms/step - loss: 260.8697 - mae: 12.8516 - mse: 260.8697 - val_loss: 250.1108 - val_mae: 12.6495 - val_mse: 250.1108\nEpoch 161/200\n86/86 - 0s - 4ms/step - loss: 256.0970 - mae: 12.7609 - mse: 256.0970 - val_loss: 250.8910 - val_mae: 12.6944 - val_mse: 250.8910\nEpoch 162/200\n86/86 - 0s - 4ms/step - loss: 257.4247 - mae: 12.7791 - mse: 257.4247 - val_loss: 252.1749 - val_mae: 12.6906 - val_mse: 252.1749\nEpoch 163/200\n86/86 - 0s - 3ms/step - loss: 256.0346 - mae: 12.6871 - mse: 256.0346 - val_loss: 252.0110 - val_mae: 12.7147 - val_mse: 252.0110\nEpoch 164/200\n86/86 - 0s - 3ms/step - loss: 259.0955 - mae: 12.7887 - mse: 259.0955 - val_loss: 254.3523 - val_mae: 12.7881 - val_mse: 254.3523\nEpoch 165/200\n86/86 - 0s - 3ms/step - loss: 259.3914 - mae: 12.8188 - mse: 259.3914 - val_loss: 251.3881 - val_mae: 12.7234 - val_mse: 251.3881\nEpoch 166/200\n86/86 - 0s - 4ms/step - loss: 259.5448 - mae: 12.8471 - mse: 259.5448 - val_loss: 251.9398 - val_mae: 12.7135 - val_mse: 251.9398\nEpoch 167/200\n86/86 - 0s - 3ms/step - loss: 253.9601 - mae: 12.6933 - mse: 253.9601 - val_loss: 250.4386 - val_mae: 12.6694 - val_mse: 250.4386\nEpoch 168/200\n86/86 - 0s - 4ms/step - loss: 256.9627 - mae: 12.7674 - mse: 256.9627 - val_loss: 250.0522 - val_mae: 12.6338 - val_mse: 250.0522\nEpoch 169/200\n86/86 - 0s - 3ms/step - loss: 251.2018 - mae: 12.6648 - mse: 251.2018 - val_loss: 251.6719 - val_mae: 12.7261 - val_mse: 251.6719\nEpoch 170/200\n86/86 - 0s - 3ms/step - loss: 258.9488 - mae: 12.8224 - mse: 258.9488 - val_loss: 251.0063 - val_mae: 12.6773 - val_mse: 251.0063\nEpoch 171/200\n86/86 - 0s - 3ms/step - loss: 254.5059 - mae: 12.7067 - mse: 254.5059 - val_loss: 252.0054 - val_mae: 12.7144 - val_mse: 252.0054\nEpoch 172/200\n86/86 - 0s - 3ms/step - loss: 252.0174 - mae: 12.6677 - mse: 252.0174 - val_loss: 251.8160 - val_mae: 12.6741 - val_mse: 251.8160\nEpoch 173/200\n86/86 - 0s - 3ms/step - loss: 258.1415 - mae: 12.7365 - mse: 258.1415 - val_loss: 250.5279 - val_mae: 12.6391 - val_mse: 250.5279\nEpoch 174/200\n86/86 - 0s - 3ms/step - loss: 260.3581 - mae: 12.8474 - mse: 260.3581 - val_loss: 249.8164 - val_mae: 12.6703 - val_mse: 249.8164\nEpoch 175/200\n86/86 - 0s - 3ms/step - loss: 257.5038 - mae: 12.8334 - mse: 257.5038 - val_loss: 250.3514 - val_mae: 12.6702 - val_mse: 250.3514\nEpoch 176/200\n86/86 - 0s - 3ms/step - loss: 255.1528 - mae: 12.7585 - mse: 255.1528 - val_loss: 249.4594 - val_mae: 12.6221 - val_mse: 249.4594\nEpoch 177/200\n86/86 - 0s - 3ms/step - loss: 253.8751 - mae: 12.7552 - mse: 253.8751 - val_loss: 250.1816 - val_mae: 12.6799 - val_mse: 250.1816\nEpoch 178/200\n86/86 - 0s - 4ms/step - loss: 257.5077 - mae: 12.8503 - mse: 257.5077 - val_loss: 249.7818 - val_mae: 12.7128 - val_mse: 249.7818\nEpoch 179/200\n86/86 - 0s - 4ms/step - loss: 252.8387 - mae: 12.6752 - mse: 252.8387 - val_loss: 248.8751 - val_mae: 12.6403 - val_mse: 248.8751\nEpoch 180/200\n86/86 - 0s - 3ms/step - loss: 252.9101 - mae: 12.6142 - mse: 252.9101 - val_loss: 249.1306 - val_mae: 12.6366 - val_mse: 249.1306\nEpoch 181/200\n86/86 - 0s - 3ms/step - loss: 255.1724 - mae: 12.7404 - mse: 255.1724 - val_loss: 249.0359 - val_mae: 12.6610 - val_mse: 249.0359\nEpoch 182/200\n86/86 - 0s - 3ms/step - loss: 255.2357 - mae: 12.7286 - mse: 255.2357 - val_loss: 250.8710 - val_mae: 12.6819 - val_mse: 250.8710\nEpoch 183/200\n86/86 - 0s - 4ms/step - loss: 258.1654 - mae: 12.7815 - mse: 258.1654 - val_loss: 250.5552 - val_mae: 12.6846 - val_mse: 250.5552\nEpoch 184/200\n86/86 - 0s - 3ms/step - loss: 250.1433 - mae: 12.6437 - mse: 250.1433 - val_loss: 251.5624 - val_mae: 12.7206 - val_mse: 251.5624\nEpoch 185/200\n86/86 - 0s - 3ms/step - loss: 255.1261 - mae: 12.7564 - mse: 255.1261 - val_loss: 249.0822 - val_mae: 12.6637 - val_mse: 249.0822\nEpoch 186/200\n86/86 - 0s - 3ms/step - loss: 251.2710 - mae: 12.6732 - mse: 251.2710 - val_loss: 250.1649 - val_mae: 12.6884 - val_mse: 250.1649\nEpoch 187/200\n86/86 - 0s - 3ms/step - loss: 257.2504 - mae: 12.8072 - mse: 257.2504 - val_loss: 248.5038 - val_mae: 12.6264 - val_mse: 248.5038\nEpoch 188/200\n86/86 - 0s - 3ms/step - loss: 252.0136 - mae: 12.6488 - mse: 252.0136 - val_loss: 250.9979 - val_mae: 12.7250 - val_mse: 250.9979\nEpoch 189/200\n86/86 - 0s - 3ms/step - loss: 255.2935 - mae: 12.7486 - mse: 255.2935 - val_loss: 247.2467 - val_mae: 12.5910 - val_mse: 247.2467\nEpoch 190/200\n86/86 - 0s - 3ms/step - loss: 252.5647 - mae: 12.6691 - mse: 252.5647 - val_loss: 247.2072 - val_mae: 12.5922 - val_mse: 247.2072\nEpoch 191/200\n86/86 - 0s - 3ms/step - loss: 256.0273 - mae: 12.8014 - mse: 256.0273 - val_loss: 248.5992 - val_mae: 12.6240 - val_mse: 248.5992\nEpoch 192/200\n86/86 - 0s - 3ms/step - loss: 249.8657 - mae: 12.5700 - mse: 249.8657 - val_loss: 249.0905 - val_mae: 12.6452 - val_mse: 249.0905\nEpoch 193/200\n86/86 - 0s - 3ms/step - loss: 253.0085 - mae: 12.6700 - mse: 253.0085 - val_loss: 247.5946 - val_mae: 12.6033 - val_mse: 247.5946\nEpoch 194/200\n86/86 - 0s - 3ms/step - loss: 252.2908 - mae: 12.6273 - mse: 252.2908 - val_loss: 247.6086 - val_mae: 12.6314 - val_mse: 247.6086\nEpoch 195/200\n86/86 - 0s - 3ms/step - loss: 248.5775 - mae: 12.6386 - mse: 248.5775 - val_loss: 245.9001 - val_mae: 12.5815 - val_mse: 245.9001\nEpoch 196/200\n86/86 - 0s - 3ms/step - loss: 254.5846 - mae: 12.7312 - mse: 254.5846 - val_loss: 247.7819 - val_mae: 12.5684 - val_mse: 247.7819\nEpoch 197/200\n86/86 - 0s - 3ms/step - loss: 251.6949 - mae: 12.6210 - mse: 251.6949 - val_loss: 249.8721 - val_mae: 12.6831 - val_mse: 249.8721\nEpoch 198/200\n86/86 - 0s - 3ms/step - loss: 256.0367 - mae: 12.7768 - mse: 256.0367 - val_loss: 248.0795 - val_mae: 12.6074 - val_mse: 248.0795\nEpoch 199/200\n86/86 - 0s - 3ms/step - loss: 249.6297 - mae: 12.5694 - mse: 249.6297 - val_loss: 248.5117 - val_mae: 12.6278 - val_mse: 248.5117\nEpoch 200/200\n86/86 - 0s - 3ms/step - loss: 248.8774 - mae: 12.5822 - mse: 248.8774 - val_loss: 247.1253 - val_mae: 12.6171 - val_mse: 247.1253\n86/86 - 0s - 2ms/step - loss: 245.9001 - mae: 12.5815 - mse: 245.9001\nEpoch 1/200\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"86/86 - 2s - 24ms/step - loss: 722.3193 - mae: 21.1718 - mse: 722.3193 - val_loss: 379.3300 - val_mae: 15.5673 - val_mse: 379.3300\nEpoch 2/200\n86/86 - 0s - 3ms/step - loss: 446.5272 - mae: 16.5810 - mse: 446.5272 - val_loss: 318.7516 - val_mae: 14.1775 - val_mse: 318.7516\nEpoch 3/200\n86/86 - 0s - 3ms/step - loss: 387.2172 - mae: 15.5455 - mse: 387.2172 - val_loss: 308.5758 - val_mae: 13.9687 - val_mse: 308.5758\nEpoch 4/200\n86/86 - 0s - 4ms/step - loss: 369.0745 - mae: 15.1756 - mse: 369.0745 - val_loss: 307.9788 - val_mae: 13.9279 - val_mse: 307.9788\nEpoch 5/200\n86/86 - 0s - 4ms/step - loss: 356.5389 - mae: 14.9275 - mse: 356.5389 - val_loss: 300.9650 - val_mae: 13.8059 - val_mse: 300.9650\nEpoch 6/200\n86/86 - 0s - 3ms/step - loss: 352.8228 - mae: 14.9101 - mse: 352.8228 - val_loss: 295.3174 - val_mae: 13.7186 - val_mse: 295.3174\nEpoch 7/200\n86/86 - 0s - 3ms/step - loss: 344.3947 - mae: 14.7814 - mse: 344.3947 - val_loss: 296.8952 - val_mae: 13.7080 - val_mse: 296.8952\nEpoch 8/200\n86/86 - 0s - 3ms/step - loss: 337.6858 - mae: 14.5841 - mse: 337.6858 - val_loss: 296.0614 - val_mae: 13.6880 - val_mse: 296.0614\nEpoch 9/200\n86/86 - 0s - 3ms/step - loss: 341.4696 - mae: 14.7046 - mse: 341.4696 - val_loss: 293.4009 - val_mae: 13.6526 - val_mse: 293.4009\nEpoch 10/200\n86/86 - 0s - 3ms/step - loss: 337.5135 - mae: 14.5364 - mse: 337.5135 - val_loss: 293.5066 - val_mae: 13.6663 - val_mse: 293.5066\nEpoch 11/200\n86/86 - 0s - 3ms/step - loss: 333.9644 - mae: 14.5008 - mse: 333.9644 - val_loss: 290.9450 - val_mae: 13.6148 - val_mse: 290.9450\nEpoch 12/200\n86/86 - 0s - 3ms/step - loss: 334.3734 - mae: 14.4840 - mse: 334.3734 - val_loss: 289.9359 - val_mae: 13.6114 - val_mse: 289.9359\nEpoch 13/200\n86/86 - 0s - 3ms/step - loss: 326.4389 - mae: 14.4586 - mse: 326.4389 - val_loss: 288.4086 - val_mae: 13.5531 - val_mse: 288.4086\nEpoch 14/200\n86/86 - 0s - 3ms/step - loss: 328.9571 - mae: 14.4162 - mse: 328.9571 - val_loss: 289.3450 - val_mae: 13.5923 - val_mse: 289.3450\nEpoch 15/200\n86/86 - 0s - 4ms/step - loss: 323.9476 - mae: 14.2646 - mse: 323.9476 - val_loss: 287.9634 - val_mae: 13.5364 - val_mse: 287.9634\nEpoch 16/200\n86/86 - 0s - 3ms/step - loss: 318.8769 - mae: 14.2110 - mse: 318.8769 - val_loss: 287.7804 - val_mae: 13.5559 - val_mse: 287.7804\nEpoch 17/200\n86/86 - 0s - 3ms/step - loss: 325.6560 - mae: 14.4242 - mse: 325.6560 - val_loss: 287.2314 - val_mae: 13.5640 - val_mse: 287.2314\nEpoch 18/200\n86/86 - 0s - 3ms/step - loss: 319.3184 - mae: 14.2511 - mse: 319.3184 - val_loss: 286.1281 - val_mae: 13.5139 - val_mse: 286.1281\nEpoch 19/200\n86/86 - 0s - 3ms/step - loss: 324.3143 - mae: 14.3428 - mse: 324.3143 - val_loss: 286.1859 - val_mae: 13.5326 - val_mse: 286.1859\nEpoch 20/200\n86/86 - 0s - 3ms/step - loss: 319.7025 - mae: 14.1891 - mse: 319.7025 - val_loss: 288.7235 - val_mae: 13.5881 - val_mse: 288.7235\nEpoch 21/200\n86/86 - 0s - 3ms/step - loss: 316.7840 - mae: 14.2293 - mse: 316.7840 - val_loss: 286.9350 - val_mae: 13.5567 - val_mse: 286.9350\nEpoch 22/200\n86/86 - 0s - 3ms/step - loss: 317.0734 - mae: 14.1198 - mse: 317.0734 - val_loss: 286.3628 - val_mae: 13.5560 - val_mse: 286.3628\nEpoch 23/200\n86/86 - 0s - 3ms/step - loss: 316.1018 - mae: 14.1374 - mse: 316.1018 - val_loss: 285.7172 - val_mae: 13.5162 - val_mse: 285.7172\nEpoch 24/200\n86/86 - 0s - 3ms/step - loss: 318.6303 - mae: 14.1607 - mse: 318.6303 - val_loss: 286.6664 - val_mae: 13.5311 - val_mse: 286.6664\nEpoch 25/200\n86/86 - 0s - 3ms/step - loss: 312.1646 - mae: 14.0887 - mse: 312.1646 - val_loss: 285.5969 - val_mae: 13.5195 - val_mse: 285.5969\nEpoch 26/200\n86/86 - 0s - 3ms/step - loss: 311.7845 - mae: 14.0148 - mse: 311.7845 - val_loss: 283.4535 - val_mae: 13.4877 - val_mse: 283.4535\nEpoch 27/200\n86/86 - 0s - 3ms/step - loss: 311.2891 - mae: 14.0441 - mse: 311.2891 - val_loss: 283.5082 - val_mae: 13.4665 - val_mse: 283.5082\nEpoch 28/200\n86/86 - 0s - 4ms/step - loss: 305.5172 - mae: 13.9172 - mse: 305.5172 - val_loss: 283.7511 - val_mae: 13.4717 - val_mse: 283.7511\nEpoch 29/200\n86/86 - 0s - 4ms/step - loss: 310.5820 - mae: 14.0300 - mse: 310.5820 - val_loss: 282.9536 - val_mae: 13.4673 - val_mse: 282.9536\nEpoch 30/200\n86/86 - 0s - 3ms/step - loss: 307.4362 - mae: 13.9528 - mse: 307.4362 - val_loss: 283.6435 - val_mae: 13.4695 - val_mse: 283.6435\nEpoch 31/200\n86/86 - 0s - 3ms/step - loss: 308.8482 - mae: 13.9716 - mse: 308.8482 - val_loss: 282.5593 - val_mae: 13.4376 - val_mse: 282.5593\nEpoch 32/200\n86/86 - 0s - 3ms/step - loss: 307.8539 - mae: 14.0106 - mse: 307.8539 - val_loss: 281.1323 - val_mae: 13.4096 - val_mse: 281.1323\nEpoch 33/200\n86/86 - 0s - 3ms/step - loss: 307.2737 - mae: 13.9705 - mse: 307.2737 - val_loss: 281.7007 - val_mae: 13.3870 - val_mse: 281.7007\nEpoch 34/200\n86/86 - 0s - 3ms/step - loss: 309.8887 - mae: 14.0097 - mse: 309.8887 - val_loss: 280.0359 - val_mae: 13.3766 - val_mse: 280.0359\nEpoch 35/200\n86/86 - 0s - 3ms/step - loss: 301.6211 - mae: 13.8849 - mse: 301.6211 - val_loss: 277.9946 - val_mae: 13.2761 - val_mse: 277.9946\nEpoch 36/200\n86/86 - 0s - 3ms/step - loss: 298.4415 - mae: 13.7758 - mse: 298.4415 - val_loss: 278.3684 - val_mae: 13.2848 - val_mse: 278.3684\nEpoch 37/200\n86/86 - 0s - 3ms/step - loss: 305.2707 - mae: 13.9501 - mse: 305.2707 - val_loss: 277.2902 - val_mae: 13.2825 - val_mse: 277.2902\nEpoch 38/200\n86/86 - 0s - 3ms/step - loss: 301.3827 - mae: 13.8182 - mse: 301.3827 - val_loss: 279.4140 - val_mae: 13.3212 - val_mse: 279.4140\nEpoch 39/200\n86/86 - 0s - 4ms/step - loss: 298.3995 - mae: 13.7996 - mse: 298.3995 - val_loss: 281.6562 - val_mae: 13.3676 - val_mse: 281.6562\nEpoch 40/200\n86/86 - 0s - 3ms/step - loss: 303.8247 - mae: 13.8535 - mse: 303.8247 - val_loss: 280.1077 - val_mae: 13.3300 - val_mse: 280.1077\nEpoch 41/200\n86/86 - 0s - 3ms/step - loss: 293.8950 - mae: 13.6830 - mse: 293.8950 - val_loss: 278.8469 - val_mae: 13.3114 - val_mse: 278.8469\nEpoch 42/200\n86/86 - 0s - 3ms/step - loss: 299.0935 - mae: 13.8022 - mse: 299.0935 - val_loss: 278.0713 - val_mae: 13.3064 - val_mse: 278.0713\nEpoch 43/200\n86/86 - 0s - 3ms/step - loss: 299.5246 - mae: 13.8719 - mse: 299.5246 - val_loss: 275.5435 - val_mae: 13.2267 - val_mse: 275.5435\nEpoch 44/200\n86/86 - 0s - 4ms/step - loss: 292.7765 - mae: 13.6599 - mse: 292.7765 - val_loss: 277.1996 - val_mae: 13.2725 - val_mse: 277.1996\nEpoch 45/200\n86/86 - 0s - 3ms/step - loss: 298.9530 - mae: 13.7583 - mse: 298.9530 - val_loss: 276.0307 - val_mae: 13.2481 - val_mse: 276.0307\nEpoch 46/200\n86/86 - 0s - 4ms/step - loss: 294.3924 - mae: 13.6305 - mse: 294.3924 - val_loss: 275.2265 - val_mae: 13.2037 - val_mse: 275.2265\nEpoch 47/200\n86/86 - 0s - 4ms/step - loss: 295.3673 - mae: 13.7486 - mse: 295.3673 - val_loss: 276.5237 - val_mae: 13.2645 - val_mse: 276.5237\nEpoch 48/200\n86/86 - 0s - 4ms/step - loss: 290.1409 - mae: 13.5644 - mse: 290.1409 - val_loss: 276.5093 - val_mae: 13.2460 - val_mse: 276.5093\nEpoch 49/200\n86/86 - 0s - 4ms/step - loss: 293.0067 - mae: 13.6688 - mse: 293.0067 - val_loss: 273.7498 - val_mae: 13.2097 - val_mse: 273.7498\nEpoch 50/200\n86/86 - 0s - 4ms/step - loss: 288.3125 - mae: 13.5415 - mse: 288.3125 - val_loss: 275.8991 - val_mae: 13.2112 - val_mse: 275.8991\nEpoch 51/200\n86/86 - 0s - 3ms/step - loss: 291.8356 - mae: 13.6157 - mse: 291.8356 - val_loss: 274.6791 - val_mae: 13.2083 - val_mse: 274.6791\nEpoch 52/200\n86/86 - 0s - 3ms/step - loss: 293.6011 - mae: 13.6729 - mse: 293.6011 - val_loss: 274.3265 - val_mae: 13.2127 - val_mse: 274.3265\nEpoch 53/200\n86/86 - 0s - 3ms/step - loss: 292.5089 - mae: 13.6785 - mse: 292.5089 - val_loss: 273.7774 - val_mae: 13.2118 - val_mse: 273.7774\nEpoch 54/200\n86/86 - 0s - 3ms/step - loss: 287.8065 - mae: 13.5149 - mse: 287.8065 - val_loss: 270.3593 - val_mae: 13.0473 - val_mse: 270.3593\nEpoch 55/200\n86/86 - 0s - 3ms/step - loss: 288.0297 - mae: 13.5688 - mse: 288.0297 - val_loss: 270.9660 - val_mae: 13.0737 - val_mse: 270.9660\nEpoch 56/200\n86/86 - 0s - 3ms/step - loss: 292.9545 - mae: 13.6175 - mse: 292.9545 - val_loss: 273.5690 - val_mae: 13.1827 - val_mse: 273.5690\nEpoch 57/200\n86/86 - 0s - 3ms/step - loss: 291.9146 - mae: 13.5719 - mse: 291.9146 - val_loss: 269.3477 - val_mae: 13.0455 - val_mse: 269.3477\nEpoch 58/200\n86/86 - 0s - 3ms/step - loss: 287.3052 - mae: 13.4916 - mse: 287.3052 - val_loss: 271.2060 - val_mae: 13.1012 - val_mse: 271.2060\nEpoch 59/200\n86/86 - 0s - 3ms/step - loss: 286.6409 - mae: 13.4599 - mse: 286.6409 - val_loss: 272.1797 - val_mae: 13.1126 - val_mse: 272.1797\nEpoch 60/200\n86/86 - 0s - 3ms/step - loss: 289.1800 - mae: 13.5043 - mse: 289.1800 - val_loss: 270.0738 - val_mae: 13.0560 - val_mse: 270.0738\nEpoch 61/200\n86/86 - 0s - 3ms/step - loss: 285.7347 - mae: 13.5195 - mse: 285.7347 - val_loss: 271.7778 - val_mae: 13.1320 - val_mse: 271.7778\nEpoch 62/200\n86/86 - 0s - 3ms/step - loss: 289.6254 - mae: 13.5693 - mse: 289.6254 - val_loss: 269.2878 - val_mae: 13.0591 - val_mse: 269.2878\nEpoch 63/200\n86/86 - 0s - 3ms/step - loss: 283.1222 - mae: 13.4162 - mse: 283.1222 - val_loss: 270.4082 - val_mae: 13.0817 - val_mse: 270.4082\nEpoch 64/200\n86/86 - 0s - 3ms/step - loss: 284.7405 - mae: 13.4227 - mse: 284.7405 - val_loss: 271.4818 - val_mae: 13.1339 - val_mse: 271.4818\nEpoch 65/200\n86/86 - 0s - 3ms/step - loss: 282.0043 - mae: 13.3922 - mse: 282.0043 - val_loss: 269.3154 - val_mae: 13.0227 - val_mse: 269.3154\nEpoch 66/200\n86/86 - 0s - 3ms/step - loss: 285.4299 - mae: 13.4039 - mse: 285.4299 - val_loss: 272.0558 - val_mae: 13.1309 - val_mse: 272.0558\nEpoch 67/200\n86/86 - 0s - 3ms/step - loss: 284.6807 - mae: 13.4249 - mse: 284.6807 - val_loss: 269.1909 - val_mae: 13.0625 - val_mse: 269.1909\nEpoch 68/200\n86/86 - 0s - 3ms/step - loss: 279.9847 - mae: 13.3585 - mse: 279.9847 - val_loss: 272.0904 - val_mae: 13.1874 - val_mse: 272.0904\nEpoch 69/200\n86/86 - 0s - 3ms/step - loss: 288.4099 - mae: 13.5678 - mse: 288.4099 - val_loss: 268.8515 - val_mae: 13.0412 - val_mse: 268.8515\nEpoch 70/200\n86/86 - 0s - 3ms/step - loss: 280.3930 - mae: 13.3348 - mse: 280.3930 - val_loss: 267.4601 - val_mae: 13.0030 - val_mse: 267.4601\nEpoch 71/200\n86/86 - 0s - 3ms/step - loss: 282.3140 - mae: 13.4292 - mse: 282.3140 - val_loss: 270.4059 - val_mae: 13.1314 - val_mse: 270.4059\nEpoch 72/200\n86/86 - 0s - 3ms/step - loss: 281.8468 - mae: 13.3451 - mse: 281.8468 - val_loss: 268.4373 - val_mae: 13.0225 - val_mse: 268.4373\nEpoch 73/200\n86/86 - 0s - 4ms/step - loss: 282.4714 - mae: 13.3484 - mse: 282.4714 - val_loss: 268.3052 - val_mae: 13.0263 - val_mse: 268.3052\nEpoch 74/200\n86/86 - 0s - 3ms/step - loss: 276.0439 - mae: 13.2262 - mse: 276.0439 - val_loss: 267.7462 - val_mae: 13.0186 - val_mse: 267.7462\nEpoch 75/200\n86/86 - 0s - 3ms/step - loss: 280.5866 - mae: 13.3466 - mse: 280.5866 - val_loss: 269.7315 - val_mae: 13.1112 - val_mse: 269.7315\nEpoch 76/200\n86/86 - 0s - 3ms/step - loss: 274.8511 - mae: 13.2110 - mse: 274.8511 - val_loss: 267.8694 - val_mae: 13.0266 - val_mse: 267.8694\nEpoch 77/200\n86/86 - 0s - 3ms/step - loss: 277.9035 - mae: 13.3287 - mse: 277.9035 - val_loss: 269.4306 - val_mae: 13.0917 - val_mse: 269.4306\nEpoch 78/200\n86/86 - 0s - 4ms/step - loss: 276.8288 - mae: 13.2582 - mse: 276.8288 - val_loss: 267.8318 - val_mae: 13.0459 - val_mse: 267.8318\nEpoch 79/200\n86/86 - 0s - 3ms/step - loss: 273.0244 - mae: 13.1778 - mse: 273.0244 - val_loss: 268.8118 - val_mae: 13.0504 - val_mse: 268.8118\nEpoch 80/200\n86/86 - 0s - 3ms/step - loss: 279.0744 - mae: 13.3118 - mse: 279.0744 - val_loss: 267.8605 - val_mae: 13.0430 - val_mse: 267.8605\nEpoch 81/200\n86/86 - 0s - 3ms/step - loss: 279.3824 - mae: 13.3053 - mse: 279.3824 - val_loss: 267.3868 - val_mae: 13.0661 - val_mse: 267.3868\nEpoch 82/200\n86/86 - 0s - 3ms/step - loss: 272.5912 - mae: 13.1657 - mse: 272.5912 - val_loss: 266.0940 - val_mae: 12.9884 - val_mse: 266.0940\nEpoch 83/200\n86/86 - 0s - 3ms/step - loss: 274.1772 - mae: 13.2048 - mse: 274.1772 - val_loss: 266.0215 - val_mae: 13.0137 - val_mse: 266.0215\nEpoch 84/200\n86/86 - 0s - 3ms/step - loss: 269.2271 - mae: 13.0839 - mse: 269.2271 - val_loss: 266.6945 - val_mae: 13.0280 - val_mse: 266.6945\nEpoch 85/200\n86/86 - 0s - 3ms/step - loss: 277.5203 - mae: 13.2357 - mse: 277.5203 - val_loss: 265.8764 - val_mae: 13.0391 - val_mse: 265.8764\nEpoch 86/200\n86/86 - 0s - 3ms/step - loss: 271.6351 - mae: 13.1664 - mse: 271.6351 - val_loss: 267.4358 - val_mae: 13.0608 - val_mse: 267.4358\nEpoch 87/200\n86/86 - 0s - 3ms/step - loss: 273.0894 - mae: 13.1992 - mse: 273.0894 - val_loss: 266.2689 - val_mae: 12.9923 - val_mse: 266.2689\nEpoch 88/200\n86/86 - 0s - 3ms/step - loss: 269.8132 - mae: 13.0990 - mse: 269.8132 - val_loss: 265.4687 - val_mae: 13.0079 - val_mse: 265.4687\nEpoch 89/200\n86/86 - 0s - 3ms/step - loss: 274.0524 - mae: 13.1447 - mse: 274.0524 - val_loss: 265.7137 - val_mae: 12.9852 - val_mse: 265.7137\nEpoch 90/200\n86/86 - 0s - 3ms/step - loss: 270.3487 - mae: 13.0457 - mse: 270.3487 - val_loss: 267.9921 - val_mae: 13.0408 - val_mse: 267.9921\nEpoch 91/200\n86/86 - 0s - 3ms/step - loss: 269.2212 - mae: 13.0848 - mse: 269.2212 - val_loss: 265.5922 - val_mae: 13.0090 - val_mse: 265.5922\nEpoch 92/200\n86/86 - 0s - 3ms/step - loss: 274.0964 - mae: 13.2108 - mse: 274.0964 - val_loss: 266.2647 - val_mae: 13.0229 - val_mse: 266.2647\nEpoch 93/200\n86/86 - 0s - 3ms/step - loss: 269.5497 - mae: 13.1027 - mse: 269.5497 - val_loss: 265.6288 - val_mae: 12.9682 - val_mse: 265.6288\nEpoch 94/200\n86/86 - 0s - 3ms/step - loss: 269.8206 - mae: 13.0817 - mse: 269.8206 - val_loss: 265.5991 - val_mae: 12.9685 - val_mse: 265.5991\nEpoch 95/200\n86/86 - 0s - 3ms/step - loss: 269.0385 - mae: 12.9903 - mse: 269.0385 - val_loss: 265.6339 - val_mae: 12.9583 - val_mse: 265.6339\nEpoch 96/200\n86/86 - 0s - 3ms/step - loss: 268.9584 - mae: 13.0526 - mse: 268.9584 - val_loss: 264.7758 - val_mae: 12.9102 - val_mse: 264.7758\nEpoch 97/200\n86/86 - 0s - 3ms/step - loss: 271.0615 - mae: 13.1032 - mse: 271.0615 - val_loss: 264.5771 - val_mae: 12.9270 - val_mse: 264.5771\nEpoch 98/200\n86/86 - 0s - 3ms/step - loss: 267.1140 - mae: 12.9588 - mse: 267.1140 - val_loss: 264.2271 - val_mae: 12.9255 - val_mse: 264.2271\nEpoch 99/200\n86/86 - 0s - 3ms/step - loss: 272.5466 - mae: 13.1080 - mse: 272.5466 - val_loss: 264.9963 - val_mae: 12.9677 - val_mse: 264.9963\nEpoch 100/200\n86/86 - 0s - 3ms/step - loss: 271.8142 - mae: 13.0863 - mse: 271.8142 - val_loss: 263.0494 - val_mae: 12.9430 - val_mse: 263.0494\nEpoch 101/200\n86/86 - 0s - 3ms/step - loss: 268.1927 - mae: 12.9926 - mse: 268.1927 - val_loss: 261.6358 - val_mae: 12.8659 - val_mse: 261.6358\nEpoch 102/200\n86/86 - 0s - 3ms/step - loss: 266.9699 - mae: 13.0026 - mse: 266.9699 - val_loss: 262.7942 - val_mae: 12.9031 - val_mse: 262.7942\nEpoch 103/200\n86/86 - 0s - 3ms/step - loss: 273.2170 - mae: 13.1763 - mse: 273.2170 - val_loss: 264.6680 - val_mae: 13.0022 - val_mse: 264.6680\nEpoch 104/200\n86/86 - 0s - 3ms/step - loss: 273.3004 - mae: 13.1632 - mse: 273.3004 - val_loss: 264.7797 - val_mae: 13.0067 - val_mse: 264.7797\nEpoch 105/200\n86/86 - 0s - 3ms/step - loss: 269.6119 - mae: 13.0927 - mse: 269.6119 - val_loss: 265.6345 - val_mae: 12.9983 - val_mse: 265.6345\nEpoch 106/200\n86/86 - 0s - 3ms/step - loss: 267.0748 - mae: 12.9761 - mse: 267.0748 - val_loss: 264.7418 - val_mae: 12.9720 - val_mse: 264.7418\nEpoch 107/200\n86/86 - 0s - 3ms/step - loss: 268.6592 - mae: 13.0210 - mse: 268.6592 - val_loss: 263.9077 - val_mae: 12.9413 - val_mse: 263.9077\nEpoch 108/200\n86/86 - 0s - 4ms/step - loss: 269.7676 - mae: 13.0893 - mse: 269.7676 - val_loss: 265.0001 - val_mae: 12.9966 - val_mse: 265.0001\nEpoch 109/200\n86/86 - 0s - 3ms/step - loss: 262.0547 - mae: 12.8990 - mse: 262.0547 - val_loss: 265.5664 - val_mae: 12.9486 - val_mse: 265.5664\nEpoch 110/200\n86/86 - 0s - 3ms/step - loss: 266.8823 - mae: 13.0557 - mse: 266.8823 - val_loss: 266.1080 - val_mae: 13.0031 - val_mse: 266.1080\nEpoch 111/200\n86/86 - 0s - 3ms/step - loss: 265.8123 - mae: 12.9645 - mse: 265.8123 - val_loss: 264.0057 - val_mae: 12.9475 - val_mse: 264.0057\nEpoch 112/200\n86/86 - 0s - 3ms/step - loss: 266.4597 - mae: 13.0135 - mse: 266.4597 - val_loss: 264.0106 - val_mae: 12.9555 - val_mse: 264.0106\nEpoch 113/200\n86/86 - 0s - 3ms/step - loss: 262.1729 - mae: 12.9146 - mse: 262.1729 - val_loss: 263.7574 - val_mae: 12.9046 - val_mse: 263.7574\nEpoch 114/200\n86/86 - 0s - 3ms/step - loss: 261.9797 - mae: 12.9023 - mse: 261.9797 - val_loss: 263.3852 - val_mae: 12.9244 - val_mse: 263.3852\nEpoch 115/200\n86/86 - 0s - 4ms/step - loss: 262.6391 - mae: 12.9048 - mse: 262.6391 - val_loss: 263.7437 - val_mae: 12.9350 - val_mse: 263.7437\nEpoch 116/200\n86/86 - 0s - 3ms/step - loss: 269.7612 - mae: 13.1023 - mse: 269.7612 - val_loss: 263.5723 - val_mae: 12.9535 - val_mse: 263.5723\nEpoch 117/200\n86/86 - 0s - 3ms/step - loss: 265.2470 - mae: 12.9859 - mse: 265.2470 - val_loss: 265.4553 - val_mae: 12.9776 - val_mse: 265.4553\nEpoch 118/200\n86/86 - 0s - 3ms/step - loss: 266.1465 - mae: 13.0040 - mse: 266.1465 - val_loss: 260.9868 - val_mae: 12.8468 - val_mse: 260.9868\nEpoch 119/200\n86/86 - 0s - 3ms/step - loss: 263.8038 - mae: 12.9421 - mse: 263.8038 - val_loss: 260.7247 - val_mae: 12.8519 - val_mse: 260.7247\nEpoch 120/200\n86/86 - 0s - 3ms/step - loss: 259.4172 - mae: 12.8177 - mse: 259.4172 - val_loss: 259.7786 - val_mae: 12.7872 - val_mse: 259.7786\nEpoch 121/200\n86/86 - 0s - 3ms/step - loss: 265.7553 - mae: 13.0265 - mse: 265.7553 - val_loss: 262.8836 - val_mae: 12.8976 - val_mse: 262.8836\nEpoch 122/200\n86/86 - 0s - 3ms/step - loss: 262.7203 - mae: 12.9291 - mse: 262.7203 - val_loss: 261.4293 - val_mae: 12.8841 - val_mse: 261.4293\nEpoch 123/200\n86/86 - 0s - 3ms/step - loss: 260.5570 - mae: 12.8723 - mse: 260.5570 - val_loss: 261.9491 - val_mae: 12.8613 - val_mse: 261.9491\nEpoch 124/200\n86/86 - 0s - 3ms/step - loss: 261.9143 - mae: 12.8838 - mse: 261.9143 - val_loss: 261.3849 - val_mae: 12.8767 - val_mse: 261.3849\nEpoch 125/200\n86/86 - 0s - 4ms/step - loss: 260.8879 - mae: 12.8679 - mse: 260.8879 - val_loss: 262.4430 - val_mae: 12.8847 - val_mse: 262.4430\nEpoch 126/200\n86/86 - 0s - 4ms/step - loss: 262.8626 - mae: 12.9063 - mse: 262.8626 - val_loss: 262.4585 - val_mae: 12.8807 - val_mse: 262.4585\nEpoch 127/200\n86/86 - 0s - 3ms/step - loss: 256.1456 - mae: 12.7514 - mse: 256.1456 - val_loss: 261.7295 - val_mae: 12.8221 - val_mse: 261.7295\nEpoch 128/200\n86/86 - 0s - 3ms/step - loss: 261.8340 - mae: 12.9407 - mse: 261.8340 - val_loss: 261.4044 - val_mae: 12.8624 - val_mse: 261.4044\nEpoch 129/200\n86/86 - 0s - 3ms/step - loss: 263.5076 - mae: 12.9800 - mse: 263.5076 - val_loss: 262.1310 - val_mae: 12.7959 - val_mse: 262.1310\nEpoch 130/200\n86/86 - 0s - 3ms/step - loss: 263.2057 - mae: 12.9601 - mse: 263.2057 - val_loss: 260.9951 - val_mae: 12.8561 - val_mse: 260.9951\nEpoch 131/200\n86/86 - 0s - 3ms/step - loss: 261.9348 - mae: 12.9489 - mse: 261.9348 - val_loss: 262.9253 - val_mae: 12.8766 - val_mse: 262.9253\nEpoch 132/200\n86/86 - 0s - 3ms/step - loss: 264.7978 - mae: 12.9714 - mse: 264.7978 - val_loss: 262.6819 - val_mae: 12.8979 - val_mse: 262.6819\nEpoch 133/200\n86/86 - 0s - 3ms/step - loss: 259.3186 - mae: 12.8327 - mse: 259.3186 - val_loss: 261.7185 - val_mae: 12.8925 - val_mse: 261.7185\nEpoch 134/200\n86/86 - 0s - 3ms/step - loss: 262.9005 - mae: 12.9569 - mse: 262.9005 - val_loss: 258.9910 - val_mae: 12.8048 - val_mse: 258.9910\nEpoch 135/200\n86/86 - 0s - 3ms/step - loss: 261.9892 - mae: 12.9245 - mse: 261.9892 - val_loss: 259.4114 - val_mae: 12.8627 - val_mse: 259.4114\nEpoch 136/200\n86/86 - 0s - 3ms/step - loss: 259.4632 - mae: 12.8068 - mse: 259.4632 - val_loss: 260.3947 - val_mae: 12.8077 - val_mse: 260.3947\nEpoch 137/200\n86/86 - 0s - 3ms/step - loss: 262.4950 - mae: 12.9195 - mse: 262.4950 - val_loss: 259.6568 - val_mae: 12.8559 - val_mse: 259.6568\nEpoch 138/200\n86/86 - 0s - 3ms/step - loss: 261.9555 - mae: 12.9169 - mse: 261.9555 - val_loss: 260.2385 - val_mae: 12.8425 - val_mse: 260.2385\nEpoch 139/200\n86/86 - 0s - 3ms/step - loss: 254.9138 - mae: 12.6824 - mse: 254.9138 - val_loss: 260.8235 - val_mae: 12.8604 - val_mse: 260.8235\nEpoch 140/200\n86/86 - 0s - 3ms/step - loss: 262.1188 - mae: 12.8565 - mse: 262.1188 - val_loss: 259.6412 - val_mae: 12.8464 - val_mse: 259.6412\nEpoch 141/200\n86/86 - 0s - 4ms/step - loss: 257.1601 - mae: 12.8071 - mse: 257.1601 - val_loss: 260.5839 - val_mae: 12.8099 - val_mse: 260.5839\nEpoch 142/200\n86/86 - 0s - 4ms/step - loss: 257.6266 - mae: 12.7750 - mse: 257.6266 - val_loss: 262.6262 - val_mae: 12.9143 - val_mse: 262.6262\nEpoch 143/200\n86/86 - 0s - 3ms/step - loss: 259.1122 - mae: 12.7952 - mse: 259.1122 - val_loss: 262.3184 - val_mae: 12.8678 - val_mse: 262.3184\nEpoch 144/200\n86/86 - 0s - 3ms/step - loss: 258.7843 - mae: 12.8452 - mse: 258.7843 - val_loss: 262.5779 - val_mae: 12.8893 - val_mse: 262.5779\nEpoch 145/200\n86/86 - 0s - 3ms/step - loss: 256.7345 - mae: 12.8078 - mse: 256.7345 - val_loss: 260.4865 - val_mae: 12.8241 - val_mse: 260.4865\nEpoch 146/200\n86/86 - 0s - 4ms/step - loss: 258.0876 - mae: 12.7552 - mse: 258.0876 - val_loss: 261.2981 - val_mae: 12.8281 - val_mse: 261.2981\nEpoch 147/200\n86/86 - 0s - 3ms/step - loss: 251.4505 - mae: 12.5978 - mse: 251.4505 - val_loss: 260.4749 - val_mae: 12.8180 - val_mse: 260.4749\nEpoch 148/200\n86/86 - 0s - 4ms/step - loss: 256.8271 - mae: 12.7908 - mse: 256.8271 - val_loss: 260.7069 - val_mae: 12.8206 - val_mse: 260.7069\nEpoch 149/200\n86/86 - 0s - 4ms/step - loss: 258.9905 - mae: 12.7847 - mse: 258.9905 - val_loss: 259.2199 - val_mae: 12.7547 - val_mse: 259.2199\nEpoch 150/200\n86/86 - 0s - 3ms/step - loss: 256.9865 - mae: 12.7861 - mse: 256.9865 - val_loss: 261.4142 - val_mae: 12.8574 - val_mse: 261.4142\nEpoch 151/200\n86/86 - 0s - 4ms/step - loss: 257.9752 - mae: 12.8391 - mse: 257.9752 - val_loss: 261.0627 - val_mae: 12.8407 - val_mse: 261.0627\nEpoch 152/200\n86/86 - 0s - 4ms/step - loss: 257.5002 - mae: 12.7692 - mse: 257.5002 - val_loss: 259.5915 - val_mae: 12.8224 - val_mse: 259.5915\nEpoch 153/200\n86/86 - 0s - 3ms/step - loss: 257.9843 - mae: 12.8098 - mse: 257.9843 - val_loss: 261.7942 - val_mae: 12.8551 - val_mse: 261.7942\nEpoch 154/200\n86/86 - 0s - 4ms/step - loss: 258.4938 - mae: 12.7821 - mse: 258.4938 - val_loss: 259.7005 - val_mae: 12.8222 - val_mse: 259.7005\n86/86 - 0s - 2ms/step - loss: 258.9911 - mae: 12.8048 - mse: 258.9911\nAverage Validation Loss: 254.84525044759116\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"# Final aggregation and prediction","metadata":{}},{"cell_type":"code","source":"y_test = test_data[['id']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:36:11.787952Z","iopub.execute_input":"2025-01-26T06:36:11.788295Z","iopub.status.idle":"2025-01-26T06:36:11.794513Z","shell.execute_reply.started":"2025-01-26T06:36:11.788266Z","shell.execute_reply":"2025-01-26T06:36:11.793149Z"}},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":"## XGBoost prediction","metadata":{}},{"cell_type":"code","source":"#y_test['PCIAT-PCIAT_Total_xgb'] = np.round(xgb_model.predict(X_test_fimpute))\ny_test['Prediction_xgb'] = xgb_model.predict(X_test_fimpute)\ny_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:36:14.435063Z","iopub.execute_input":"2025-01-26T06:36:14.435451Z","iopub.status.idle":"2025-01-26T06:36:14.455485Z","shell.execute_reply.started":"2025-01-26T06:36:14.435415Z","shell.execute_reply":"2025-01-26T06:36:14.454530Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-51-28dceee22107>:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  y_test['Prediction_xgb'] = xgb_model.predict(X_test_fimpute)\n","output_type":"stream"},{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"         id  Prediction_xgb\n0  00008ff9        1.118548\n1  000fd460        0.316382\n2  00105258        0.708425\n3  00115b9f        0.307742\n4  0016bb22        1.327444","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Prediction_xgb</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>1.118548</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>0.316382</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>0.708425</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>0.307742</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>1.327444</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":51},{"cell_type":"markdown","source":"## LightGBM prediction","metadata":{}},{"cell_type":"code","source":"y_test['Prediction_lgbm'] = lgbm_model.predict(X_test_fimpute)\ny_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:36:17.186173Z","iopub.execute_input":"2025-01-26T06:36:17.186554Z","iopub.status.idle":"2025-01-26T06:36:17.204230Z","shell.execute_reply.started":"2025-01-26T06:36:17.186521Z","shell.execute_reply":"2025-01-26T06:36:17.202970Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-52-27cd549938d4>:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  y_test['Prediction_lgbm'] = lgbm_model.predict(X_test_fimpute)\n","output_type":"stream"},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"         id  Prediction_xgb  Prediction_lgbm\n0  00008ff9        1.118548         1.389877\n1  000fd460        0.316382         0.316111\n2  00105258        0.708425         0.543918\n3  00115b9f        0.307742         0.409966\n4  0016bb22        1.327444         1.092548","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Prediction_xgb</th>\n      <th>Prediction_lgbm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>1.118548</td>\n      <td>1.389877</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>0.316382</td>\n      <td>0.316111</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>0.708425</td>\n      <td>0.543918</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>0.307742</td>\n      <td>0.409966</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>1.327444</td>\n      <td>1.092548</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":52},{"cell_type":"markdown","source":"## DNN prediction","metadata":{}},{"cell_type":"code","source":"y_test['Prediction_dnn'] = np.mean([dnn_model.predict(X_test_pca) for dnn_model in dnn_models], axis=0)\n#y_test['PCIAT-PCIAT_Total_dnn'] = model.predict(X_test_fimpute)\ny_test.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:36:20.695494Z","iopub.execute_input":"2025-01-26T06:36:20.695894Z","iopub.status.idle":"2025-01-26T06:36:20.937206Z","shell.execute_reply.started":"2025-01-26T06:36:20.695860Z","shell.execute_reply":"2025-01-26T06:36:20.935790Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-53-4b21940564f0>:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  y_test['Prediction_dnn'] = np.mean([dnn_model.predict(X_test_pca) for dnn_model in dnn_models], axis=0)\n","output_type":"stream"},{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"         id  Prediction_xgb  Prediction_lgbm  Prediction_dnn\n0  00008ff9        1.118548         1.389877       26.245413\n1  000fd460        0.316382         0.316111       15.133430\n2  00105258        0.708425         0.543918       36.935879\n3  00115b9f        0.307742         0.409966       28.593071\n4  0016bb22        1.327444         1.092548       35.217724\n5  001f3379        0.963505         0.945176       31.586981\n6  0038ba98        1.261392         0.841561       28.595926\n7  0068a485        0.743707         0.515643       23.433853\n8  0069fbed        1.497707         1.627082       49.144543\n9  0083e397        1.290760         0.964312       32.477386","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Prediction_xgb</th>\n      <th>Prediction_lgbm</th>\n      <th>Prediction_dnn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>1.118548</td>\n      <td>1.389877</td>\n      <td>26.245413</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>0.316382</td>\n      <td>0.316111</td>\n      <td>15.133430</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>0.708425</td>\n      <td>0.543918</td>\n      <td>36.935879</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>0.307742</td>\n      <td>0.409966</td>\n      <td>28.593071</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>1.327444</td>\n      <td>1.092548</td>\n      <td>35.217724</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>001f3379</td>\n      <td>0.963505</td>\n      <td>0.945176</td>\n      <td>31.586981</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0038ba98</td>\n      <td>1.261392</td>\n      <td>0.841561</td>\n      <td>28.595926</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0068a485</td>\n      <td>0.743707</td>\n      <td>0.515643</td>\n      <td>23.433853</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0069fbed</td>\n      <td>1.497707</td>\n      <td>1.627082</td>\n      <td>49.144543</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0083e397</td>\n      <td>1.290760</td>\n      <td>0.964312</td>\n      <td>32.477386</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"# I don't want to simply output a round number for DNN prediction\n# I want to translate it to a float like the XGB and LGBM predictions\n# This maintains some measure of 'confidence' in the prediction when averaging\n# e.g., 1.04 is more confident in the final '1' prediction than 1.47, and taking the average will account for that\ndef translate_pciat_sii_prediction(pciat_pred):\n    if pciat_pred<=30: #0-30: sii=0\n        sii_pred = pciat_pred/61 #output a value between 0 and 0.5, which will round to 0 by itself, but the higher the value, the less confident the prediction\n    elif pciat_pred<50: #30.001-49.999: sii=1\n        sii_pred = ((pciat_pred-30)/20)+0.5 #translate to a value between 0 and 20, then make between 0 and 1, then between 0.5 and 1.5\n    elif pciat_pred<80: #50-79.999: sii=2\n        sii_pred = ((pciat_pred-50)/30)+1.5 #translate to a value between 0 and 30, then between 0 and 1, then between 1.5 and 2.5\n    else: #80-100: sii=3\n        sii_pred = ((pciat_pred-80)/40)+2.5 #translate to a value between 0 and 20, the between 0 and 0.5, then between 2.5 and 3\n\n    return sii_pred\n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:36:25.723722Z","iopub.execute_input":"2025-01-26T06:36:25.724163Z","iopub.status.idle":"2025-01-26T06:36:25.730429Z","shell.execute_reply.started":"2025-01-26T06:36:25.724120Z","shell.execute_reply":"2025-01-26T06:36:25.729065Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"def translate_pciat_sii_prediction(pciat_pred):\n    if pciat_pred<=30: #0-30: sii=0\n        sii_pred = pciat_pred/61 #map to a value between 0 and 0.5\n    elif pciat_pred<50: #30.001-49.999: sii=1\n        sii_pred = ((pciat_pred-30)/20)+0.5 #map to a value between 0.5 and 1.5\n    elif pciat_pred<80: #50-79.999: sii=2\n        sii_pred = ((pciat_pred-50)/30)+1.5 #map to a value between 1.5 and 2.5\n    else: #80-100: sii=3\n        sii_pred = ((pciat_pred-80)/40)+2.5 #map to a value between 2.5 and 3\n\n    return sii_pred","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#y_test['Prediction_dnn'] = y_test.apply(lambda row: 0 if row['Prediction_dnn']<=30 else \n#                             (1 if row['Prediction_dnn']<50 else (\n#                                2 if row['Prediction_dnn']<80 else (3)\n#                            )), axis=1)\ny_test['Prediction_dnn'] = y_test.apply(lambda row: translate_pciat_sii_prediction(row['Prediction_dnn']), axis=1)\ny_test.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:36:30.828156Z","iopub.execute_input":"2025-01-26T06:36:30.828577Z","iopub.status.idle":"2025-01-26T06:36:30.843253Z","shell.execute_reply.started":"2025-01-26T06:36:30.828543Z","shell.execute_reply":"2025-01-26T06:36:30.842068Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-55-38578f74d588>:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  y_test['Prediction_dnn'] = y_test.apply(lambda row: translate_pciat_sii_prediction(row['Prediction_dnn']), axis=1)\n","output_type":"stream"},{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"         id  Prediction_xgb  Prediction_lgbm  Prediction_dnn\n0  00008ff9        1.118548         1.389877        0.430253\n1  000fd460        0.316382         0.316111        0.248089\n2  00105258        0.708425         0.543918        0.846794\n3  00115b9f        0.307742         0.409966        0.468739\n4  0016bb22        1.327444         1.092548        0.760886\n5  001f3379        0.963505         0.945176        0.579349\n6  0038ba98        1.261392         0.841561        0.468786\n7  0068a485        0.743707         0.515643        0.384162\n8  0069fbed        1.497707         1.627082        1.457227\n9  0083e397        1.290760         0.964312        0.623869","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Prediction_xgb</th>\n      <th>Prediction_lgbm</th>\n      <th>Prediction_dnn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>1.118548</td>\n      <td>1.389877</td>\n      <td>0.430253</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>0.316382</td>\n      <td>0.316111</td>\n      <td>0.248089</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>0.708425</td>\n      <td>0.543918</td>\n      <td>0.846794</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>0.307742</td>\n      <td>0.409966</td>\n      <td>0.468739</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>1.327444</td>\n      <td>1.092548</td>\n      <td>0.760886</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>001f3379</td>\n      <td>0.963505</td>\n      <td>0.945176</td>\n      <td>0.579349</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0038ba98</td>\n      <td>1.261392</td>\n      <td>0.841561</td>\n      <td>0.468786</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0068a485</td>\n      <td>0.743707</td>\n      <td>0.515643</td>\n      <td>0.384162</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0069fbed</td>\n      <td>1.497707</td>\n      <td>1.627082</td>\n      <td>1.457227</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0083e397</td>\n      <td>1.290760</td>\n      <td>0.964312</td>\n      <td>0.623869</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":55},{"cell_type":"markdown","source":"## Aggregate prediction","metadata":{}},{"cell_type":"code","source":"y_test['Prediction_Aggregate'] = y_test.apply(\n    #lambda row: np.round(np.clip((row['Prediction_xgb']),0,3)),\n    #lambda row: np.round(np.clip((row['Prediction_lgbm']),0,3)),\n    #lambda row: np.round(np.clip((row['Prediction_dnn']),0,3)),\n    #lambda row: np.round(np.clip(((row['Prediction_xgb']+ row['Prediction_lgbm'])/2),0,3)),\n    #lambda row: np.round(np.clip(((row['Prediction_xgb']+ row['Prediction_dnn'])/2),0,3)),\n    #lambda row: np.round(np.clip(((row['Prediction_lgbm']+ row['Prediction_dnn'])/2),0,3)),\n    lambda row: np.round(np.clip(((row['Prediction_xgb'] + row['Prediction_lgbm'] + row['Prediction_dnn'])/3),0,3)),\naxis=1)\n\n# Target sii\ny_test['sii'] = y_test['Prediction_Aggregate']\n\ny_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T07:02:27.211038Z","iopub.execute_input":"2025-01-26T07:02:27.211438Z","iopub.status.idle":"2025-01-26T07:02:27.229302Z","shell.execute_reply.started":"2025-01-26T07:02:27.211404Z","shell.execute_reply":"2025-01-26T07:02:27.228072Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-63-19d7f60a2263>:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  y_test['Prediction_Aggregate'] = y_test.apply(\n<ipython-input-63-19d7f60a2263>:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  y_test['sii'] = y_test['Prediction_Aggregate']\n","output_type":"stream"},{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"         id  Prediction_xgb  Prediction_lgbm  Prediction_dnn  \\\n0  00008ff9        1.118548         1.389877        0.430253   \n1  000fd460        0.316382         0.316111        0.248089   \n2  00105258        0.708425         0.543918        0.846794   \n3  00115b9f        0.307742         0.409966        0.468739   \n4  0016bb22        1.327444         1.092548        0.760886   \n\n   Prediction_Aggregate  sii  \n0                   1.0  1.0  \n1                   0.0  0.0  \n2                   1.0  1.0  \n3                   0.0  0.0  \n4                   1.0  1.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Prediction_xgb</th>\n      <th>Prediction_lgbm</th>\n      <th>Prediction_dnn</th>\n      <th>Prediction_Aggregate</th>\n      <th>sii</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>1.118548</td>\n      <td>1.389877</td>\n      <td>0.430253</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>0.316382</td>\n      <td>0.316111</td>\n      <td>0.248089</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>0.708425</td>\n      <td>0.543918</td>\n      <td>0.846794</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>0.307742</td>\n      <td>0.409966</td>\n      <td>0.468739</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>1.327444</td>\n      <td>1.092548</td>\n      <td>0.760886</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":63},{"cell_type":"code","source":"solution = y_test[['id','sii']]\nsolution.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T06:58:24.715570Z","iopub.execute_input":"2025-01-26T06:58:24.715934Z","iopub.status.idle":"2025-01-26T06:58:24.724135Z","shell.execute_reply.started":"2025-01-26T06:58:24.715905Z","shell.execute_reply":"2025-01-26T06:58:24.722806Z"}},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":"# Results\n\n* XGBoost only: 0.424, 0.428\n* LightGBM only: 0.398, 0.428\n* DNN only: 0.385, 0.399\n* XGBoost + LightGBM: 0.407, 0.435\n* XGBoost + DNN: 0.397, 0.418\n* LightGBM + DNN: 0.388, 0.422\n* XGBoost + LightGBM + DNN: 0.391, 0.429","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}