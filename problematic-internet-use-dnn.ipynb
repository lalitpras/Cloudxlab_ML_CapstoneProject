{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In the previous step, we have determined the best approach to imputing missing features. In this stage, we focus on improving the quality of our DNN model, trained on labelled data, in isolation. In parallel we may seek to optimise other models, such as XGBoost, and later aggregate these models.\n\n**Constants:**\n* Feature selection, preprocessing of tabular data\n* Missing feature imputation approach, as determined previously\n* Using labelled data only\n* Using DNN model only\n\n**Varying:**\n* Including or not including actigraph data\n* PCA on included features\n* Augmenting vs not augmenting data\n* DNN model parameters and architecture\n* Using sample weights based on number of features imputed\n* Evaluation function: negative mean squared error or QWK on binned target\n* Targeting PCIAT-PCIAT_Total or sii\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\n\nimport polars as pl\nfrom glob import glob\nfrom tqdm.auto import tqdm\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport random\n\nnp.random.seed(42)\ntf.random.set_seed(42)\nrandom.seed(42)\n\nfrom sklearn.impute import KNNImputer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom tensorflow.keras import layers, models, regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\nfrom sklearn.svm import SVR\n\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:28:59.038991Z","iopub.execute_input":"2025-01-17T07:28:59.039389Z","iopub.status.idle":"2025-01-17T07:29:22.009923Z","shell.execute_reply.started":"2025-01-17T07:28:59.039357Z","shell.execute_reply":"2025-01-17T07:29:22.008796Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"train_data=pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest_data = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:29:22.011330Z","iopub.execute_input":"2025-01-17T07:29:22.012125Z","iopub.status.idle":"2025-01-17T07:29:22.097342Z","shell.execute_reply.started":"2025-01-17T07:29:22.012088Z","shell.execute_reply":"2025-01-17T07:29:22.096253Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"INPUT_DIR = \"/kaggle/input/child-mind-institute-problematic-internet-use/\"\n\n# Import aggregate fields from parquet files\n# Modified code from rsakata: https://www.kaggle.com/code/rsakata/cmi-piu-16th-place-solution\n\nfiles_train = glob(INPUT_DIR + \"series_train.parquet/*\")\n#if IS_SUBMIT:\n#    files += glob(INPUT_DIR + \"series_test.parquet/*\")\n\nlist_df_train = []\nfor file in tqdm(files_train):\n    df_series = (\n        pl.read_parquet(file)\n        .with_columns(\n            (\n                (pl.col(\"relative_date_PCIAT\") - pl.col(\"relative_date_PCIAT\").min())*24\n                + (pl.col(\"time_of_day\") // int(1e9)) / 3600\n            ).floor().cast(int).alias(\"total_hours\")\n        )\n        .filter(pl.col(\"non-wear_flag\") != 1)\n        .filter(pl.col(\"step\").count().over(\"total_hours\") == 12 * 60)\n        .group_by(\"total_hours\").agg(\n            pl.col(\"enmo\").std().alias(\"enmo_std\"),\n            pl.col(\"anglez\").std().alias(\"anglez_std\"),\n            pl.col(\"light\").std().alias(\"light_std\")\n        )\n        .with_columns(\n            (pl.col(\"total_hours\") % 24).alias(\"hour\"),\n            pl.lit(file.split(\"/\")[-1][3:]).alias(\"id\")\n        )\n    )\n    list_df_train.append(df_series.to_pandas())\n\ndf_series = pd.concat(list_df_train)\ndf_series[\"enmo_std\"] = np.log(df_series[\"enmo_std\"] + 0.01)\ndf_series[\"anglez_std\"] = np.log(df_series[\"anglez_std\"] + 1)\ndf_series[\"light_std\"] = np.log(df_series[\"light_std\"] + 0.01)\n\ndf_agg_train = df_series.groupby(\"id\")[[\"enmo_std\", \"anglez_std\", \"light_std\"]].agg([\"mean\", \"std\"]).reset_index()\ndf_agg_train.columns = [cols[0] + \"_\" + cols[1] if cols[1] != \"\" else cols[0] for cols in df_agg_train.columns]\ndf_agg_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:29:22.099136Z","iopub.execute_input":"2025-01-17T07:29:22.099448Z","iopub.status.idle":"2025-01-17T07:30:34.986401Z","shell.execute_reply.started":"2025-01-17T07:29:22.099424Z","shell.execute_reply":"2025-01-17T07:30:34.984853Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/996 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a89e7fb3b886400585c09fd8440f48d3"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"           id  enmo_std_mean  enmo_std_std  anglez_std_mean  anglez_std_std  \\\n0    00115b9f      -4.000377           NaN         1.989905             NaN   \n1    001f3379      -3.514671      0.652348         3.236993        0.678888   \n2    00f332d1      -3.071176      0.927238         3.249122        0.463244   \n3    01085eb3      -2.902040      0.791255         3.389762        0.315061   \n4    012cadd8      -2.806918      1.171675         3.337322        0.388409   \n..        ...            ...           ...              ...             ...   \n964  fe9c71d8      -3.116904      0.961804         3.037607        0.943554   \n965  fecc07d6      -3.969482      0.981531         1.332831        1.428363   \n966  ff18b749      -2.820076      0.937540         3.258458        0.417267   \n967  ffcd4dbd      -3.271800      0.827489         3.183395        0.629553   \n968  ffed1dd5      -3.359100      1.151259         2.489726        1.391600   \n\n     light_std_mean  light_std_std  \n0          0.051475            NaN  \n1          0.774591       2.945807  \n2          1.138379       2.939823  \n3          1.054698       2.185839  \n4          0.823770       3.350365  \n..              ...            ...  \n964       -0.394200       2.742634  \n965       -0.438018       1.795653  \n966        1.236652       3.341580  \n967        0.521227       2.665325  \n968       -1.597793       2.344771  \n\n[969 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>enmo_std_mean</th>\n      <th>enmo_std_std</th>\n      <th>anglez_std_mean</th>\n      <th>anglez_std_std</th>\n      <th>light_std_mean</th>\n      <th>light_std_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00115b9f</td>\n      <td>-4.000377</td>\n      <td>NaN</td>\n      <td>1.989905</td>\n      <td>NaN</td>\n      <td>0.051475</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001f3379</td>\n      <td>-3.514671</td>\n      <td>0.652348</td>\n      <td>3.236993</td>\n      <td>0.678888</td>\n      <td>0.774591</td>\n      <td>2.945807</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00f332d1</td>\n      <td>-3.071176</td>\n      <td>0.927238</td>\n      <td>3.249122</td>\n      <td>0.463244</td>\n      <td>1.138379</td>\n      <td>2.939823</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>01085eb3</td>\n      <td>-2.902040</td>\n      <td>0.791255</td>\n      <td>3.389762</td>\n      <td>0.315061</td>\n      <td>1.054698</td>\n      <td>2.185839</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>012cadd8</td>\n      <td>-2.806918</td>\n      <td>1.171675</td>\n      <td>3.337322</td>\n      <td>0.388409</td>\n      <td>0.823770</td>\n      <td>3.350365</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>964</th>\n      <td>fe9c71d8</td>\n      <td>-3.116904</td>\n      <td>0.961804</td>\n      <td>3.037607</td>\n      <td>0.943554</td>\n      <td>-0.394200</td>\n      <td>2.742634</td>\n    </tr>\n    <tr>\n      <th>965</th>\n      <td>fecc07d6</td>\n      <td>-3.969482</td>\n      <td>0.981531</td>\n      <td>1.332831</td>\n      <td>1.428363</td>\n      <td>-0.438018</td>\n      <td>1.795653</td>\n    </tr>\n    <tr>\n      <th>966</th>\n      <td>ff18b749</td>\n      <td>-2.820076</td>\n      <td>0.937540</td>\n      <td>3.258458</td>\n      <td>0.417267</td>\n      <td>1.236652</td>\n      <td>3.341580</td>\n    </tr>\n    <tr>\n      <th>967</th>\n      <td>ffcd4dbd</td>\n      <td>-3.271800</td>\n      <td>0.827489</td>\n      <td>3.183395</td>\n      <td>0.629553</td>\n      <td>0.521227</td>\n      <td>2.665325</td>\n    </tr>\n    <tr>\n      <th>968</th>\n      <td>ffed1dd5</td>\n      <td>-3.359100</td>\n      <td>1.151259</td>\n      <td>2.489726</td>\n      <td>1.391600</td>\n      <td>-1.597793</td>\n      <td>2.344771</td>\n    </tr>\n  </tbody>\n</table>\n<p>969 rows × 7 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"train_data2 = train_data.merge(df_agg_train, how=\"left\", on=\"id\")\ntrain_data2.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:30:42.390530Z","iopub.execute_input":"2025-01-17T07:30:42.391124Z","iopub.status.idle":"2025-01-17T07:30:42.442827Z","shell.execute_reply.started":"2025-01-17T07:30:42.391084Z","shell.execute_reply":"2025-01-17T07:30:42.440805Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"         id Basic_Demos-Enroll_Season  Basic_Demos-Age  Basic_Demos-Sex  \\\n0  00008ff9                      Fall                5                0   \n1  000fd460                    Summer                9                0   \n2  00105258                    Summer               10                1   \n3  00115b9f                    Winter                9                0   \n4  0016bb22                    Spring               18                1   \n\n  CGAS-Season  CGAS-CGAS_Score Physical-Season  Physical-BMI  Physical-Height  \\\n0      Winter             51.0            Fall     16.877316             46.0   \n1         NaN              NaN            Fall     14.035590             48.0   \n2        Fall             71.0            Fall     16.648696             56.5   \n3        Fall             71.0          Summer     18.292347             56.0   \n4      Summer              NaN             NaN           NaN              NaN   \n\n   Physical-Weight  ...  SDS-SDS_Total_T  PreInt_EduHx-Season  \\\n0             50.8  ...              NaN                 Fall   \n1             46.0  ...             64.0               Summer   \n2             75.6  ...             54.0               Summer   \n3             81.6  ...             45.0               Winter   \n4              NaN  ...              NaN                  NaN   \n\n   PreInt_EduHx-computerinternet_hoursday  sii enmo_std_mean  enmo_std_std  \\\n0                                     3.0  2.0           NaN           NaN   \n1                                     0.0  0.0           NaN           NaN   \n2                                     2.0  0.0           NaN           NaN   \n3                                     0.0  1.0     -4.000377           NaN   \n4                                     NaN  NaN           NaN           NaN   \n\n   anglez_std_mean  anglez_std_std light_std_mean  light_std_std  \n0              NaN             NaN            NaN            NaN  \n1              NaN             NaN            NaN            NaN  \n2              NaN             NaN            NaN            NaN  \n3         1.989905             NaN       0.051475            NaN  \n4              NaN             NaN            NaN            NaN  \n\n[5 rows x 88 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Basic_Demos-Enroll_Season</th>\n      <th>Basic_Demos-Age</th>\n      <th>Basic_Demos-Sex</th>\n      <th>CGAS-Season</th>\n      <th>CGAS-CGAS_Score</th>\n      <th>Physical-Season</th>\n      <th>Physical-BMI</th>\n      <th>Physical-Height</th>\n      <th>Physical-Weight</th>\n      <th>...</th>\n      <th>SDS-SDS_Total_T</th>\n      <th>PreInt_EduHx-Season</th>\n      <th>PreInt_EduHx-computerinternet_hoursday</th>\n      <th>sii</th>\n      <th>enmo_std_mean</th>\n      <th>enmo_std_std</th>\n      <th>anglez_std_mean</th>\n      <th>anglez_std_std</th>\n      <th>light_std_mean</th>\n      <th>light_std_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>Fall</td>\n      <td>5</td>\n      <td>0</td>\n      <td>Winter</td>\n      <td>51.0</td>\n      <td>Fall</td>\n      <td>16.877316</td>\n      <td>46.0</td>\n      <td>50.8</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>Fall</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>Summer</td>\n      <td>9</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Fall</td>\n      <td>14.035590</td>\n      <td>48.0</td>\n      <td>46.0</td>\n      <td>...</td>\n      <td>64.0</td>\n      <td>Summer</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>Summer</td>\n      <td>10</td>\n      <td>1</td>\n      <td>Fall</td>\n      <td>71.0</td>\n      <td>Fall</td>\n      <td>16.648696</td>\n      <td>56.5</td>\n      <td>75.6</td>\n      <td>...</td>\n      <td>54.0</td>\n      <td>Summer</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>Winter</td>\n      <td>9</td>\n      <td>0</td>\n      <td>Fall</td>\n      <td>71.0</td>\n      <td>Summer</td>\n      <td>18.292347</td>\n      <td>56.0</td>\n      <td>81.6</td>\n      <td>...</td>\n      <td>45.0</td>\n      <td>Winter</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-4.000377</td>\n      <td>NaN</td>\n      <td>1.989905</td>\n      <td>NaN</td>\n      <td>0.051475</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>Spring</td>\n      <td>18</td>\n      <td>1</td>\n      <td>Summer</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 88 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"files_test = glob(INPUT_DIR + \"series_test.parquet/*\")\n\n\nlist_df_test = []\nfor file in tqdm(files_test):\n    df_series = (\n        pl.read_parquet(file)\n        .with_columns(\n            (\n                (pl.col(\"relative_date_PCIAT\") - pl.col(\"relative_date_PCIAT\").min())*24\n                + (pl.col(\"time_of_day\") // int(1e9)) / 3600\n            ).floor().cast(int).alias(\"total_hours\")\n        )\n        .filter(pl.col(\"non-wear_flag\") != 1)\n        .filter(pl.col(\"step\").count().over(\"total_hours\") == 12 * 60)\n        .group_by(\"total_hours\").agg(\n            pl.col(\"enmo\").std().alias(\"enmo_std\"),\n            pl.col(\"anglez\").std().alias(\"anglez_std\"),\n            pl.col(\"light\").std().alias(\"light_std\")\n        )\n        .with_columns(\n            (pl.col(\"total_hours\") % 24).alias(\"hour\"),\n            pl.lit(file.split(\"/\")[-1][3:]).alias(\"id\")\n        )\n    )\n    list_df_test.append(df_series.to_pandas())\n\ndf_series = pd.concat(list_df_test)\ndf_series[\"enmo_std\"] = np.log(df_series[\"enmo_std\"] + 0.01)\ndf_series[\"anglez_std\"] = np.log(df_series[\"anglez_std\"] + 1)\ndf_series[\"light_std\"] = np.log(df_series[\"light_std\"] + 0.01)\n\ndf_agg_test = df_series.groupby(\"id\")[[\"enmo_std\", \"anglez_std\", \"light_std\"]].agg([\"mean\", \"std\"]).reset_index()\ndf_agg_test.columns = [cols[0] + \"_\" + cols[1] if cols[1] != \"\" else cols[0] for cols in df_agg_test.columns]\ndf_agg_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:30:45.461273Z","iopub.execute_input":"2025-01-17T07:30:45.461661Z","iopub.status.idle":"2025-01-17T07:30:45.678440Z","shell.execute_reply.started":"2025-01-17T07:30:45.461631Z","shell.execute_reply":"2025-01-17T07:30:45.677313Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18db3bf3174f4c848a6f41ccad3bc9b2"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"         id  enmo_std_mean  enmo_std_std  anglez_std_mean  anglez_std_std  \\\n0  00115b9f      -4.000377           NaN         1.989905             NaN   \n1  001f3379      -3.514671      0.652348         3.236993        0.678888   \n\n   light_std_mean  light_std_std  \n0        0.051475            NaN  \n1        0.774591       2.945807  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>enmo_std_mean</th>\n      <th>enmo_std_std</th>\n      <th>anglez_std_mean</th>\n      <th>anglez_std_std</th>\n      <th>light_std_mean</th>\n      <th>light_std_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00115b9f</td>\n      <td>-4.000377</td>\n      <td>NaN</td>\n      <td>1.989905</td>\n      <td>NaN</td>\n      <td>0.051475</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001f3379</td>\n      <td>-3.514671</td>\n      <td>0.652348</td>\n      <td>3.236993</td>\n      <td>0.678888</td>\n      <td>0.774591</td>\n      <td>2.945807</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"test_data2 = test_data.merge(df_agg_test, how=\"left\", on=\"id\")\ntest_data2.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:30:48.700833Z","iopub.execute_input":"2025-01-17T07:30:48.701194Z","iopub.status.idle":"2025-01-17T07:30:48.730846Z","shell.execute_reply.started":"2025-01-17T07:30:48.701165Z","shell.execute_reply":"2025-01-17T07:30:48.729615Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"         id Basic_Demos-Enroll_Season  Basic_Demos-Age  Basic_Demos-Sex  \\\n0  00008ff9                      Fall                5                0   \n1  000fd460                    Summer                9                0   \n2  00105258                    Summer               10                1   \n3  00115b9f                    Winter                9                0   \n4  0016bb22                    Spring               18                1   \n\n  CGAS-Season  CGAS-CGAS_Score Physical-Season  Physical-BMI  Physical-Height  \\\n0      Winter             51.0            Fall     16.877316             46.0   \n1         NaN              NaN            Fall     14.035590             48.0   \n2        Fall             71.0            Fall     16.648696             56.5   \n3        Fall             71.0          Summer     18.292347             56.0   \n4      Summer              NaN             NaN           NaN              NaN   \n\n   Physical-Weight  ...  SDS-SDS_Total_Raw  SDS-SDS_Total_T  \\\n0             50.8  ...                NaN              NaN   \n1             46.0  ...               46.0             64.0   \n2             75.6  ...               38.0             54.0   \n3             81.6  ...               31.0             45.0   \n4              NaN  ...                NaN              NaN   \n\n   PreInt_EduHx-Season  PreInt_EduHx-computerinternet_hoursday enmo_std_mean  \\\n0                 Fall                                     3.0           NaN   \n1               Summer                                     0.0           NaN   \n2               Summer                                     2.0           NaN   \n3               Winter                                     0.0     -4.000377   \n4                  NaN                                     NaN           NaN   \n\n   enmo_std_std  anglez_std_mean  anglez_std_std light_std_mean  light_std_std  \n0           NaN              NaN             NaN            NaN            NaN  \n1           NaN              NaN             NaN            NaN            NaN  \n2           NaN              NaN             NaN            NaN            NaN  \n3           NaN         1.989905             NaN       0.051475            NaN  \n4           NaN              NaN             NaN            NaN            NaN  \n\n[5 rows x 65 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Basic_Demos-Enroll_Season</th>\n      <th>Basic_Demos-Age</th>\n      <th>Basic_Demos-Sex</th>\n      <th>CGAS-Season</th>\n      <th>CGAS-CGAS_Score</th>\n      <th>Physical-Season</th>\n      <th>Physical-BMI</th>\n      <th>Physical-Height</th>\n      <th>Physical-Weight</th>\n      <th>...</th>\n      <th>SDS-SDS_Total_Raw</th>\n      <th>SDS-SDS_Total_T</th>\n      <th>PreInt_EduHx-Season</th>\n      <th>PreInt_EduHx-computerinternet_hoursday</th>\n      <th>enmo_std_mean</th>\n      <th>enmo_std_std</th>\n      <th>anglez_std_mean</th>\n      <th>anglez_std_std</th>\n      <th>light_std_mean</th>\n      <th>light_std_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>Fall</td>\n      <td>5</td>\n      <td>0</td>\n      <td>Winter</td>\n      <td>51.0</td>\n      <td>Fall</td>\n      <td>16.877316</td>\n      <td>46.0</td>\n      <td>50.8</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Fall</td>\n      <td>3.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>Summer</td>\n      <td>9</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Fall</td>\n      <td>14.035590</td>\n      <td>48.0</td>\n      <td>46.0</td>\n      <td>...</td>\n      <td>46.0</td>\n      <td>64.0</td>\n      <td>Summer</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>Summer</td>\n      <td>10</td>\n      <td>1</td>\n      <td>Fall</td>\n      <td>71.0</td>\n      <td>Fall</td>\n      <td>16.648696</td>\n      <td>56.5</td>\n      <td>75.6</td>\n      <td>...</td>\n      <td>38.0</td>\n      <td>54.0</td>\n      <td>Summer</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>Winter</td>\n      <td>9</td>\n      <td>0</td>\n      <td>Fall</td>\n      <td>71.0</td>\n      <td>Summer</td>\n      <td>18.292347</td>\n      <td>56.0</td>\n      <td>81.6</td>\n      <td>...</td>\n      <td>31.0</td>\n      <td>45.0</td>\n      <td>Winter</td>\n      <td>0.0</td>\n      <td>-4.000377</td>\n      <td>NaN</td>\n      <td>1.989905</td>\n      <td>NaN</td>\n      <td>0.051475</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>Spring</td>\n      <td>18</td>\n      <td>1</td>\n      <td>Summer</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 65 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"X_train = train_data2[['Basic_Demos-Age',\n                      'Basic_Demos-Sex',\n                      'CGAS-CGAS_Score',\n                      'Physical-BMI',\n                      'BIA-BIA_BMI',\n                      'Physical-Waist_Circumference',\n                      'Physical-Diastolic_BP',\n                      'Physical-HeartRate',\n                      'Physical-Systolic_BP',\n                      'Fitness_Endurance-Max_Stage',\n                      'Fitness_Endurance-Time_Mins',\n                      'Fitness_Endurance-Time_Sec',\n                      'FGC-FGC_CU_Zone',\n                      'FGC-FGC_GSND_Zone',\n                      'FGC-FGC_GSD_Zone',\n                      'FGC-FGC_PU_Zone',\n                      'FGC-FGC_SRL_Zone',\n                      'FGC-FGC_SRR_Zone',\n                      'FGC-FGC_TL_Zone',\n                      'BIA-BIA_Activity_Level_num',\n                      'BIA-BIA_BMC',\n                      'BIA-BIA_BMR',\n                      'BIA-BIA_DEE',\n                      'BIA-BIA_ECW',\n                      'BIA-BIA_FFM',\n                      'BIA-BIA_FFMI',\n                      'BIA-BIA_FMI',\n                      'BIA-BIA_Fat',\n                      'BIA-BIA_ICW',\n                      'BIA-BIA_LDM',\n                      'BIA-BIA_LST',\n                      'BIA-BIA_SMM',\n                      'BIA-BIA_TBW',\n                      'PAQ_A-PAQ_A_Total',\n                      'PAQ_C-PAQ_C_Total',\n                      'SDS-SDS_Total_T',\n                      'PreInt_EduHx-computerinternet_hoursday'\n                      # ,\n                      #'enmo_std_mean',\n                      #'enmo_std_std',\n                      #'anglez_std_mean',\n                      #'anglez_std_std',\n                      #'light_std_mean',\n                      #'light_std_std',\n                      ]]\n\ny_train = train_data2['PCIAT-PCIAT_Total']\n\nX_test = test_data2[['Basic_Demos-Age',\n                      'Basic_Demos-Sex',\n                      'CGAS-CGAS_Score',\n                      'Physical-BMI',\n                      'BIA-BIA_BMI',\n                      'Physical-Waist_Circumference',\n                      'Physical-Diastolic_BP',\n                      'Physical-HeartRate',\n                      'Physical-Systolic_BP',\n                      'Fitness_Endurance-Max_Stage',\n                      'Fitness_Endurance-Time_Mins',\n                      'Fitness_Endurance-Time_Sec',\n                      'FGC-FGC_CU_Zone',\n                      'FGC-FGC_GSND_Zone',\n                      'FGC-FGC_GSD_Zone',\n                      'FGC-FGC_PU_Zone',\n                      'FGC-FGC_SRL_Zone',\n                      'FGC-FGC_SRR_Zone',\n                      'FGC-FGC_TL_Zone',\n                      'BIA-BIA_Activity_Level_num',\n                      'BIA-BIA_BMC',\n                      'BIA-BIA_BMR',\n                      'BIA-BIA_DEE',\n                      'BIA-BIA_ECW',\n                      'BIA-BIA_FFM',\n                      'BIA-BIA_FFMI',\n                      'BIA-BIA_FMI',\n                      'BIA-BIA_Fat',\n                      'BIA-BIA_ICW',\n                      'BIA-BIA_LDM',\n                      'BIA-BIA_LST',\n                      'BIA-BIA_SMM',\n                      'BIA-BIA_TBW',\n                      'PAQ_A-PAQ_A_Total',\n                      'PAQ_C-PAQ_C_Total',\n                      'SDS-SDS_Total_T',\n                      'PreInt_EduHx-computerinternet_hoursday'\n                      #,\n                      #'enmo_std_mean',\n                      #'enmo_std_std',\n                      #'anglez_std_mean',\n                      #'anglez_std_std',\n                      #'light_std_mean',\n                      #'light_std_std',\n                   ]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:30:49.988388Z","iopub.execute_input":"2025-01-17T07:30:49.988776Z","iopub.status.idle":"2025-01-17T07:30:49.998486Z","shell.execute_reply.started":"2025-01-17T07:30:49.988746Z","shell.execute_reply":"2025-01-17T07:30:49.997334Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Add calculated fields\nX_train['Physical-BMI_Calc'] = X_train.apply(lambda row: row['Physical-BMI'] if row['Physical-BMI']==row['Physical-BMI'] else row['BIA-BIA_BMI'],axis=1)\nX_train['Fitness_Endurance-Time_Sec_Calc'] = X_train.apply(lambda row: row['Fitness_Endurance-Time_Sec'] + (row['Fitness_Endurance-Time_Mins']*60), axis=1)\nX_train['PAQ_Total'] = X_train.apply(lambda row: row['PAQ_A-PAQ_A_Total'] if row['PAQ_A-PAQ_A_Total']==row['PAQ_A-PAQ_A_Total'] else row['PAQ_C-PAQ_C_Total'],axis=1)\n\n\n# Drop fields no longer needed\nX_train = X_train.drop(columns=['PAQ_A-PAQ_A_Total','PAQ_C-PAQ_C_Total',\n                     'Physical-BMI','BIA-BIA_BMI',\n                     'Fitness_Endurance-Time_Mins','Fitness_Endurance-Time_Sec'])\n\n# Remove outliers - may give warnings due to NaN value comparison\nX_train.loc[X_train['CGAS-CGAS_Score']>=100.0,'CGAS-CGAS_Score'] = np.nan\nX_train.loc[X_train['Physical-Systolic_BP']>=180.0,'Physical-Systolic_BP'] = np.nan\nX_train.loc[X_train['Physical-Diastolic_BP']>=120.0,'Physical-Diastolic_BP'] = np.nan\nX_train.loc[X_train['BIA-BIA_DEE']>=6000.0,'BIA-BIA_DEE'] = np.nan\nX_train.loc[(X_train['BIA-BIA_BMC']<=0.0) | (X_train['BIA-BIA_BMC']>=16.0),'BIA-BIA_BMC'] = np.nan\nX_train.loc[(X_train['BIA-BIA_BMR']<=0.0) | (X_train['BIA-BIA_BMR']>=2400.0),'BIA-BIA_BMR'] = np.nan\nX_train.loc[(X_train['BIA-BIA_ECW']<=0.0) | (X_train['BIA-BIA_ECW']>=60.0),'BIA-BIA_ECW'] = np.nan\nX_train.loc[(X_train['BIA-BIA_FFM']<=0.0) | (X_train['BIA-BIA_FFM']>=200.0),'BIA-BIA_FFM'] = np.nan\nX_train.loc[(X_train['BIA-BIA_FFMI']<=0.0) | (X_train['BIA-BIA_FFMI']>=25.0),'BIA-BIA_FFMI'] = np.nan\nX_train.loc[(X_train['BIA-BIA_FMI']<=0.0) | (X_train['BIA-BIA_FMI']>=25.0),'BIA-BIA_FMI'] = np.nan\nX_train.loc[(X_train['BIA-BIA_Fat']<=8.0) | (X_train['BIA-BIA_Fat']>=60.0),'BIA-BIA_Fat'] = np.nan\nX_train.loc[(X_train['BIA-BIA_ICW']<=0.0) | (X_train['BIA-BIA_ICW']>=80.0),'BIA-BIA_ICW'] = np.nan\nX_train.loc[(X_train['BIA-BIA_LDM']<=0.0) | (X_train['BIA-BIA_LDM']>=60.0),'BIA-BIA_LDM'] = np.nan\nX_train.loc[(X_train['BIA-BIA_LST']<=0.0) | (X_train['BIA-BIA_LST']>=150.0),'BIA-BIA_LST'] = np.nan\nX_train.loc[(X_train['BIA-BIA_SMM']<=0.0) | (X_train['BIA-BIA_SMM']>=100.0),'BIA-BIA_SMM'] = np.nan\nX_train.loc[(X_train['BIA-BIA_TBW']<=0.0) | (X_train['BIA-BIA_TBW']>=150.0),'BIA-BIA_TBW'] = np.nan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:30:52.260802Z","iopub.execute_input":"2025-01-17T07:30:52.261181Z","iopub.status.idle":"2025-01-17T07:30:52.437738Z","shell.execute_reply.started":"2025-01-17T07:30:52.261151Z","shell.execute_reply":"2025-01-17T07:30:52.436533Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-8-7507d9027830>:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_train['Physical-BMI_Calc'] = X_train.apply(lambda row: row['Physical-BMI'] if row['Physical-BMI']==row['Physical-BMI'] else row['BIA-BIA_BMI'],axis=1)\n<ipython-input-8-7507d9027830>:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_train['Fitness_Endurance-Time_Sec_Calc'] = X_train.apply(lambda row: row['Fitness_Endurance-Time_Sec'] + (row['Fitness_Endurance-Time_Mins']*60), axis=1)\n<ipython-input-8-7507d9027830>:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_train['PAQ_Total'] = X_train.apply(lambda row: row['PAQ_A-PAQ_A_Total'] if row['PAQ_A-PAQ_A_Total']==row['PAQ_A-PAQ_A_Total'] else row['PAQ_C-PAQ_C_Total'],axis=1)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Noting number of features we are imputing\n# These different weighting strategies will be tried when fitting our model\n\n# Approach 1 - proportion of total features imputed\nfeatures_missing = X_train.isnull().sum(axis=1)/X_train.shape[1]\nweights = 1 - features_missing  # Weighting based on missingness score\n\n# Approach 2 - geometric reduction in weight by features imputed\nfeatures_missing2 = X_train.isnull().sum(axis=1)\nweights2 = 1 * ((0.95)**features_missing2)\n\n# Approach 3 - exponential decay. Intention is to heavily penalise rows with most features missing\nweights3 = np.exp((-2)*features_missing)\nweights3.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:30:56.238214Z","iopub.execute_input":"2025-01-17T07:30:56.238581Z","iopub.status.idle":"2025-01-17T07:30:56.258484Z","shell.execute_reply.started":"2025-01-17T07:30:56.238554Z","shell.execute_reply":"2025-01-17T07:30:56.257021Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0    0.555306\n1    0.702619\n2    0.413808\n3    0.838223\n4    0.161455\n5    0.790338\n6    0.662480\n7    0.790338\n8    0.161455\n9    0.152231\ndtype: float64"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"features_missing_labelled = X_train.loc[y_train.notna()].isnull().sum(axis=1)/X_train.shape[1]\nweights_labelled = 1 - features_missing_labelled\nweights_labelled.shape\n\nfeatures_missing_labelled2 = X_train.loc[y_train.notna()].isnull().sum(axis=1)\nweights_labelled2 = 1 * ((0.95)**features_missing_labelled2)\n\nweights_labelled3 = np.exp((-2)*features_missing_labelled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:30:58.185049Z","iopub.execute_input":"2025-01-17T07:30:58.185427Z","iopub.status.idle":"2025-01-17T07:30:58.200617Z","shell.execute_reply.started":"2025-01-17T07:30:58.185399Z","shell.execute_reply":"2025-01-17T07:30:58.199356Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"#MICE\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\niter_imputer = IterativeImputer(max_iter=10, random_state=42)\nX_train_fimpute = pd.DataFrame(iter_imputer.fit_transform(X_train), columns = X_train.columns)\nX_train_fimpute.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:30:59.435085Z","iopub.execute_input":"2025-01-17T07:30:59.435422Z","iopub.status.idle":"2025-01-17T07:31:00.552845Z","shell.execute_reply.started":"2025-01-17T07:30:59.435396Z","shell.execute_reply":"2025-01-17T07:31:00.551803Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"       Basic_Demos-Age  Basic_Demos-Sex  CGAS-CGAS_Score  \\\ncount      3960.000000      3960.000000      3960.000000   \nmean         10.433586         0.372727        64.818333   \nstd           3.574648         0.483591         9.805847   \nmin           5.000000         0.000000       -23.164848   \n25%           8.000000         0.000000        60.000000   \n50%          10.000000         0.000000        64.765647   \n75%          13.000000         1.000000        70.000000   \nmax          22.000000         1.000000        95.000000   \n\n       Physical-Waist_Circumference  Physical-Diastolic_BP  \\\ncount                   3960.000000            3960.000000   \nmean                      26.974426              69.403169   \nstd                        4.770198              11.011761   \nmin                        4.967968               0.000000   \n25%                       24.000000              64.000000   \n50%                       26.137767              69.000000   \n75%                       29.035486              73.000000   \nmax                       56.944995             119.000000   \n\n       Physical-HeartRate  Physical-Systolic_BP  Fitness_Endurance-Max_Stage  \\\ncount         3960.000000           3960.000000                  3960.000000   \nmean            81.519388            116.805724                     4.974030   \nstd             12.021297             14.228392                     0.935016   \nmin             27.000000              0.000000                     0.000000   \n25%             74.477905            109.000000                     4.865373   \n50%             81.485263            115.604541                     4.973003   \n75%             87.000000            122.000000                     5.081351   \nmax            138.000000            179.000000                    28.000000   \n\n       FGC-FGC_CU_Zone  FGC-FGC_GSND_Zone  ...  BIA-BIA_ICW  BIA-BIA_LDM  \\\ncount      3960.000000        3960.000000  ...  3960.000000  3960.000000   \nmean          0.480343           1.849340  ...    31.365620    18.066786   \nstd           0.389401           0.346190  ...     7.099089     5.021442   \nmin          -0.172314          -1.772461  ...    14.489000     4.635810   \n25%           0.000000           1.766934  ...    28.665877    16.394825   \n50%           0.485209           1.872826  ...    31.347850    18.058831   \n75%           1.000000           2.000000  ...    32.689980    18.232694   \nmax           1.129295           3.000000  ...    86.587576    52.527500   \n\n       BIA-BIA_LST  BIA-BIA_SMM  BIA-BIA_TBW  SDS-SDS_Total_T  \\\ncount  3960.000000  3960.000000  3960.000000      3960.000000   \nmean     63.749033    31.477562    50.138601        57.881433   \nstd      18.068723    10.250418    13.839926        10.737030   \nmin      23.620100     4.655730    20.589200        38.000000   \n25%      56.747700    27.190300    44.819225        51.000000   \n50%      63.734357    30.792761    50.135566        57.770114   \n75%      63.917531    33.007960    50.147145        60.000000   \nmax     188.145195   111.835760   146.075000       100.000000   \n\n       PreInt_EduHx-computerinternet_hoursday  Physical-BMI_Calc  \\\ncount                             3960.000000        3960.000000   \nmean                                 1.073783          19.374281   \nstd                                  1.022870           4.520383   \nmin                                 -0.278039           0.000000   \n25%                                  0.000000          16.485531   \n50%                                  1.000000          18.589876   \n75%                                  2.000000          21.015855   \nmax                                  3.019947          59.132048   \n\n       Fitness_Endurance-Time_Sec_Calc    PAQ_Total  \ncount                      3960.000000  3960.000000  \nmean                        468.721596     2.537644  \nstd                          89.856124     0.653839  \nmin                           5.000000     0.580000  \n25%                         450.588533     2.150000  \n50%                         469.207062     2.567600  \n75%                         487.416029     2.886524  \nmax                        2154.275208     4.790000  \n\n[8 rows x 34 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Basic_Demos-Age</th>\n      <th>Basic_Demos-Sex</th>\n      <th>CGAS-CGAS_Score</th>\n      <th>Physical-Waist_Circumference</th>\n      <th>Physical-Diastolic_BP</th>\n      <th>Physical-HeartRate</th>\n      <th>Physical-Systolic_BP</th>\n      <th>Fitness_Endurance-Max_Stage</th>\n      <th>FGC-FGC_CU_Zone</th>\n      <th>FGC-FGC_GSND_Zone</th>\n      <th>...</th>\n      <th>BIA-BIA_ICW</th>\n      <th>BIA-BIA_LDM</th>\n      <th>BIA-BIA_LST</th>\n      <th>BIA-BIA_SMM</th>\n      <th>BIA-BIA_TBW</th>\n      <th>SDS-SDS_Total_T</th>\n      <th>PreInt_EduHx-computerinternet_hoursday</th>\n      <th>Physical-BMI_Calc</th>\n      <th>Fitness_Endurance-Time_Sec_Calc</th>\n      <th>PAQ_Total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>...</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>10.433586</td>\n      <td>0.372727</td>\n      <td>64.818333</td>\n      <td>26.974426</td>\n      <td>69.403169</td>\n      <td>81.519388</td>\n      <td>116.805724</td>\n      <td>4.974030</td>\n      <td>0.480343</td>\n      <td>1.849340</td>\n      <td>...</td>\n      <td>31.365620</td>\n      <td>18.066786</td>\n      <td>63.749033</td>\n      <td>31.477562</td>\n      <td>50.138601</td>\n      <td>57.881433</td>\n      <td>1.073783</td>\n      <td>19.374281</td>\n      <td>468.721596</td>\n      <td>2.537644</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3.574648</td>\n      <td>0.483591</td>\n      <td>9.805847</td>\n      <td>4.770198</td>\n      <td>11.011761</td>\n      <td>12.021297</td>\n      <td>14.228392</td>\n      <td>0.935016</td>\n      <td>0.389401</td>\n      <td>0.346190</td>\n      <td>...</td>\n      <td>7.099089</td>\n      <td>5.021442</td>\n      <td>18.068723</td>\n      <td>10.250418</td>\n      <td>13.839926</td>\n      <td>10.737030</td>\n      <td>1.022870</td>\n      <td>4.520383</td>\n      <td>89.856124</td>\n      <td>0.653839</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>5.000000</td>\n      <td>0.000000</td>\n      <td>-23.164848</td>\n      <td>4.967968</td>\n      <td>0.000000</td>\n      <td>27.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.172314</td>\n      <td>-1.772461</td>\n      <td>...</td>\n      <td>14.489000</td>\n      <td>4.635810</td>\n      <td>23.620100</td>\n      <td>4.655730</td>\n      <td>20.589200</td>\n      <td>38.000000</td>\n      <td>-0.278039</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>8.000000</td>\n      <td>0.000000</td>\n      <td>60.000000</td>\n      <td>24.000000</td>\n      <td>64.000000</td>\n      <td>74.477905</td>\n      <td>109.000000</td>\n      <td>4.865373</td>\n      <td>0.000000</td>\n      <td>1.766934</td>\n      <td>...</td>\n      <td>28.665877</td>\n      <td>16.394825</td>\n      <td>56.747700</td>\n      <td>27.190300</td>\n      <td>44.819225</td>\n      <td>51.000000</td>\n      <td>0.000000</td>\n      <td>16.485531</td>\n      <td>450.588533</td>\n      <td>2.150000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>10.000000</td>\n      <td>0.000000</td>\n      <td>64.765647</td>\n      <td>26.137767</td>\n      <td>69.000000</td>\n      <td>81.485263</td>\n      <td>115.604541</td>\n      <td>4.973003</td>\n      <td>0.485209</td>\n      <td>1.872826</td>\n      <td>...</td>\n      <td>31.347850</td>\n      <td>18.058831</td>\n      <td>63.734357</td>\n      <td>30.792761</td>\n      <td>50.135566</td>\n      <td>57.770114</td>\n      <td>1.000000</td>\n      <td>18.589876</td>\n      <td>469.207062</td>\n      <td>2.567600</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>13.000000</td>\n      <td>1.000000</td>\n      <td>70.000000</td>\n      <td>29.035486</td>\n      <td>73.000000</td>\n      <td>87.000000</td>\n      <td>122.000000</td>\n      <td>5.081351</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>...</td>\n      <td>32.689980</td>\n      <td>18.232694</td>\n      <td>63.917531</td>\n      <td>33.007960</td>\n      <td>50.147145</td>\n      <td>60.000000</td>\n      <td>2.000000</td>\n      <td>21.015855</td>\n      <td>487.416029</td>\n      <td>2.886524</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>22.000000</td>\n      <td>1.000000</td>\n      <td>95.000000</td>\n      <td>56.944995</td>\n      <td>119.000000</td>\n      <td>138.000000</td>\n      <td>179.000000</td>\n      <td>28.000000</td>\n      <td>1.129295</td>\n      <td>3.000000</td>\n      <td>...</td>\n      <td>86.587576</td>\n      <td>52.527500</td>\n      <td>188.145195</td>\n      <td>111.835760</td>\n      <td>146.075000</td>\n      <td>100.000000</td>\n      <td>3.019947</td>\n      <td>59.132048</td>\n      <td>2154.275208</td>\n      <td>4.790000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 34 columns</p>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Clip imputed values to original max and min\nfor column in X_train_fimpute.columns:\n    max_val = np.max(X_train[column])\n    min_val = np.min(X_train[column])\n    X_train_fimpute.loc[X_train_fimpute[column]>max_val,column] = max_val\n    X_train_fimpute.loc[X_train_fimpute[column]<min_val, column] = min_val\n\nX_train_fimpute.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:31:01.277861Z","iopub.execute_input":"2025-01-17T07:31:01.278225Z","iopub.status.idle":"2025-01-17T07:31:01.415923Z","shell.execute_reply.started":"2025-01-17T07:31:01.278199Z","shell.execute_reply":"2025-01-17T07:31:01.414639Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"       Basic_Demos-Age  Basic_Demos-Sex  CGAS-CGAS_Score  \\\ncount      3960.000000      3960.000000      3960.000000   \nmean         10.433586         0.372727        64.830496   \nstd           3.574648         0.483591         9.726236   \nmin           5.000000         0.000000        25.000000   \n25%           8.000000         0.000000        60.000000   \n50%          10.000000         0.000000        64.765647   \n75%          13.000000         1.000000        70.000000   \nmax          22.000000         1.000000        95.000000   \n\n       Physical-Waist_Circumference  Physical-Diastolic_BP  \\\ncount                   3960.000000            3960.000000   \nmean                      26.998653              69.403169   \nstd                        4.692416              11.011761   \nmin                       18.000000               0.000000   \n25%                       24.000000              64.000000   \n50%                       26.137767              69.000000   \n75%                       29.035486              73.000000   \nmax                       50.000000             119.000000   \n\n       Physical-HeartRate  Physical-Systolic_BP  Fitness_Endurance-Max_Stage  \\\ncount         3960.000000           3960.000000                  3960.000000   \nmean            81.519388            116.805724                     4.974030   \nstd             12.021297             14.228392                     0.935016   \nmin             27.000000              0.000000                     0.000000   \n25%             74.477905            109.000000                     4.865373   \n50%             81.485263            115.604541                     4.973003   \n75%             87.000000            122.000000                     5.081351   \nmax            138.000000            179.000000                    28.000000   \n\n       FGC-FGC_CU_Zone  FGC-FGC_GSND_Zone  ...  BIA-BIA_ICW  BIA-BIA_LDM  \\\ncount      3960.000000        3960.000000  ...  3960.000000  3960.000000   \nmean          0.480419           1.850041  ...    31.363446    18.066786   \nstd           0.389164           0.341634  ...     7.083396     5.021442   \nmin           0.000000           1.000000  ...    14.489000     4.635810   \n25%           0.000000           1.766934  ...    28.665877    16.394825   \n50%           0.485209           1.872826  ...    31.347850    18.058831   \n75%           1.000000           2.000000  ...    32.689980    18.232694   \nmax           1.000000           3.000000  ...    79.473800    52.527500   \n\n       BIA-BIA_LST  BIA-BIA_SMM  BIA-BIA_TBW  SDS-SDS_Total_T  \\\ncount  3960.000000  3960.000000  3960.000000      3960.000000   \nmean     63.672368    31.469455    50.138601        57.881433   \nstd      17.644465    10.193244    13.839926        10.737030   \nmin      23.620100     4.655730    20.589200        38.000000   \n25%      56.747700    27.190300    44.819225        51.000000   \n50%      63.734357    30.792761    50.135566        57.770114   \n75%      63.917531    33.007960    50.147145        60.000000   \nmax     149.830000    97.923100   146.075000       100.000000   \n\n       PreInt_EduHx-computerinternet_hoursday  Physical-BMI_Calc  \\\ncount                             3960.000000        3960.000000   \nmean                                 1.073848          19.374281   \nstd                                  1.022777           4.520383   \nmin                                  0.000000           0.000000   \n25%                                  0.000000          16.485531   \n50%                                  1.000000          18.589876   \n75%                                  2.000000          21.015855   \nmax                                  3.000000          59.132048   \n\n       Fitness_Endurance-Time_Sec_Calc    PAQ_Total  \ncount                      3960.000000  3960.000000  \nmean                        468.480617     2.537644  \nstd                          86.553527     0.653839  \nmin                           5.000000     0.580000  \n25%                         450.588533     2.150000  \n50%                         469.207062     2.567600  \n75%                         487.416029     2.886524  \nmax                        1200.000000     4.790000  \n\n[8 rows x 34 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Basic_Demos-Age</th>\n      <th>Basic_Demos-Sex</th>\n      <th>CGAS-CGAS_Score</th>\n      <th>Physical-Waist_Circumference</th>\n      <th>Physical-Diastolic_BP</th>\n      <th>Physical-HeartRate</th>\n      <th>Physical-Systolic_BP</th>\n      <th>Fitness_Endurance-Max_Stage</th>\n      <th>FGC-FGC_CU_Zone</th>\n      <th>FGC-FGC_GSND_Zone</th>\n      <th>...</th>\n      <th>BIA-BIA_ICW</th>\n      <th>BIA-BIA_LDM</th>\n      <th>BIA-BIA_LST</th>\n      <th>BIA-BIA_SMM</th>\n      <th>BIA-BIA_TBW</th>\n      <th>SDS-SDS_Total_T</th>\n      <th>PreInt_EduHx-computerinternet_hoursday</th>\n      <th>Physical-BMI_Calc</th>\n      <th>Fitness_Endurance-Time_Sec_Calc</th>\n      <th>PAQ_Total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>...</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n      <td>3960.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>10.433586</td>\n      <td>0.372727</td>\n      <td>64.830496</td>\n      <td>26.998653</td>\n      <td>69.403169</td>\n      <td>81.519388</td>\n      <td>116.805724</td>\n      <td>4.974030</td>\n      <td>0.480419</td>\n      <td>1.850041</td>\n      <td>...</td>\n      <td>31.363446</td>\n      <td>18.066786</td>\n      <td>63.672368</td>\n      <td>31.469455</td>\n      <td>50.138601</td>\n      <td>57.881433</td>\n      <td>1.073848</td>\n      <td>19.374281</td>\n      <td>468.480617</td>\n      <td>2.537644</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3.574648</td>\n      <td>0.483591</td>\n      <td>9.726236</td>\n      <td>4.692416</td>\n      <td>11.011761</td>\n      <td>12.021297</td>\n      <td>14.228392</td>\n      <td>0.935016</td>\n      <td>0.389164</td>\n      <td>0.341634</td>\n      <td>...</td>\n      <td>7.083396</td>\n      <td>5.021442</td>\n      <td>17.644465</td>\n      <td>10.193244</td>\n      <td>13.839926</td>\n      <td>10.737030</td>\n      <td>1.022777</td>\n      <td>4.520383</td>\n      <td>86.553527</td>\n      <td>0.653839</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>5.000000</td>\n      <td>0.000000</td>\n      <td>25.000000</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n      <td>27.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>14.489000</td>\n      <td>4.635810</td>\n      <td>23.620100</td>\n      <td>4.655730</td>\n      <td>20.589200</td>\n      <td>38.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>8.000000</td>\n      <td>0.000000</td>\n      <td>60.000000</td>\n      <td>24.000000</td>\n      <td>64.000000</td>\n      <td>74.477905</td>\n      <td>109.000000</td>\n      <td>4.865373</td>\n      <td>0.000000</td>\n      <td>1.766934</td>\n      <td>...</td>\n      <td>28.665877</td>\n      <td>16.394825</td>\n      <td>56.747700</td>\n      <td>27.190300</td>\n      <td>44.819225</td>\n      <td>51.000000</td>\n      <td>0.000000</td>\n      <td>16.485531</td>\n      <td>450.588533</td>\n      <td>2.150000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>10.000000</td>\n      <td>0.000000</td>\n      <td>64.765647</td>\n      <td>26.137767</td>\n      <td>69.000000</td>\n      <td>81.485263</td>\n      <td>115.604541</td>\n      <td>4.973003</td>\n      <td>0.485209</td>\n      <td>1.872826</td>\n      <td>...</td>\n      <td>31.347850</td>\n      <td>18.058831</td>\n      <td>63.734357</td>\n      <td>30.792761</td>\n      <td>50.135566</td>\n      <td>57.770114</td>\n      <td>1.000000</td>\n      <td>18.589876</td>\n      <td>469.207062</td>\n      <td>2.567600</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>13.000000</td>\n      <td>1.000000</td>\n      <td>70.000000</td>\n      <td>29.035486</td>\n      <td>73.000000</td>\n      <td>87.000000</td>\n      <td>122.000000</td>\n      <td>5.081351</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>...</td>\n      <td>32.689980</td>\n      <td>18.232694</td>\n      <td>63.917531</td>\n      <td>33.007960</td>\n      <td>50.147145</td>\n      <td>60.000000</td>\n      <td>2.000000</td>\n      <td>21.015855</td>\n      <td>487.416029</td>\n      <td>2.886524</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>22.000000</td>\n      <td>1.000000</td>\n      <td>95.000000</td>\n      <td>50.000000</td>\n      <td>119.000000</td>\n      <td>138.000000</td>\n      <td>179.000000</td>\n      <td>28.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>...</td>\n      <td>79.473800</td>\n      <td>52.527500</td>\n      <td>149.830000</td>\n      <td>97.923100</td>\n      <td>146.075000</td>\n      <td>100.000000</td>\n      <td>3.000000</td>\n      <td>59.132048</td>\n      <td>1200.000000</td>\n      <td>4.790000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 34 columns</p>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"scaler = StandardScaler()                  \n\nX_train_fimpute[X_train_fimpute.columns] = scaler.fit_transform(X_train_fimpute[X_train_fimpute.columns])\nX_train_fimpute.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:31:03.036384Z","iopub.execute_input":"2025-01-17T07:31:03.036780Z","iopub.status.idle":"2025-01-17T07:31:03.146357Z","shell.execute_reply.started":"2025-01-17T07:31:03.036748Z","shell.execute_reply":"2025-01-17T07:31:03.145163Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"       Basic_Demos-Age  Basic_Demos-Sex  CGAS-CGAS_Score  \\\ncount     3.960000e+03     3.960000e+03     3.960000e+03   \nmean     -1.928872e-16    -5.831474e-17     8.343494e-16   \nstd       1.000126e+00     1.000126e+00     1.000126e+00   \nmin      -1.520226e+00    -7.708456e-01    -4.095678e+00   \n25%      -6.808763e-01    -7.708456e-01    -4.967087e-01   \n50%      -1.213100e-01    -7.708456e-01    -6.668295e-03   \n75%       7.180394e-01     1.297277e+00     5.315681e-01   \nmax       3.236088e+00     1.297277e+00     3.102260e+00   \n\n       Physical-Waist_Circumference  Physical-Diastolic_BP  \\\ncount                  3.960000e+03           3.960000e+03   \nmean                   5.921189e-17           2.866394e-16   \nstd                    1.000126e+00           1.000126e+00   \nmin                   -1.917944e+00          -6.303437e+00   \n25%                   -6.391231e-01          -4.907346e-01   \n50%                   -1.834866e-01          -3.661725e-02   \n75%                    4.341239e-01           3.266766e-01   \nmax                    4.902432e+00           4.504556e+00   \n\n       Physical-HeartRate  Physical-Systolic_BP  Fitness_Endurance-Max_Stage  \\\ncount        3.960000e+03          3.960000e+03                 3.960000e+03   \nmean         4.893953e-16         -3.545985e-16                 6.948426e-16   \nstd          1.000126e+00          1.000126e+00                 1.000126e+00   \nmin         -4.535806e+00         -8.210378e+00                -5.320400e+00   \n25%         -5.858247e-01         -5.486713e-01                -1.162227e-01   \n50%         -2.839099e-03         -8.443218e-02                -1.098129e-03   \n75%          4.559661e-01          3.651103e-01                 1.147947e-01   \nmax          4.698973e+00          4.371691e+00                 2.462940e+01   \n\n       FGC-FGC_CU_Zone  FGC-FGC_GSND_Zone  ...   BIA-BIA_ICW   BIA-BIA_LDM  \\\ncount     3.960000e+03       3.960000e+03  ...  3.960000e+03  3.960000e+03   \nmean     -1.282924e-16       1.058637e-16  ... -3.184882e-16 -1.004808e-16   \nstd       1.000126e+00       1.000126e+00  ...  1.000126e+00  1.000126e+00   \nmin      -1.234646e+00      -2.488480e+00  ... -2.382555e+00 -2.675063e+00   \n25%      -1.234646e+00      -2.432958e-01  ... -3.808780e-01 -3.330064e-01   \n50%       1.230976e-02       6.670247e-02  ... -2.202046e-03 -1.584385e-03   \n75%       1.335289e+00       4.390017e-01  ...  1.872974e-01  3.304415e-02   \nmax       1.335289e+00       3.366483e+00  ...  6.792848e+00  6.863580e+00   \n\n        BIA-BIA_LST   BIA-BIA_SMM   BIA-BIA_TBW  SDS-SDS_Total_T  \\\ncount  3.960000e+03  3.960000e+03  3.960000e+03     3.960000e+03   \nmean   1.040694e-16 -4.135861e-16  1.964758e-16    -1.821214e-16   \nstd    1.000126e+00  1.000126e+00  1.000126e+00     1.000126e+00   \nmin   -2.270249e+00 -2.630871e+00 -2.135353e+00    -1.851904e+00   \n25%   -3.925051e-01 -4.198561e-01 -3.843986e-01    -6.409875e-01   \n50%    3.513687e-03 -6.639494e-02 -2.193295e-04    -1.036914e-02   \n75%    1.389639e-02  1.509529e-01  6.173593e-04     1.973390e-01   \nmax    4.883600e+00  6.520205e+00  6.932733e+00     3.923235e+00   \n\n       PreInt_EduHx-computerinternet_hoursday  Physical-BMI_Calc  \\\ncount                            3.960000e+03       3.960000e+03   \nmean                            -5.741759e-17       9.796877e-16   \nstd                              1.000126e+00       1.000126e+00   \nmin                             -1.050066e+00      -4.286523e+00   \n25%                             -1.050066e+00      -6.391307e-01   \n50%                             -7.221268e-02      -1.735482e-01   \n75%                              9.056408e-01       3.631950e-01   \nmax                              1.883494e+00       8.796331e+00   \n\n       Fitness_Endurance-Time_Sec_Calc     PAQ_Total  \ncount                     3.960000e+03  3.960000e+03  \nmean                      4.831152e-16 -1.578984e-16  \nstd                       1.000126e+00  1.000126e+00  \nmin                      -5.355520e+00 -2.994455e+00  \n25%                      -2.067431e-01 -5.929487e-01  \n50%                       8.394070e-03  4.582149e-02  \n75%                       2.187987e-01  5.336544e-01  \nmax                       8.452709e+00  3.445252e+00  \n\n[8 rows x 34 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Basic_Demos-Age</th>\n      <th>Basic_Demos-Sex</th>\n      <th>CGAS-CGAS_Score</th>\n      <th>Physical-Waist_Circumference</th>\n      <th>Physical-Diastolic_BP</th>\n      <th>Physical-HeartRate</th>\n      <th>Physical-Systolic_BP</th>\n      <th>Fitness_Endurance-Max_Stage</th>\n      <th>FGC-FGC_CU_Zone</th>\n      <th>FGC-FGC_GSND_Zone</th>\n      <th>...</th>\n      <th>BIA-BIA_ICW</th>\n      <th>BIA-BIA_LDM</th>\n      <th>BIA-BIA_LST</th>\n      <th>BIA-BIA_SMM</th>\n      <th>BIA-BIA_TBW</th>\n      <th>SDS-SDS_Total_T</th>\n      <th>PreInt_EduHx-computerinternet_hoursday</th>\n      <th>Physical-BMI_Calc</th>\n      <th>Fitness_Endurance-Time_Sec_Calc</th>\n      <th>PAQ_Total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>...</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n      <td>3.960000e+03</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-1.928872e-16</td>\n      <td>-5.831474e-17</td>\n      <td>8.343494e-16</td>\n      <td>5.921189e-17</td>\n      <td>2.866394e-16</td>\n      <td>4.893953e-16</td>\n      <td>-3.545985e-16</td>\n      <td>6.948426e-16</td>\n      <td>-1.282924e-16</td>\n      <td>1.058637e-16</td>\n      <td>...</td>\n      <td>-3.184882e-16</td>\n      <td>-1.004808e-16</td>\n      <td>1.040694e-16</td>\n      <td>-4.135861e-16</td>\n      <td>1.964758e-16</td>\n      <td>-1.821214e-16</td>\n      <td>-5.741759e-17</td>\n      <td>9.796877e-16</td>\n      <td>4.831152e-16</td>\n      <td>-1.578984e-16</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>...</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n      <td>1.000126e+00</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-1.520226e+00</td>\n      <td>-7.708456e-01</td>\n      <td>-4.095678e+00</td>\n      <td>-1.917944e+00</td>\n      <td>-6.303437e+00</td>\n      <td>-4.535806e+00</td>\n      <td>-8.210378e+00</td>\n      <td>-5.320400e+00</td>\n      <td>-1.234646e+00</td>\n      <td>-2.488480e+00</td>\n      <td>...</td>\n      <td>-2.382555e+00</td>\n      <td>-2.675063e+00</td>\n      <td>-2.270249e+00</td>\n      <td>-2.630871e+00</td>\n      <td>-2.135353e+00</td>\n      <td>-1.851904e+00</td>\n      <td>-1.050066e+00</td>\n      <td>-4.286523e+00</td>\n      <td>-5.355520e+00</td>\n      <td>-2.994455e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-6.808763e-01</td>\n      <td>-7.708456e-01</td>\n      <td>-4.967087e-01</td>\n      <td>-6.391231e-01</td>\n      <td>-4.907346e-01</td>\n      <td>-5.858247e-01</td>\n      <td>-5.486713e-01</td>\n      <td>-1.162227e-01</td>\n      <td>-1.234646e+00</td>\n      <td>-2.432958e-01</td>\n      <td>...</td>\n      <td>-3.808780e-01</td>\n      <td>-3.330064e-01</td>\n      <td>-3.925051e-01</td>\n      <td>-4.198561e-01</td>\n      <td>-3.843986e-01</td>\n      <td>-6.409875e-01</td>\n      <td>-1.050066e+00</td>\n      <td>-6.391307e-01</td>\n      <td>-2.067431e-01</td>\n      <td>-5.929487e-01</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-1.213100e-01</td>\n      <td>-7.708456e-01</td>\n      <td>-6.668295e-03</td>\n      <td>-1.834866e-01</td>\n      <td>-3.661725e-02</td>\n      <td>-2.839099e-03</td>\n      <td>-8.443218e-02</td>\n      <td>-1.098129e-03</td>\n      <td>1.230976e-02</td>\n      <td>6.670247e-02</td>\n      <td>...</td>\n      <td>-2.202046e-03</td>\n      <td>-1.584385e-03</td>\n      <td>3.513687e-03</td>\n      <td>-6.639494e-02</td>\n      <td>-2.193295e-04</td>\n      <td>-1.036914e-02</td>\n      <td>-7.221268e-02</td>\n      <td>-1.735482e-01</td>\n      <td>8.394070e-03</td>\n      <td>4.582149e-02</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>7.180394e-01</td>\n      <td>1.297277e+00</td>\n      <td>5.315681e-01</td>\n      <td>4.341239e-01</td>\n      <td>3.266766e-01</td>\n      <td>4.559661e-01</td>\n      <td>3.651103e-01</td>\n      <td>1.147947e-01</td>\n      <td>1.335289e+00</td>\n      <td>4.390017e-01</td>\n      <td>...</td>\n      <td>1.872974e-01</td>\n      <td>3.304415e-02</td>\n      <td>1.389639e-02</td>\n      <td>1.509529e-01</td>\n      <td>6.173593e-04</td>\n      <td>1.973390e-01</td>\n      <td>9.056408e-01</td>\n      <td>3.631950e-01</td>\n      <td>2.187987e-01</td>\n      <td>5.336544e-01</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3.236088e+00</td>\n      <td>1.297277e+00</td>\n      <td>3.102260e+00</td>\n      <td>4.902432e+00</td>\n      <td>4.504556e+00</td>\n      <td>4.698973e+00</td>\n      <td>4.371691e+00</td>\n      <td>2.462940e+01</td>\n      <td>1.335289e+00</td>\n      <td>3.366483e+00</td>\n      <td>...</td>\n      <td>6.792848e+00</td>\n      <td>6.863580e+00</td>\n      <td>4.883600e+00</td>\n      <td>6.520205e+00</td>\n      <td>6.932733e+00</td>\n      <td>3.923235e+00</td>\n      <td>1.883494e+00</td>\n      <td>8.796331e+00</td>\n      <td>8.452709e+00</td>\n      <td>3.445252e+00</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 34 columns</p>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Repeat the above for X_test\n\n# Add calculated fields\nX_test['Physical-BMI_Calc'] = X_test.apply(lambda row: row['Physical-BMI'] if row['Physical-BMI']==row['Physical-BMI'] else row['BIA-BIA_BMI'],axis=1)\nX_test['Fitness_Endurance-Time_Sec_Calc'] = X_test.apply(lambda row: row['Fitness_Endurance-Time_Sec'] + (row['Fitness_Endurance-Time_Mins']*60), axis=1)\nX_test['PAQ_Total'] = X_test.apply(lambda row: row['PAQ_A-PAQ_A_Total'] if row['PAQ_A-PAQ_A_Total']==row['PAQ_A-PAQ_A_Total'] else row['PAQ_C-PAQ_C_Total'],axis=1)\n\n# Drop fields no longer needed\nX_test = X_test.drop(columns=['PAQ_A-PAQ_A_Total','PAQ_C-PAQ_C_Total',\n                     'Physical-BMI','BIA-BIA_BMI',\n                     'Fitness_Endurance-Time_Mins','Fitness_Endurance-Time_Sec'])\n\n# Remove outliers\nX_test.loc[X_test['CGAS-CGAS_Score']>=100.0,'CGAS-CGAS_Score'] = np.nan\nX_test.loc[X_test['Physical-Systolic_BP']>=180.0,'Physical-Systolic_BP'] = np.nan\nX_test.loc[X_test['Physical-Diastolic_BP']>=120.0,'Physical-Diastolic_BP'] = np.nan\nX_test.loc[X_test['BIA-BIA_DEE']>=6000.0,'BIA-BIA_DEE'] = np.nan\nX_test.loc[(X_test['BIA-BIA_BMC']<=0.0) | (X_test['BIA-BIA_BMC']>=16.0),'BIA-BIA_BMC'] = np.nan\nX_test.loc[(X_test['BIA-BIA_BMR']<=0.0) | (X_test['BIA-BIA_BMR']>=2400.0),'BIA-BIA_BMR'] = np.nan\nX_test.loc[(X_test['BIA-BIA_ECW']<=0.0) | (X_test['BIA-BIA_ECW']>=60.0),'BIA-BIA_ECW'] = np.nan\nX_test.loc[(X_test['BIA-BIA_FFM']<=0.0) | (X_test['BIA-BIA_FFM']>=200.0),'BIA-BIA_FFM'] = np.nan\nX_test.loc[(X_test['BIA-BIA_FFMI']<=0.0) | (X_test['BIA-BIA_FFMI']>=25.0),'BIA-BIA_FFMI'] = np.nan\nX_test.loc[(X_test['BIA-BIA_FMI']<=0.0) | (X_test['BIA-BIA_FMI']>=25.0),'BIA-BIA_FMI'] = np.nan\nX_test.loc[(X_test['BIA-BIA_Fat']<=8.0) | (X_test['BIA-BIA_Fat']>=60.0),'BIA-BIA_Fat'] = np.nan\nX_test.loc[(X_test['BIA-BIA_ICW']<=0.0) | (X_test['BIA-BIA_ICW']>=80.0),'BIA-BIA_ICW'] = np.nan\nX_test.loc[(X_test['BIA-BIA_LDM']<=0.0) | (X_test['BIA-BIA_LDM']>=60.0),'BIA-BIA_LDM'] = np.nan\nX_test.loc[(X_test['BIA-BIA_LST']<=0.0) | (X_test['BIA-BIA_LST']>=150.0),'BIA-BIA_LST'] = np.nan\nX_test.loc[(X_test['BIA-BIA_SMM']<=0.0) | (X_test['BIA-BIA_SMM']>=100.0),'BIA-BIA_SMM'] = np.nan\nX_test.loc[(X_test['BIA-BIA_TBW']<=0.0) | (X_test['BIA-BIA_TBW']>=150.0),'BIA-BIA_TBW'] = np.nan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:31:07.310307Z","iopub.execute_input":"2025-01-17T07:31:07.310675Z","iopub.status.idle":"2025-01-17T07:31:07.350954Z","shell.execute_reply.started":"2025-01-17T07:31:07.310642Z","shell.execute_reply":"2025-01-17T07:31:07.349469Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-14-ee14b5292b05>:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_test['Physical-BMI_Calc'] = X_test.apply(lambda row: row['Physical-BMI'] if row['Physical-BMI']==row['Physical-BMI'] else row['BIA-BIA_BMI'],axis=1)\n<ipython-input-14-ee14b5292b05>:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_test['Fitness_Endurance-Time_Sec_Calc'] = X_test.apply(lambda row: row['Fitness_Endurance-Time_Sec'] + (row['Fitness_Endurance-Time_Mins']*60), axis=1)\n<ipython-input-14-ee14b5292b05>:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_test['PAQ_Total'] = X_test.apply(lambda row: row['PAQ_A-PAQ_A_Total'] if row['PAQ_A-PAQ_A_Total']==row['PAQ_A-PAQ_A_Total'] else row['PAQ_C-PAQ_C_Total'],axis=1)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal\n  return op(a, b)\n/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Imputation\nX_test_fimpute = pd.DataFrame(iter_imputer.transform(X_test), columns = X_test.columns)\n\n# Clipping\nfor column in X_test_fimpute.columns:\n    max_val = np.max(X_train[column])\n    min_val = np.min(X_train[column])\n    X_test_fimpute.loc[X_test_fimpute[column]>max_val,column] = max_val\n    X_test_fimpute.loc[X_test_fimpute[column]<min_val, column] = min_val\n\n# Scaling\nX_test_fimpute[X_test_fimpute.columns] = scaler.transform(X_test_fimpute[X_test_fimpute.columns])\n\nX_test_fimpute.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:31:12.734982Z","iopub.execute_input":"2025-01-17T07:31:12.735357Z","iopub.status.idle":"2025-01-17T07:31:12.878939Z","shell.execute_reply.started":"2025-01-17T07:31:12.735326Z","shell.execute_reply":"2025-01-17T07:31:12.877508Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"       Basic_Demos-Age  Basic_Demos-Sex  CGAS-CGAS_Score  \\\ncount        20.000000        20.000000        20.000000   \nmean          0.088527         0.056403        -0.045878   \nstd           1.042416         1.039489         0.801382   \nmin          -1.520226        -0.770846        -1.524985   \n25%          -0.401093        -0.770846        -0.398350   \n50%          -0.121310        -0.770846         0.053470   \n75%           0.508202         1.297277         0.442934   \nmax           2.396738         1.297277         1.559845   \n\n       Physical-Waist_Circumference  Physical-Diastolic_BP  \\\ncount                     20.000000              20.000000   \nmean                       0.062078              -0.054532   \nstd                        0.834814               0.728397   \nmin                       -1.917944              -1.126499   \n25%                       -0.639123              -0.626970   \n50%                        0.186887               0.013374   \n75%                        0.601291               0.157655   \nmax                        1.827302               2.099974   \n\n       Physical-HeartRate  Physical-Systolic_BP  Fitness_Endurance-Max_Stage  \\\ncount           20.000000             20.000000                    20.000000   \nmean            -0.024661              0.096746                    -0.099940   \nstd              0.630397              1.095310                     0.609633   \nmin             -0.958369             -1.532744                    -1.982619   \n25%             -0.479991             -0.323267                    -0.117459   \n50%             -0.096654              0.013656                    -0.010525   \n75%              0.349697              0.359988                     0.109218   \nmax              1.287928              3.247037                     1.097415   \n\n       FGC-FGC_CU_Zone  FGC-FGC_GSND_Zone  ...  BIA-BIA_ICW  BIA-BIA_LDM  \\\ncount        20.000000          20.000000  ...    20.000000    20.000000   \nmean         -0.092855          -0.200220  ...    -0.201746    -0.212473   \nstd           1.064675           0.889571  ...     0.502229     0.550313   \nmin          -1.234646          -2.488480  ...    -1.458277    -1.826683   \n25%          -1.234646          -0.271331  ...    -0.353243    -0.152373   \n50%          -0.226210          -0.084494  ...    -0.172488    -0.012274   \n75%           1.335289           0.310673  ...     0.143634     0.007697   \nmax           1.335289           1.002942  ...     0.662725     0.564693   \n\n       BIA-BIA_LST  BIA-BIA_SMM  BIA-BIA_TBW  SDS-SDS_Total_T  \\\ncount    20.000000    20.000000    20.000000        20.000000   \nmean     -0.149911    -0.186412    -0.143597        -0.248715   \nstd       0.534173     0.543306     0.553989         0.532311   \nmin      -1.403148    -1.575630    -1.668095        -1.665609   \n25%      -0.072266    -0.321185    -0.053025        -0.384832   \n50%       0.001130    -0.057763    -0.000024        -0.036684   \n75%       0.007959     0.164499     0.000693         0.043558   \nmax       0.908379     0.466421     0.938555         0.569929   \n\n       PreInt_EduHx-computerinternet_hoursday  Physical-BMI_Calc  \\\ncount                               20.000000          20.000000   \nmean                                 0.397885           0.159731   \nstd                                  1.029493           0.885501   \nmin                                 -1.050066          -1.181175   \n25%                                 -0.316676          -0.484882   \n50%                                  0.905641           0.059206   \n75%                                  0.934083           0.438107   \nmax                                  1.883494           2.371861   \n\n       Fitness_Endurance-Time_Sec_Calc  PAQ_Total  \ncount                        20.000000  20.000000  \nmean                         -0.138408  -0.321770  \nstd                           0.532544   1.215554  \nmin                          -1.577034  -2.290829  \n25%                          -0.194965  -0.793832  \n50%                          -0.060168  -0.247980  \n75%                           0.054260   0.197347  \nmax                           1.253942   2.405110  \n\n[8 rows x 34 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Basic_Demos-Age</th>\n      <th>Basic_Demos-Sex</th>\n      <th>CGAS-CGAS_Score</th>\n      <th>Physical-Waist_Circumference</th>\n      <th>Physical-Diastolic_BP</th>\n      <th>Physical-HeartRate</th>\n      <th>Physical-Systolic_BP</th>\n      <th>Fitness_Endurance-Max_Stage</th>\n      <th>FGC-FGC_CU_Zone</th>\n      <th>FGC-FGC_GSND_Zone</th>\n      <th>...</th>\n      <th>BIA-BIA_ICW</th>\n      <th>BIA-BIA_LDM</th>\n      <th>BIA-BIA_LST</th>\n      <th>BIA-BIA_SMM</th>\n      <th>BIA-BIA_TBW</th>\n      <th>SDS-SDS_Total_T</th>\n      <th>PreInt_EduHx-computerinternet_hoursday</th>\n      <th>Physical-BMI_Calc</th>\n      <th>Fitness_Endurance-Time_Sec_Calc</th>\n      <th>PAQ_Total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>...</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n      <td>20.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.088527</td>\n      <td>0.056403</td>\n      <td>-0.045878</td>\n      <td>0.062078</td>\n      <td>-0.054532</td>\n      <td>-0.024661</td>\n      <td>0.096746</td>\n      <td>-0.099940</td>\n      <td>-0.092855</td>\n      <td>-0.200220</td>\n      <td>...</td>\n      <td>-0.201746</td>\n      <td>-0.212473</td>\n      <td>-0.149911</td>\n      <td>-0.186412</td>\n      <td>-0.143597</td>\n      <td>-0.248715</td>\n      <td>0.397885</td>\n      <td>0.159731</td>\n      <td>-0.138408</td>\n      <td>-0.321770</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.042416</td>\n      <td>1.039489</td>\n      <td>0.801382</td>\n      <td>0.834814</td>\n      <td>0.728397</td>\n      <td>0.630397</td>\n      <td>1.095310</td>\n      <td>0.609633</td>\n      <td>1.064675</td>\n      <td>0.889571</td>\n      <td>...</td>\n      <td>0.502229</td>\n      <td>0.550313</td>\n      <td>0.534173</td>\n      <td>0.543306</td>\n      <td>0.553989</td>\n      <td>0.532311</td>\n      <td>1.029493</td>\n      <td>0.885501</td>\n      <td>0.532544</td>\n      <td>1.215554</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-1.520226</td>\n      <td>-0.770846</td>\n      <td>-1.524985</td>\n      <td>-1.917944</td>\n      <td>-1.126499</td>\n      <td>-0.958369</td>\n      <td>-1.532744</td>\n      <td>-1.982619</td>\n      <td>-1.234646</td>\n      <td>-2.488480</td>\n      <td>...</td>\n      <td>-1.458277</td>\n      <td>-1.826683</td>\n      <td>-1.403148</td>\n      <td>-1.575630</td>\n      <td>-1.668095</td>\n      <td>-1.665609</td>\n      <td>-1.050066</td>\n      <td>-1.181175</td>\n      <td>-1.577034</td>\n      <td>-2.290829</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.401093</td>\n      <td>-0.770846</td>\n      <td>-0.398350</td>\n      <td>-0.639123</td>\n      <td>-0.626970</td>\n      <td>-0.479991</td>\n      <td>-0.323267</td>\n      <td>-0.117459</td>\n      <td>-1.234646</td>\n      <td>-0.271331</td>\n      <td>...</td>\n      <td>-0.353243</td>\n      <td>-0.152373</td>\n      <td>-0.072266</td>\n      <td>-0.321185</td>\n      <td>-0.053025</td>\n      <td>-0.384832</td>\n      <td>-0.316676</td>\n      <td>-0.484882</td>\n      <td>-0.194965</td>\n      <td>-0.793832</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-0.121310</td>\n      <td>-0.770846</td>\n      <td>0.053470</td>\n      <td>0.186887</td>\n      <td>0.013374</td>\n      <td>-0.096654</td>\n      <td>0.013656</td>\n      <td>-0.010525</td>\n      <td>-0.226210</td>\n      <td>-0.084494</td>\n      <td>...</td>\n      <td>-0.172488</td>\n      <td>-0.012274</td>\n      <td>0.001130</td>\n      <td>-0.057763</td>\n      <td>-0.000024</td>\n      <td>-0.036684</td>\n      <td>0.905641</td>\n      <td>0.059206</td>\n      <td>-0.060168</td>\n      <td>-0.247980</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.508202</td>\n      <td>1.297277</td>\n      <td>0.442934</td>\n      <td>0.601291</td>\n      <td>0.157655</td>\n      <td>0.349697</td>\n      <td>0.359988</td>\n      <td>0.109218</td>\n      <td>1.335289</td>\n      <td>0.310673</td>\n      <td>...</td>\n      <td>0.143634</td>\n      <td>0.007697</td>\n      <td>0.007959</td>\n      <td>0.164499</td>\n      <td>0.000693</td>\n      <td>0.043558</td>\n      <td>0.934083</td>\n      <td>0.438107</td>\n      <td>0.054260</td>\n      <td>0.197347</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.396738</td>\n      <td>1.297277</td>\n      <td>1.559845</td>\n      <td>1.827302</td>\n      <td>2.099974</td>\n      <td>1.287928</td>\n      <td>3.247037</td>\n      <td>1.097415</td>\n      <td>1.335289</td>\n      <td>1.002942</td>\n      <td>...</td>\n      <td>0.662725</td>\n      <td>0.564693</td>\n      <td>0.908379</td>\n      <td>0.466421</td>\n      <td>0.938555</td>\n      <td>0.569929</td>\n      <td>1.883494</td>\n      <td>2.371861</td>\n      <td>1.253942</td>\n      <td>2.405110</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 34 columns</p>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"# Use labelled data only","metadata":{}},{"cell_type":"code","source":"X_train_labelled = X_train_fimpute.loc[y_train.notna()]\ny_train_labelled = y_train[y_train.notna()]\nprint(\"Size of labelled train data set is: \", (X_train_labelled.shape, y_train_labelled.shape))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:31:18.479602Z","iopub.execute_input":"2025-01-17T07:31:18.480029Z","iopub.status.idle":"2025-01-17T07:31:18.490831Z","shell.execute_reply.started":"2025-01-17T07:31:18.479997Z","shell.execute_reply":"2025-01-17T07:31:18.489738Z"}},"outputs":[{"name":"stdout","text":"Size of labelled train data set is:  ((2736, 34), (2736,))\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Since we are not using the unlabelled data in this notebook, we can drop the indices\n# This will help us prevent any accidents when doing cross-validation when we split by index, don't have to think about loc vs iloc etc.\n\nX_train_labelled = X_train_labelled.reset_index(drop=True)\ny_train_labelled = y_train_labelled.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:31:19.805799Z","iopub.execute_input":"2025-01-17T07:31:19.806185Z","iopub.status.idle":"2025-01-17T07:31:19.813157Z","shell.execute_reply.started":"2025-01-17T07:31:19.806155Z","shell.execute_reply":"2025-01-17T07:31:19.812071Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"weights_labelled = weights_labelled.reset_index(drop=True)\nweights_labelled2 = weights_labelled2.reset_index(drop=True)\nweights_labelled3 = weights_labelled3.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:31:21.934031Z","iopub.execute_input":"2025-01-17T07:31:21.934409Z","iopub.status.idle":"2025-01-17T07:31:21.939767Z","shell.execute_reply.started":"2025-01-17T07:31:21.934378Z","shell.execute_reply":"2025-01-17T07:31:21.938446Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Experimentation - optimising model","metadata":{}},{"cell_type":"markdown","source":"We are trying to find the best combination of:\n* Parquet data inclusion: No, Yes\n* PCA: all, 95%, 90%\n* Augment data: None, 1x noisy, 2x noisy, 1x noisier, 2x noisier\n* Loss/Evaluation function: NMSE or QWK\n* Model parameters:\n    * Number of hidden layers: 1, 2, 3, 4, 5\n    * Number of neurons in each layer\n    * Regularisation parameters: None, L1, L2, Elastic Net\n    * Dropout layers: None, 0.1, 0.2, 0.3\n    * Activation function: selu, relu\n    * Initialiser: He, Glorot\n    * Batch normalisation: No, Yes\n    * Learning rate (scheduler): 0.002, 0.005, 0.008, step decay, exponential decay\n* Sample weight when fitting: None, Linear, Exponential\n\n**Approach**:\n1. Begin with basic parameters:\n    * No Parquet\n    * No PCA\n    * No Data Augmentation\n    * NMSE Loss\n    * No Sample Weight\n    * Basic model:\n        * 1 hidden layer\n        * 50 neurons\n        * No L1 or L2 regularisation\n        * Dropout 0.2 (I think it is generally a good idea to include dropout, especially as our model inevitably becomes more complex)\n        * selu activation (probably won't experiment with this too much)\n        * He initialisation (probably won't experiment much)\n        * No batch normalisation\n        * Learning rate constant 0.005\n2. Evaluate model - underfitting, overfitting?\n3. Make modifications - increase complexity if underfitting, add regularization if overfitting\n4. Iteratively improve - ideally we will increase the model complexity until it starts to overfit, then test regularisation. Will probably look to improve the DNN architecture for a while before starting to change anything else\n\nSounds straightforward in theory, but we know better.","metadata":{}},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:31:30.320839Z","iopub.execute_input":"2025-01-17T07:31:30.321216Z","iopub.status.idle":"2025-01-17T07:31:30.326083Z","shell.execute_reply.started":"2025-01-17T07:31:30.321189Z","shell.execute_reply":"2025-01-17T07:31:30.324708Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def bin_target(y_val, max_val=100, num_classes=11):\n    #return y_val//(num_classes-1) #integer division for ordinal classes\n    return y_val//(max_val//(num_classes-1)) \n    #e.g., 100 split into 11 classes: 0-9, 10-19, ..., 90-99, 100\n    # 100 split into 6 classes: 0-19, 20-39, ..., 80-99, 100\n    # With 0-100 values doing integer division we have this extra 100 term in its own class - there's probably a better way of splitting","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:47:07.536868Z","iopub.execute_input":"2025-01-17T07:47:07.537292Z","iopub.status.idle":"2025-01-17T07:47:07.542461Z","shell.execute_reply.started":"2025-01-17T07:47:07.537263Z","shell.execute_reply":"2025-01-17T07:47:07.541075Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"'''\n# QWK Calculation\ndef quadratic_weighted_kappa(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Discretize continuous predictions if needed (e.g., rounding)\n    y_pred = np.round(y_pred)\n    \n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\n# Convert QWK to a TensorFlow-friendly metric\ndef qwk_metric(y_true, y_pred):\n    qwk_value = tf.py_function(quadratic_weighted_kappa, [y_true, y_pred], tf.float64)\n    return qwk_value\n\n# Custom loss that combines MSE and QWK\ndef combined_loss(y_true, y_pred):\n    mse_loss = tf.reduce_mean(tf.square(y_true - y_pred))  # Mean Squared Error\n    qwk_loss = tf.py_function(quadratic_weighted_kappa, [y_true, y_pred], tf.float64)  # QWK\n    qwk_loss = tf.convert_to_tensor(qwk_loss, dtype=tf.float32)\n    # Add a weighting factor\n    return mse_loss - 0.5 * qwk_loss\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:32:05.784294Z","iopub.execute_input":"2025-01-17T07:32:05.784731Z","iopub.status.idle":"2025-01-17T07:32:05.791310Z","shell.execute_reply.started":"2025-01-17T07:32:05.784697Z","shell.execute_reply":"2025-01-17T07:32:05.790060Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"#np.linspace(0,100,5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T01:52:01.945349Z","iopub.execute_input":"2025-01-17T01:52:01.945734Z","iopub.status.idle":"2025-01-17T01:52:01.950117Z","shell.execute_reply.started":"2025-01-17T01:52:01.945703Z","shell.execute_reply":"2025-01-17T01:52:01.948823Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"#a = pd.DataFrame([13,27,64,1,6,97,100])\n#print(bin_target(a,100,11))\n#print(bin_target(a,100,6))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T01:52:02.453154Z","iopub.execute_input":"2025-01-17T01:52:02.453552Z","iopub.status.idle":"2025-01-17T01:52:02.457717Z","shell.execute_reply.started":"2025-01-17T01:52:02.453516Z","shell.execute_reply":"2025-01-17T01:52:02.456568Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"#a = tf.constant([13,27,64,1,6,97,100]) \n#b = bin_target(a.numpy())\n#print(b)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T01:52:03.076400Z","iopub.execute_input":"2025-01-17T01:52:03.076849Z","iopub.status.idle":"2025-01-17T01:52:03.080843Z","shell.execute_reply.started":"2025-01-17T01:52:03.076811Z","shell.execute_reply":"2025-01-17T01:52:03.079897Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Some reference to https://medium.com/@nlztrk/quadratic-weighted-kappa-qwk-metric-and-how-to-optimize-it-062cc9121baa\n\nfrom sklearn.metrics import confusion_matrix\n\ndef qwk(y_true, y_pred):\n    num_classes=11 # Ranging from 0 to 10 (from (0 to 100) // 10). Slightly sketch since 10 is 1/10th as likely as any other class. The hope is that there aren't enough 100s to matter\n    #return cohen_kappa_score(y_true.numpy(), y_pred.numpy(), weights='quadratic') \n    # didn't manage to get the above working, ended up writing the function manually\n    max_val=100\n    y_true_bin = bin_target(y_true.numpy(),max_val,num_classes)\n    y_pred_bin = bin_target(y_pred.numpy(),max_val,num_classes) # Not working because SymbolicTensorflow doesn't have .numpy() apparently - solved here: https://github.com/tensorflow/tensorflow/issues/27519 comment by kenyukobayashi\n    \n    #y_true_bin = bin_target(tf.make_ndarray(y_true),num_classes)\n    #y_pred_bin = bin_target(tf.make_ndarray(y_pred),num_classes)\n\n    # Confusion matrix O\n    O = confusion_matrix(y_true_bin, y_pred_bin, labels=np.arange(num_classes))\n    O = O/O.sum()\n\n    # Calculate the weight matrix W\n    W = np.zeros((num_classes, num_classes))\n    for i in range(num_classes):\n        for j in range(num_classes):\n            W[i, j] = (i - j) ** 2 / (num_classes - 1) ** 2\n\n    E = np.outer(np.sum(O, axis=1), np.sum(O, axis=0))\n\n    num = np.sum(W * O)\n    denom = np.sum(W * E)\n    return (1 - (num/denom)) \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:32:39.179261Z","iopub.execute_input":"2025-01-17T07:32:39.179660Z","iopub.status.idle":"2025-01-17T07:32:39.187343Z","shell.execute_reply.started":"2025-01-17T07:32:39.179632Z","shell.execute_reply":"2025-01-17T07:32:39.186045Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"a = tf.constant([9,54,72])\nb = tf.constant([6,61,75])\n\nqwk(a,b)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:32:40.392937Z","iopub.execute_input":"2025-01-17T07:32:40.393337Z","iopub.status.idle":"2025-01-17T07:32:40.946443Z","shell.execute_reply.started":"2025-01-17T07:32:40.393304Z","shell.execute_reply":"2025-01-17T07:32:40.943948Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-2cd46d422218>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m61\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mqwk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-b535c13fa51f>\u001b[0m in \u001b[0;36mqwk\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# didn't manage to get the above working, ended up writing the function manually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmax_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0my_true_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbin_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0my_pred_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbin_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Not working because SymbolicTensorflow doesn't have .numpy() apparently - solved here: https://github.com/tensorflow/tensorflow/issues/27519 comment by kenyukobayashi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'bin_target' is not defined"],"ename":"NameError","evalue":"name 'bin_target' is not defined","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"k = 3  # Number of folds for cross-validation\nkf = KFold(n_splits=k, shuffle=True, random_state=42)  # Set shuffle=True to randomize data splits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:32:41.704347Z","iopub.execute_input":"2025-01-17T07:32:41.704747Z","iopub.status.idle":"2025-01-17T07:32:41.709660Z","shell.execute_reply.started":"2025-01-17T07:32:41.704712Z","shell.execute_reply":"2025-01-17T07:32:41.708342Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# V1: Basic one layer","metadata":{}},{"cell_type":"code","source":"def create_model_v1():\n    model = keras.models.Sequential([\n        keras.layers.Dense(50, input_shape=(X_train_labelled.shape[1],), activation=\"selu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(1, activation=\"linear\")\n    ])\n\n    model.compile(optimizer=Adam(learning_rate=0.005), loss='mean_squared_error', metrics=['mae','mse',qwk],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T01:52:11.635466Z","iopub.execute_input":"2025-01-17T01:52:11.635878Z","iopub.status.idle":"2025-01-17T01:52:11.641827Z","shell.execute_reply.started":"2025-01-17T01:52:11.635844Z","shell.execute_reply":"2025-01-17T01:52:11.640286Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Prepare to store results\nvalidation_losses = []\nmodels = []\n\n# Loop over each fold\n#for train_index, val_index in kf.split(X_train_labelled):\n    # Split the data into training and validation sets for the current fold\n#    X_train_t, X_train_v = X_train_labelled.loc[train_index], X_train_labelled.loc[val_index]\n#    y_train_t, y_train_v = y_train_labelled.loc[train_index], y_train_labelled.loc[val_index]\n\n    # Build a new model for each fold\n#    model = create_model_v1()\n    \n    # Define early stopping to avoid overfitting\n#    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n    \n    # Train the model on the training set and evaluate on the validation set\n#    history = model.fit(\n#        X_train_t, y_train_t,\n#        validation_data=(X_train_v, y_train_v),\n#        epochs=200,\n#        batch_size=64,\n#        callbacks=[early_stopping],\n#        verbose=2\n#    )\n    \n    # Evaluate the model on the validation set\n#    val_loss, val_mae, val_mse, val_qwk = model.evaluate(X_train_v, y_train_v, verbose=2)\n#    validation_losses.append(val_loss)\n#    models.append(model)\n\n# Calculate the average validation loss across all folds\n#avg_val_loss = np.mean(validation_losses)\n#print(f\"Average Validation Loss: {avg_val_loss}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T04:28:44.740647Z","iopub.execute_input":"2025-01-16T04:28:44.741069Z","iopub.status.idle":"2025-01-16T04:31:36.495411Z","shell.execute_reply.started":"2025-01-16T04:28:44.741034Z","shell.execute_reply":"2025-01-16T04:31:36.494313Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/200\n29/29 - 1s - 51ms/step - loss: 1093.4996 - mae: 27.3390 - mse: 1093.4996 - qwk: 0.0158 - val_loss: 889.1447 - val_mae: 24.1379 - val_mse: 889.1447 - val_qwk: 0.0812\nEpoch 2/200\n29/29 - 1s - 47ms/step - loss: 817.4344 - mae: 23.1854 - mse: 817.4344 - qwk: 0.0976 - val_loss: 611.1226 - val_mae: 19.4533 - val_mse: 611.1226 - val_qwk: 0.1787\nEpoch 3/200\n29/29 - 1s - 49ms/step - loss: 576.6937 - mae: 19.0706 - mse: 576.6937 - qwk: 0.1683 - val_loss: 460.4323 - val_mae: 16.9806 - val_mse: 460.4323 - val_qwk: 0.2751\nEpoch 4/200\n29/29 - 1s - 48ms/step - loss: 460.0558 - mae: 17.0183 - mse: 460.0558 - qwk: 0.2430 - val_loss: 388.8248 - val_mae: 15.6383 - val_mse: 388.8248 - val_qwk: 0.3257\nEpoch 5/200\n29/29 - 1s - 48ms/step - loss: 377.0742 - mae: 15.5223 - mse: 377.0742 - qwk: 0.3213 - val_loss: 343.5949 - val_mae: 14.7882 - val_mse: 343.5949 - val_qwk: 0.3775\nEpoch 6/200\n29/29 - 1s - 47ms/step - loss: 337.9909 - mae: 14.7248 - mse: 337.9909 - qwk: 0.3899 - val_loss: 320.5114 - val_mae: 14.3245 - val_mse: 320.5114 - val_qwk: 0.4134\nEpoch 7/200\n29/29 - 1s - 48ms/step - loss: 320.9376 - mae: 14.2667 - mse: 320.9376 - qwk: 0.3945 - val_loss: 316.5328 - val_mae: 14.2128 - val_mse: 316.5328 - val_qwk: 0.4241\nEpoch 8/200\n29/29 - 1s - 47ms/step - loss: 316.4108 - mae: 14.2084 - mse: 316.4108 - qwk: 0.3969 - val_loss: 315.6665 - val_mae: 14.1823 - val_mse: 315.6665 - val_qwk: 0.4329\nEpoch 9/200\n29/29 - 1s - 48ms/step - loss: 301.5677 - mae: 13.8104 - mse: 301.5677 - qwk: 0.4369 - val_loss: 314.8459 - val_mae: 14.1881 - val_mse: 314.8459 - val_qwk: 0.4236\nEpoch 10/200\n29/29 - 1s - 48ms/step - loss: 299.1038 - mae: 13.8214 - mse: 299.1038 - qwk: 0.4344 - val_loss: 317.2433 - val_mae: 14.2656 - val_mse: 317.2433 - val_qwk: 0.4049\nEpoch 11/200\n29/29 - 1s - 48ms/step - loss: 295.8041 - mae: 13.7049 - mse: 295.8041 - qwk: 0.4533 - val_loss: 315.1753 - val_mae: 14.2116 - val_mse: 315.1753 - val_qwk: 0.4221\nEpoch 12/200\n29/29 - 1s - 49ms/step - loss: 295.4490 - mae: 13.6747 - mse: 295.4490 - qwk: 0.4301 - val_loss: 317.6443 - val_mae: 14.2772 - val_mse: 317.6443 - val_qwk: 0.4035\nEpoch 13/200\n29/29 - 1s - 48ms/step - loss: 298.3127 - mae: 13.7634 - mse: 298.3127 - qwk: 0.4399 - val_loss: 317.1663 - val_mae: 14.2576 - val_mse: 317.1663 - val_qwk: 0.4153\nEpoch 14/200\n29/29 - 1s - 48ms/step - loss: 292.9801 - mae: 13.6296 - mse: 292.9801 - qwk: 0.4437 - val_loss: 316.0804 - val_mae: 14.2367 - val_mse: 316.0804 - val_qwk: 0.4229\nEpoch 15/200\n29/29 - 1s - 48ms/step - loss: 288.0186 - mae: 13.4984 - mse: 288.0186 - qwk: 0.4535 - val_loss: 316.4469 - val_mae: 14.2313 - val_mse: 316.4469 - val_qwk: 0.4116\nEpoch 16/200\n29/29 - 1s - 47ms/step - loss: 291.2866 - mae: 13.5794 - mse: 291.2866 - qwk: 0.4436 - val_loss: 316.0037 - val_mae: 14.2081 - val_mse: 316.0037 - val_qwk: 0.4188\nEpoch 17/200\n29/29 - 1s - 48ms/step - loss: 290.3376 - mae: 13.6030 - mse: 290.3376 - qwk: 0.4554 - val_loss: 315.6141 - val_mae: 14.2229 - val_mse: 315.6141 - val_qwk: 0.4112\nEpoch 18/200\n29/29 - 1s - 47ms/step - loss: 288.9763 - mae: 13.6098 - mse: 288.9763 - qwk: 0.4531 - val_loss: 316.8550 - val_mae: 14.2618 - val_mse: 316.8550 - val_qwk: 0.3971\nEpoch 19/200\n29/29 - 1s - 47ms/step - loss: 282.7868 - mae: 13.3937 - mse: 282.7868 - qwk: 0.4662 - val_loss: 315.9553 - val_mae: 14.2852 - val_mse: 315.9553 - val_qwk: 0.4071\nEpoch 20/200\n29/29 - 1s - 48ms/step - loss: 292.8925 - mae: 13.6941 - mse: 292.8925 - qwk: 0.4427 - val_loss: 315.9643 - val_mae: 14.1953 - val_mse: 315.9643 - val_qwk: 0.4060\nEpoch 21/200\n29/29 - 1s - 48ms/step - loss: 292.9356 - mae: 13.6539 - mse: 292.9356 - qwk: 0.4479 - val_loss: 317.6725 - val_mae: 14.2932 - val_mse: 317.6725 - val_qwk: 0.4002\nEpoch 22/200\n29/29 - 1s - 48ms/step - loss: 284.6253 - mae: 13.3902 - mse: 284.6253 - qwk: 0.4590 - val_loss: 317.8546 - val_mae: 14.2912 - val_mse: 317.8546 - val_qwk: 0.3976\nEpoch 23/200\n29/29 - 1s - 48ms/step - loss: 281.5523 - mae: 13.4118 - mse: 281.5523 - qwk: 0.4711 - val_loss: 317.5572 - val_mae: 14.2771 - val_mse: 317.5572 - val_qwk: 0.4076\nEpoch 24/200\n29/29 - 1s - 48ms/step - loss: 285.4773 - mae: 13.5294 - mse: 285.4773 - qwk: 0.4682 - val_loss: 317.0803 - val_mae: 14.2676 - val_mse: 317.0803 - val_qwk: 0.3969\nEpoch 25/200\n29/29 - 1s - 48ms/step - loss: 281.1327 - mae: 13.3439 - mse: 281.1327 - qwk: 0.4687 - val_loss: 318.8616 - val_mae: 14.3441 - val_mse: 318.8616 - val_qwk: 0.3879\nEpoch 26/200\n29/29 - 1s - 48ms/step - loss: 279.5972 - mae: 13.3535 - mse: 279.5972 - qwk: 0.4708 - val_loss: 318.1159 - val_mae: 14.2887 - val_mse: 318.1159 - val_qwk: 0.3906\nEpoch 27/200\n29/29 - 1s - 48ms/step - loss: 281.4975 - mae: 13.2888 - mse: 281.4975 - qwk: 0.4622 - val_loss: 320.2851 - val_mae: 14.3397 - val_mse: 320.2851 - val_qwk: 0.3796\nEpoch 28/200\n29/29 - 1s - 47ms/step - loss: 280.5766 - mae: 13.3270 - mse: 280.5766 - qwk: 0.4660 - val_loss: 318.2916 - val_mae: 14.2637 - val_mse: 318.2916 - val_qwk: 0.3981\nEpoch 29/200\n29/29 - 1s - 47ms/step - loss: 276.1646 - mae: 13.2307 - mse: 276.1646 - qwk: 0.4825 - val_loss: 320.3269 - val_mae: 14.3348 - val_mse: 320.3269 - val_qwk: 0.3874\n29/29 - 0s - 15ms/step - loss: 314.8458 - mae: 14.1881 - mse: 314.8458 - qwk: 0.4181\nEpoch 1/200\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"29/29 - 1s - 50ms/step - loss: 1100.2269 - mae: 27.0517 - mse: 1100.2269 - qwk: 0.0175 - val_loss: 968.7047 - val_mae: 25.6485 - val_mse: 968.7047 - val_qwk: 0.0276\nEpoch 2/200\n29/29 - 1s - 49ms/step - loss: 839.3163 - mae: 23.2492 - mse: 839.3163 - qwk: 0.1040 - val_loss: 687.4525 - val_mae: 21.0107 - val_mse: 687.4525 - val_qwk: 0.1095\nEpoch 3/200\n29/29 - 1s - 48ms/step - loss: 597.5433 - mae: 19.4010 - mse: 597.5433 - qwk: 0.1837 - val_loss: 507.9373 - val_mae: 17.9267 - val_mse: 507.9373 - val_qwk: 0.1738\nEpoch 4/200\n29/29 - 1s - 47ms/step - loss: 486.8837 - mae: 17.4935 - mse: 486.8837 - qwk: 0.2515 - val_loss: 411.6736 - val_mae: 16.1071 - val_mse: 411.6736 - val_qwk: 0.2652\nEpoch 5/200\n29/29 - 1s - 49ms/step - loss: 405.4732 - mae: 15.8904 - mse: 405.4732 - qwk: 0.3352 - val_loss: 351.7168 - val_mae: 14.8674 - val_mse: 351.7168 - val_qwk: 0.3541\nEpoch 6/200\n29/29 - 1s - 48ms/step - loss: 360.5857 - mae: 15.1124 - mse: 360.5857 - qwk: 0.3730 - val_loss: 319.9222 - val_mae: 14.2113 - val_mse: 319.9222 - val_qwk: 0.3921\nEpoch 7/200\n29/29 - 1s - 47ms/step - loss: 347.1412 - mae: 14.9374 - mse: 347.1412 - qwk: 0.3990 - val_loss: 302.6654 - val_mae: 13.8816 - val_mse: 302.6654 - val_qwk: 0.4080\nEpoch 8/200\n29/29 - 1s - 48ms/step - loss: 334.5537 - mae: 14.6718 - mse: 334.5537 - qwk: 0.4168 - val_loss: 294.9374 - val_mae: 13.6838 - val_mse: 294.9374 - val_qwk: 0.4054\nEpoch 9/200\n29/29 - 1s - 48ms/step - loss: 326.1955 - mae: 14.4300 - mse: 326.1955 - qwk: 0.4331 - val_loss: 293.0035 - val_mae: 13.6325 - val_mse: 293.0035 - val_qwk: 0.4131\nEpoch 10/200\n29/29 - 1s - 48ms/step - loss: 317.0810 - mae: 14.2978 - mse: 317.0810 - qwk: 0.4408 - val_loss: 291.8250 - val_mae: 13.6339 - val_mse: 291.8250 - val_qwk: 0.4148\nEpoch 11/200\n29/29 - 1s - 47ms/step - loss: 317.7388 - mae: 14.3438 - mse: 317.7388 - qwk: 0.4342 - val_loss: 287.4574 - val_mae: 13.5234 - val_mse: 287.4574 - val_qwk: 0.4118\nEpoch 12/200\n29/29 - 1s - 48ms/step - loss: 316.7048 - mae: 14.2503 - mse: 316.7048 - qwk: 0.4440 - val_loss: 285.7821 - val_mae: 13.4716 - val_mse: 285.7821 - val_qwk: 0.4242\nEpoch 13/200\n29/29 - 1s - 48ms/step - loss: 317.8458 - mae: 14.3267 - mse: 317.8458 - qwk: 0.4294 - val_loss: 285.8104 - val_mae: 13.5147 - val_mse: 285.8104 - val_qwk: 0.4246\nEpoch 14/200\n29/29 - 1s - 48ms/step - loss: 305.5484 - mae: 14.0715 - mse: 305.5484 - qwk: 0.4553 - val_loss: 283.9528 - val_mae: 13.4674 - val_mse: 283.9528 - val_qwk: 0.4253\nEpoch 15/200\n29/29 - 1s - 47ms/step - loss: 310.1704 - mae: 14.1307 - mse: 310.1704 - qwk: 0.4521 - val_loss: 284.6226 - val_mae: 13.4629 - val_mse: 284.6226 - val_qwk: 0.4310\nEpoch 16/200\n29/29 - 1s - 48ms/step - loss: 310.0807 - mae: 14.1861 - mse: 310.0807 - qwk: 0.4572 - val_loss: 281.6093 - val_mae: 13.4016 - val_mse: 281.6093 - val_qwk: 0.4325\nEpoch 17/200\n29/29 - 1s - 49ms/step - loss: 307.5580 - mae: 14.0189 - mse: 307.5580 - qwk: 0.4483 - val_loss: 281.0702 - val_mae: 13.4104 - val_mse: 281.0702 - val_qwk: 0.4375\nEpoch 18/200\n29/29 - 1s - 48ms/step - loss: 306.1458 - mae: 13.9754 - mse: 306.1458 - qwk: 0.4551 - val_loss: 280.5082 - val_mae: 13.3633 - val_mse: 280.5082 - val_qwk: 0.4438\nEpoch 19/200\n29/29 - 1s - 48ms/step - loss: 299.1089 - mae: 13.8772 - mse: 299.1089 - qwk: 0.4630 - val_loss: 279.6340 - val_mae: 13.3622 - val_mse: 279.6340 - val_qwk: 0.4375\nEpoch 20/200\n29/29 - 1s - 48ms/step - loss: 300.0452 - mae: 13.9456 - mse: 300.0452 - qwk: 0.4611 - val_loss: 280.5485 - val_mae: 13.3799 - val_mse: 280.5485 - val_qwk: 0.4424\nEpoch 21/200\n29/29 - 1s - 47ms/step - loss: 303.1552 - mae: 14.0496 - mse: 303.1552 - qwk: 0.4691 - val_loss: 279.9716 - val_mae: 13.3873 - val_mse: 279.9716 - val_qwk: 0.4354\nEpoch 22/200\n29/29 - 1s - 48ms/step - loss: 295.1074 - mae: 13.7280 - mse: 295.1074 - qwk: 0.4711 - val_loss: 279.6157 - val_mae: 13.4010 - val_mse: 279.6157 - val_qwk: 0.4454\nEpoch 23/200\n29/29 - 1s - 48ms/step - loss: 298.0308 - mae: 13.9651 - mse: 298.0308 - qwk: 0.4665 - val_loss: 279.4962 - val_mae: 13.3798 - val_mse: 279.4962 - val_qwk: 0.4291\nEpoch 24/200\n29/29 - 1s - 49ms/step - loss: 291.8180 - mae: 13.6831 - mse: 291.8180 - qwk: 0.4774 - val_loss: 277.6897 - val_mae: 13.3568 - val_mse: 277.6897 - val_qwk: 0.4446\nEpoch 25/200\n29/29 - 1s - 48ms/step - loss: 297.1332 - mae: 13.8596 - mse: 297.1332 - qwk: 0.4684 - val_loss: 278.4695 - val_mae: 13.3425 - val_mse: 278.4695 - val_qwk: 0.4313\nEpoch 26/200\n29/29 - 1s - 48ms/step - loss: 296.9771 - mae: 13.8500 - mse: 296.9771 - qwk: 0.4742 - val_loss: 279.7657 - val_mae: 13.3710 - val_mse: 279.7657 - val_qwk: 0.4479\nEpoch 27/200\n29/29 - 1s - 50ms/step - loss: 292.0429 - mae: 13.7464 - mse: 292.0429 - qwk: 0.4863 - val_loss: 279.5809 - val_mae: 13.3810 - val_mse: 279.5809 - val_qwk: 0.4441\nEpoch 28/200\n29/29 - 1s - 48ms/step - loss: 297.6758 - mae: 13.8217 - mse: 297.6758 - qwk: 0.4684 - val_loss: 279.9328 - val_mae: 13.3535 - val_mse: 279.9328 - val_qwk: 0.4338\nEpoch 29/200\n29/29 - 1s - 48ms/step - loss: 288.6616 - mae: 13.5453 - mse: 288.6616 - qwk: 0.4892 - val_loss: 279.7585 - val_mae: 13.3533 - val_mse: 279.7585 - val_qwk: 0.4442\nEpoch 30/200\n29/29 - 1s - 48ms/step - loss: 295.4221 - mae: 13.7698 - mse: 295.4221 - qwk: 0.4801 - val_loss: 280.3601 - val_mae: 13.3878 - val_mse: 280.3601 - val_qwk: 0.4372\nEpoch 31/200\n29/29 - 1s - 49ms/step - loss: 293.4941 - mae: 13.7747 - mse: 293.4941 - qwk: 0.4799 - val_loss: 281.4715 - val_mae: 13.4018 - val_mse: 281.4715 - val_qwk: 0.4384\nEpoch 32/200\n29/29 - 1s - 49ms/step - loss: 288.8588 - mae: 13.6219 - mse: 288.8588 - qwk: 0.4877 - val_loss: 281.1591 - val_mae: 13.3916 - val_mse: 281.1591 - val_qwk: 0.4395\nEpoch 33/200\n29/29 - 1s - 48ms/step - loss: 296.8676 - mae: 13.7522 - mse: 296.8676 - qwk: 0.4688 - val_loss: 282.6869 - val_mae: 13.4294 - val_mse: 282.6869 - val_qwk: 0.4477\nEpoch 34/200\n29/29 - 1s - 48ms/step - loss: 288.1034 - mae: 13.6526 - mse: 288.1034 - qwk: 0.4994 - val_loss: 283.5471 - val_mae: 13.4535 - val_mse: 283.5471 - val_qwk: 0.4479\nEpoch 35/200\n29/29 - 1s - 49ms/step - loss: 290.0633 - mae: 13.6320 - mse: 290.0633 - qwk: 0.4933 - val_loss: 283.1329 - val_mae: 13.4365 - val_mse: 283.1329 - val_qwk: 0.4521\nEpoch 36/200\n29/29 - 1s - 49ms/step - loss: 294.5460 - mae: 13.6873 - mse: 294.5460 - qwk: 0.4695 - val_loss: 283.4980 - val_mae: 13.4500 - val_mse: 283.4980 - val_qwk: 0.4503\nEpoch 37/200\n29/29 - 1s - 49ms/step - loss: 290.4794 - mae: 13.5805 - mse: 290.4794 - qwk: 0.4918 - val_loss: 282.9837 - val_mae: 13.4425 - val_mse: 282.9837 - val_qwk: 0.4547\nEpoch 38/200\n29/29 - 1s - 49ms/step - loss: 294.0301 - mae: 13.7354 - mse: 294.0301 - qwk: 0.4793 - val_loss: 282.2808 - val_mae: 13.4340 - val_mse: 282.2808 - val_qwk: 0.4388\nEpoch 39/200\n29/29 - 1s - 49ms/step - loss: 288.2396 - mae: 13.5984 - mse: 288.2396 - qwk: 0.5018 - val_loss: 281.5887 - val_mae: 13.4258 - val_mse: 281.5887 - val_qwk: 0.4393\nEpoch 40/200\n29/29 - 1s - 48ms/step - loss: 285.6175 - mae: 13.5336 - mse: 285.6175 - qwk: 0.5065 - val_loss: 282.1188 - val_mae: 13.4402 - val_mse: 282.1188 - val_qwk: 0.4358\nEpoch 41/200\n29/29 - 1s - 48ms/step - loss: 287.0143 - mae: 13.4940 - mse: 287.0143 - qwk: 0.4999 - val_loss: 282.3267 - val_mae: 13.4489 - val_mse: 282.3267 - val_qwk: 0.4406\nEpoch 42/200\n29/29 - 1s - 48ms/step - loss: 289.5935 - mae: 13.6181 - mse: 289.5935 - qwk: 0.4907 - val_loss: 283.0640 - val_mae: 13.4574 - val_mse: 283.0640 - val_qwk: 0.4336\nEpoch 43/200\n29/29 - 1s - 48ms/step - loss: 282.5920 - mae: 13.4065 - mse: 282.5920 - qwk: 0.5086 - val_loss: 282.9899 - val_mae: 13.4609 - val_mse: 282.9899 - val_qwk: 0.4334\nEpoch 44/200\n29/29 - 1s - 49ms/step - loss: 285.2780 - mae: 13.6385 - mse: 285.2780 - qwk: 0.4917 - val_loss: 283.2421 - val_mae: 13.4786 - val_mse: 283.2421 - val_qwk: 0.4385\n29/29 - 0s - 15ms/step - loss: 277.6896 - mae: 13.3568 - mse: 277.6896 - qwk: 0.4337\nEpoch 1/200\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"29/29 - 2s - 53ms/step - loss: 1066.0626 - mae: 26.2789 - mse: 1066.0626 - qwk: 0.0093 - val_loss: 1006.8986 - val_mae: 25.7076 - val_mse: 1006.8986 - val_qwk: 0.0258\nEpoch 2/200\n29/29 - 1s - 49ms/step - loss: 793.1336 - mae: 22.4547 - mse: 793.1336 - qwk: 0.0769 - val_loss: 709.0662 - val_mae: 21.2366 - val_mse: 709.0662 - val_qwk: 0.1300\nEpoch 3/200\n29/29 - 1s - 50ms/step - loss: 564.6698 - mae: 18.8144 - mse: 564.6698 - qwk: 0.1883 - val_loss: 543.9591 - val_mae: 18.7049 - val_mse: 543.9591 - val_qwk: 0.1778\nEpoch 4/200\n29/29 - 1s - 49ms/step - loss: 454.3202 - mae: 16.8684 - mse: 454.3202 - qwk: 0.2715 - val_loss: 441.3652 - val_mae: 16.8963 - val_mse: 441.3652 - val_qwk: 0.2393\nEpoch 5/200\n29/29 - 1s - 51ms/step - loss: 370.1662 - mae: 15.2804 - mse: 370.1662 - qwk: 0.3519 - val_loss: 374.6832 - val_mae: 15.5310 - val_mse: 374.6832 - val_qwk: 0.3317\nEpoch 6/200\n29/29 - 1s - 49ms/step - loss: 338.9432 - mae: 14.7202 - mse: 338.9432 - qwk: 0.3785 - val_loss: 345.7259 - val_mae: 14.8663 - val_mse: 345.7259 - val_qwk: 0.3733\nEpoch 7/200\n29/29 - 1s - 48ms/step - loss: 313.2272 - mae: 14.1047 - mse: 313.2272 - qwk: 0.4137 - val_loss: 333.3381 - val_mae: 14.6542 - val_mse: 333.3381 - val_qwk: 0.3908\nEpoch 8/200\n29/29 - 1s - 49ms/step - loss: 311.6311 - mae: 14.1039 - mse: 311.6311 - qwk: 0.4085 - val_loss: 327.2693 - val_mae: 14.5519 - val_mse: 327.2693 - val_qwk: 0.4028\nEpoch 9/200\n29/29 - 1s - 50ms/step - loss: 302.9420 - mae: 13.9191 - mse: 302.9420 - qwk: 0.4199 - val_loss: 323.2084 - val_mae: 14.4614 - val_mse: 323.2084 - val_qwk: 0.4294\nEpoch 10/200\n29/29 - 1s - 47ms/step - loss: 295.9997 - mae: 13.6994 - mse: 295.9997 - qwk: 0.4442 - val_loss: 321.0573 - val_mae: 14.4094 - val_mse: 321.0573 - val_qwk: 0.4272\nEpoch 11/200\n29/29 - 1s - 48ms/step - loss: 296.3770 - mae: 13.7317 - mse: 296.3770 - qwk: 0.4341 - val_loss: 321.5795 - val_mae: 14.4279 - val_mse: 321.5795 - val_qwk: 0.4251\nEpoch 12/200\n29/29 - 1s - 48ms/step - loss: 291.0767 - mae: 13.5574 - mse: 291.0767 - qwk: 0.4561 - val_loss: 319.2728 - val_mae: 14.3421 - val_mse: 319.2728 - val_qwk: 0.4361\nEpoch 13/200\n29/29 - 1s - 48ms/step - loss: 289.1328 - mae: 13.5254 - mse: 289.1328 - qwk: 0.4464 - val_loss: 319.9622 - val_mae: 14.3801 - val_mse: 319.9622 - val_qwk: 0.4413\nEpoch 14/200\n29/29 - 1s - 48ms/step - loss: 290.3820 - mae: 13.5982 - mse: 290.3820 - qwk: 0.4400 - val_loss: 319.2216 - val_mae: 14.3434 - val_mse: 319.2216 - val_qwk: 0.4368\nEpoch 15/200\n29/29 - 1s - 49ms/step - loss: 291.1878 - mae: 13.5165 - mse: 291.1878 - qwk: 0.4421 - val_loss: 318.9314 - val_mae: 14.3381 - val_mse: 318.9314 - val_qwk: 0.4372\nEpoch 16/200\n29/29 - 1s - 48ms/step - loss: 282.1682 - mae: 13.4151 - mse: 282.1682 - qwk: 0.4603 - val_loss: 318.4952 - val_mae: 14.3247 - val_mse: 318.4952 - val_qwk: 0.4282\nEpoch 17/200\n29/29 - 1s - 48ms/step - loss: 288.4608 - mae: 13.5481 - mse: 288.4608 - qwk: 0.4576 - val_loss: 319.5766 - val_mae: 14.3604 - val_mse: 319.5766 - val_qwk: 0.4325\nEpoch 18/200\n29/29 - 1s - 49ms/step - loss: 283.7050 - mae: 13.4556 - mse: 283.7050 - qwk: 0.4691 - val_loss: 319.4092 - val_mae: 14.3423 - val_mse: 319.4092 - val_qwk: 0.4220\nEpoch 19/200\n29/29 - 1s - 48ms/step - loss: 281.0119 - mae: 13.3514 - mse: 281.0119 - qwk: 0.4668 - val_loss: 318.9759 - val_mae: 14.3439 - val_mse: 318.9759 - val_qwk: 0.4286\nEpoch 20/200\n29/29 - 1s - 49ms/step - loss: 285.8788 - mae: 13.4593 - mse: 285.8788 - qwk: 0.4578 - val_loss: 319.4824 - val_mae: 14.3234 - val_mse: 319.4824 - val_qwk: 0.4239\nEpoch 21/200\n29/29 - 1s - 50ms/step - loss: 282.9421 - mae: 13.4156 - mse: 282.9421 - qwk: 0.4529 - val_loss: 318.2559 - val_mae: 14.2943 - val_mse: 318.2559 - val_qwk: 0.4360\nEpoch 22/200\n29/29 - 1s - 49ms/step - loss: 276.6063 - mae: 13.1880 - mse: 276.6063 - qwk: 0.4684 - val_loss: 318.5461 - val_mae: 14.3017 - val_mse: 318.5461 - val_qwk: 0.4256\nEpoch 23/200\n29/29 - 1s - 49ms/step - loss: 273.3379 - mae: 13.1406 - mse: 273.3379 - qwk: 0.4831 - val_loss: 318.1225 - val_mae: 14.2958 - val_mse: 318.1225 - val_qwk: 0.4349\nEpoch 24/200\n29/29 - 1s - 49ms/step - loss: 274.8122 - mae: 13.2045 - mse: 274.8122 - qwk: 0.4794 - val_loss: 319.8242 - val_mae: 14.3023 - val_mse: 319.8242 - val_qwk: 0.4371\nEpoch 25/200\n29/29 - 1s - 49ms/step - loss: 274.6873 - mae: 13.1709 - mse: 274.6873 - qwk: 0.4891 - val_loss: 320.0070 - val_mae: 14.3466 - val_mse: 320.0070 - val_qwk: 0.4374\nEpoch 26/200\n29/29 - 1s - 49ms/step - loss: 273.3723 - mae: 13.1875 - mse: 273.3723 - qwk: 0.4879 - val_loss: 318.5416 - val_mae: 14.3032 - val_mse: 318.5416 - val_qwk: 0.4404\nEpoch 27/200\n29/29 - 1s - 50ms/step - loss: 277.3415 - mae: 13.2658 - mse: 277.3415 - qwk: 0.4704 - val_loss: 318.0399 - val_mae: 14.2896 - val_mse: 318.0399 - val_qwk: 0.4429\nEpoch 28/200\n29/29 - 1s - 50ms/step - loss: 278.4477 - mae: 13.3588 - mse: 278.4477 - qwk: 0.4720 - val_loss: 319.2470 - val_mae: 14.3197 - val_mse: 319.2470 - val_qwk: 0.4450\nEpoch 29/200\n29/29 - 1s - 50ms/step - loss: 279.2364 - mae: 13.3145 - mse: 279.2364 - qwk: 0.4793 - val_loss: 319.6557 - val_mae: 14.3103 - val_mse: 319.6557 - val_qwk: 0.4466\nEpoch 30/200\n29/29 - 1s - 48ms/step - loss: 269.5308 - mae: 13.0457 - mse: 269.5308 - qwk: 0.4893 - val_loss: 319.1924 - val_mae: 14.2961 - val_mse: 319.1924 - val_qwk: 0.4446\nEpoch 31/200\n29/29 - 1s - 48ms/step - loss: 274.1245 - mae: 13.1730 - mse: 274.1245 - qwk: 0.4858 - val_loss: 318.9316 - val_mae: 14.2975 - val_mse: 318.9316 - val_qwk: 0.4468\nEpoch 32/200\n29/29 - 1s - 48ms/step - loss: 270.0785 - mae: 12.9897 - mse: 270.0785 - qwk: 0.4932 - val_loss: 320.8391 - val_mae: 14.3292 - val_mse: 320.8391 - val_qwk: 0.4351\nEpoch 33/200\n29/29 - 1s - 48ms/step - loss: 268.1847 - mae: 13.0842 - mse: 268.1847 - qwk: 0.4970 - val_loss: 320.7555 - val_mae: 14.3483 - val_mse: 320.7555 - val_qwk: 0.4310\nEpoch 34/200\n29/29 - 1s - 50ms/step - loss: 270.4593 - mae: 13.0445 - mse: 270.4593 - qwk: 0.4907 - val_loss: 320.7559 - val_mae: 14.3107 - val_mse: 320.7559 - val_qwk: 0.4343\nEpoch 35/200\n29/29 - 1s - 49ms/step - loss: 267.2334 - mae: 12.9430 - mse: 267.2334 - qwk: 0.4978 - val_loss: 320.7900 - val_mae: 14.3327 - val_mse: 320.7900 - val_qwk: 0.4387\nEpoch 36/200\n29/29 - 1s - 50ms/step - loss: 276.8363 - mae: 13.2221 - mse: 276.8363 - qwk: 0.4787 - val_loss: 320.4044 - val_mae: 14.3295 - val_mse: 320.4044 - val_qwk: 0.4393\nEpoch 37/200\n29/29 - 1s - 49ms/step - loss: 268.0228 - mae: 13.0194 - mse: 268.0228 - qwk: 0.4922 - val_loss: 321.2675 - val_mae: 14.3504 - val_mse: 321.2675 - val_qwk: 0.4273\nEpoch 38/200\n29/29 - 1s - 49ms/step - loss: 269.0878 - mae: 13.1030 - mse: 269.0878 - qwk: 0.4978 - val_loss: 320.8832 - val_mae: 14.3491 - val_mse: 320.8832 - val_qwk: 0.4381\nEpoch 39/200\n29/29 - 1s - 49ms/step - loss: 266.0735 - mae: 12.9710 - mse: 266.0735 - qwk: 0.5040 - val_loss: 322.1839 - val_mae: 14.3573 - val_mse: 322.1839 - val_qwk: 0.4382\nEpoch 40/200\n29/29 - 1s - 49ms/step - loss: 267.4377 - mae: 13.0266 - mse: 267.4377 - qwk: 0.4989 - val_loss: 320.9687 - val_mae: 14.3454 - val_mse: 320.9687 - val_qwk: 0.4421\nEpoch 41/200\n29/29 - 1s - 49ms/step - loss: 265.6859 - mae: 13.0079 - mse: 265.6859 - qwk: 0.5060 - val_loss: 322.0104 - val_mae: 14.3760 - val_mse: 322.0104 - val_qwk: 0.4371\nEpoch 42/200\n29/29 - 1s - 49ms/step - loss: 273.6431 - mae: 13.1551 - mse: 273.6431 - qwk: 0.4794 - val_loss: 321.9019 - val_mae: 14.3853 - val_mse: 321.9019 - val_qwk: 0.4383\nEpoch 43/200\n29/29 - 1s - 50ms/step - loss: 267.8120 - mae: 13.0136 - mse: 267.8120 - qwk: 0.4972 - val_loss: 321.1859 - val_mae: 14.3396 - val_mse: 321.1859 - val_qwk: 0.4435\nEpoch 44/200\n29/29 - 1s - 49ms/step - loss: 267.2551 - mae: 13.0005 - mse: 267.2551 - qwk: 0.5037 - val_loss: 322.7798 - val_mae: 14.4191 - val_mse: 322.7798 - val_qwk: 0.4348\nEpoch 45/200\n29/29 - 1s - 49ms/step - loss: 265.6153 - mae: 12.9603 - mse: 265.6153 - qwk: 0.5020 - val_loss: 322.3899 - val_mae: 14.3908 - val_mse: 322.3899 - val_qwk: 0.4325\nEpoch 46/200\n29/29 - 1s - 50ms/step - loss: 267.1429 - mae: 13.0713 - mse: 267.1429 - qwk: 0.4912 - val_loss: 323.1451 - val_mae: 14.3914 - val_mse: 323.1451 - val_qwk: 0.4364\nEpoch 47/200\n29/29 - 1s - 50ms/step - loss: 267.1810 - mae: 12.9274 - mse: 267.1810 - qwk: 0.5012 - val_loss: 322.8592 - val_mae: 14.3870 - val_mse: 322.8592 - val_qwk: 0.4365\n29/29 - 0s - 16ms/step - loss: 318.0399 - mae: 14.2896 - mse: 318.0399 - qwk: 0.4277\nAverage Validation Loss: 303.52512613932294\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# Graph most recent model history\n'''\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nval_loss, val_mae, val_mse, val_qwk = model.evaluate(X_train_v, y_train_v)\nprint(f\"Validation loss: {val_loss:.4f}, Validation MAE: {val_mae:.4f}\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T04:33:43.908474Z","iopub.execute_input":"2025-01-16T04:33:43.909013Z","iopub.status.idle":"2025-01-16T04:33:44.732435Z","shell.execute_reply.started":"2025-01-16T04:33:43.908970Z","shell.execute_reply":"2025-01-16T04:33:44.731316Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 329.2241 - mae: 14.5348 - mse: 329.2241 - qwk: 0.4041\nValidation loss: 318.0399, Validation MAE: 14.2896\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"'''\nplt.plot(history.history['qwk'])\nplt.plot(history.history['val_qwk'])\nplt.title('Model QWK')\nplt.xlabel('Epoch')\nplt.ylabel('QWK')\nplt.legend(['qwk','val_qwk'])\nplt.show()\n\nval_loss, val_mae, val_mse, val_qwk = model.evaluate(X_train_v, y_train_v)\nprint(f\"Validation QWK: {val_qwk:.4f}\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T07:31:26.796918Z","iopub.execute_input":"2025-01-16T07:31:26.797360Z","iopub.status.idle":"2025-01-16T07:31:26.804974Z","shell.execute_reply.started":"2025-01-16T07:31:26.797330Z","shell.execute_reply":"2025-01-16T07:31:26.803759Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"'\\nplt.plot(history.history[\\'qwk\\'])\\nplt.plot(history.history[\\'val_qwk\\'])\\nplt.title(\\'Model QWK\\')\\nplt.xlabel(\\'Epoch\\')\\nplt.ylabel(\\'QWK\\')\\nplt.legend([\\'qwk\\',\\'val_qwk\\'])\\nplt.show()\\n\\nval_loss, val_mae, val_mse, val_qwk = model.evaluate(X_train_v, y_train_v)\\nprint(f\"Validation QWK: {val_qwk:.4f}\")\\n'"},"metadata":{}}],"execution_count":49},{"cell_type":"markdown","source":"Model not really improving after a time. Probably not complex enough, as expected.","metadata":{}},{"cell_type":"markdown","source":"**Score: 0.378, 0.370**","metadata":{}},{"cell_type":"code","source":"def neg_qwk(y_true, y_pred):\n    return (-qwk(y_true,y_pred))\n# Was trying to build a loss function, but wasn't really working as it seems it isn't differentiable\n# Will look to build a different loss function with more in common with qwk than mse","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T06:56:18.516930Z","iopub.execute_input":"2025-01-16T06:56:18.517352Z","iopub.status.idle":"2025-01-16T06:56:18.522296Z","shell.execute_reply.started":"2025-01-16T06:56:18.517314Z","shell.execute_reply":"2025-01-16T06:56:18.521111Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"# V2: Layered model","metadata":{}},{"cell_type":"code","source":"# Increase neurons and add another hidden layer\ndef create_model_v2():\n    #model = keras.models.Sequential([\n    #    keras.layers.Dense(128, input_shape=(X_train_labelled.shape[1],), activation=\"selu\", kernel_initializer=\"he_normal\"),\n    #    keras.layers.Dropout(rate=0.2),\n    #    keras.layers.Dense(64, activation=\"selu\", kernel_initializer=\"he_normal\"),\n    #    keras.layers.Dropout(rate=0.2),\n    #    keras.layers.Dense(32, activation=\"selu\", kernel_initializer=\"he_normal\"),\n    #    keras.layers.Dropout(rate=0.2),\n    #    keras.layers.Dense(1, activation=\"linear\")\n    #])\n\n    model = keras.models.Sequential()\n    model.add(keras.layers.Dense(128, input_shape=(X_train_labelled.shape[1],), kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(keras.layers.Activation('selu'))\n    model.add(Dropout(0.2))\n    model.add(keras.layers.Dense(64, kernel_initializer='he_normal'))\n    model.add(BatchNormalization())\n    model.add(keras.layers.Activation('selu'))\n    model.add(Dropout(0.2))\n    model.add(keras.layers.Dense(32, kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n    model.add(BatchNormalization())\n    model.add(keras.layers.Activation('selu'))\n    model.add(Dropout(0.2))\n    model.add(keras.layers.Dense(32, kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n    model.add(BatchNormalization())\n    model.add(keras.layers.Activation('selu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(1, activation=\"linear\"))\n\n    model.compile(optimizer=Adam(learning_rate=0.002), loss='mean_squared_error', metrics=['mae','mse',qwk],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    #model.compile(optimizer=Adam(learning_rate=0.005), loss=neg_qwk, metrics=[qwk],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T06:59:19.770025Z","iopub.execute_input":"2025-01-16T06:59:19.770423Z","iopub.status.idle":"2025-01-16T06:59:19.779660Z","shell.execute_reply.started":"2025-01-16T06:59:19.770394Z","shell.execute_reply":"2025-01-16T06:59:19.778303Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"'''\n# Prepare to store results\nvalidation_qwk = []\nvalidation_losses = []\nmodels = []\n\n# Loop over each fold\nfor train_index, val_index in kf.split(X_train_labelled):\n    # Split the data into training and validation sets for the current fold\n    X_train_t, X_train_v = X_train_labelled.loc[train_index], X_train_labelled.loc[val_index]\n    y_train_t, y_train_v = y_train_labelled.loc[train_index], y_train_labelled.loc[val_index]\n\n    # Build a new model for each fold\n    model = create_model_v2()\n    \n    # Define early stopping to avoid overfitting\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n    \n    # Train the model on the training set and evaluate on the validation set\n    history = model.fit(\n        X_train_t, y_train_t,\n        validation_data=(X_train_v, y_train_v),\n        epochs=100,\n        batch_size=64,\n        callbacks=[early_stopping],\n        verbose=2\n    )\n    \n    # Evaluate the model on the validation set\n    val_loss, val_mae, val_mse, val_qwk = model.evaluate(X_train_v, y_train_v, verbose=2)\n    #val_qwk = model.evaluate(X_train_v, y_train_v, verbose=2)\n    validation_qwk.append(val_qwk)\n    validation_losses.append(val_loss)\n    models.append(model)\n\n# Calculate the average validation loss across all folds\navg_val_qwk = np.mean(validation_qwk)\navg_val_loss = np.mean(validation_losses)\nprint(f\"Average Validation QWK: {avg_val_qwk}, Average Validation Loss: {avg_val_loss}\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T06:59:23.224904Z","iopub.execute_input":"2025-01-16T06:59:23.225257Z","iopub.status.idle":"2025-01-16T07:06:12.461171Z","shell.execute_reply.started":"2025-01-16T06:59:23.225231Z","shell.execute_reply":"2025-01-16T07:06:12.459617Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/100\n29/29 - 5s - 163ms/step - loss: 1154.2693 - mae: 28.3949 - mse: 1153.0018 - qwk: 0.0034 - val_loss: 1349.8922 - val_mae: 32.0319 - val_mse: 1348.6193 - val_qwk: 0.0841\nEpoch 2/100\n29/29 - 5s - 156ms/step - loss: 1066.2209 - mae: 27.8121 - mse: 1064.9446 - qwk: 0.0282 - val_loss: 1153.4597 - val_mae: 29.4269 - val_mse: 1152.1808 - val_qwk: 0.0938\nEpoch 3/100\n29/29 - 5s - 157ms/step - loss: 1000.6998 - mae: 26.9617 - mse: 999.4197 - qwk: 0.0486 - val_loss: 1029.4392 - val_mae: 27.4757 - val_mse: 1028.1587 - val_qwk: 0.1262\nEpoch 4/100\n29/29 - 4s - 155ms/step - loss: 927.5646 - mae: 25.7799 - mse: 926.2841 - qwk: 0.0695 - val_loss: 929.2010 - val_mae: 25.8085 - val_mse: 927.9206 - val_qwk: 0.1102\nEpoch 5/100\n29/29 - 5s - 162ms/step - loss: 839.9752 - mae: 24.2582 - mse: 838.6946 - qwk: 0.0986 - val_loss: 803.2578 - val_mae: 23.5898 - val_mse: 801.9775 - val_qwk: 0.1290\nEpoch 6/100\n29/29 - 5s - 162ms/step - loss: 743.5049 - mae: 22.6001 - mse: 742.2236 - qwk: 0.1178 - val_loss: 698.0325 - val_mae: 21.6067 - val_mse: 696.7502 - val_qwk: 0.1502\nEpoch 7/100\n29/29 - 5s - 158ms/step - loss: 646.5876 - mae: 20.5582 - mse: 645.3044 - qwk: 0.1613 - val_loss: 586.0716 - val_mae: 19.3442 - val_mse: 584.7878 - val_qwk: 0.1965\nEpoch 8/100\n29/29 - 5s - 158ms/step - loss: 547.2163 - mae: 18.5792 - mse: 545.9315 - qwk: 0.2185 - val_loss: 490.4483 - val_mae: 17.3773 - val_mse: 489.1625 - val_qwk: 0.2503\nEpoch 9/100\n29/29 - 5s - 166ms/step - loss: 461.6791 - mae: 16.8091 - mse: 460.3921 - qwk: 0.2744 - val_loss: 417.5815 - val_mae: 15.8263 - val_mse: 416.2932 - val_qwk: 0.3048\nEpoch 10/100\n29/29 - 5s - 162ms/step - loss: 401.8138 - mae: 15.5063 - mse: 400.5240 - qwk: 0.3238 - val_loss: 366.4514 - val_mae: 14.8180 - val_mse: 365.1604 - val_qwk: 0.3666\nEpoch 11/100\n29/29 - 5s - 160ms/step - loss: 353.2971 - mae: 14.6696 - mse: 352.0051 - qwk: 0.3767 - val_loss: 338.1527 - val_mae: 14.3273 - val_mse: 336.8598 - val_qwk: 0.3906\nEpoch 12/100\n29/29 - 5s - 162ms/step - loss: 325.3322 - mae: 14.0704 - mse: 324.0382 - qwk: 0.4140 - val_loss: 326.2563 - val_mae: 14.1734 - val_mse: 324.9611 - val_qwk: 0.4302\nEpoch 13/100\n29/29 - 5s - 162ms/step - loss: 309.1308 - mae: 13.8807 - mse: 307.8347 - qwk: 0.4329 - val_loss: 317.4733 - val_mae: 14.0531 - val_mse: 316.1767 - val_qwk: 0.4428\nEpoch 14/100\n29/29 - 5s - 163ms/step - loss: 298.8276 - mae: 13.6866 - mse: 297.5306 - qwk: 0.4488 - val_loss: 313.5804 - val_mae: 13.9794 - val_mse: 312.2825 - val_qwk: 0.4372\nEpoch 15/100\n29/29 - 5s - 160ms/step - loss: 298.7249 - mae: 13.6281 - mse: 297.4261 - qwk: 0.4570 - val_loss: 313.1704 - val_mae: 14.0239 - val_mse: 311.8712 - val_qwk: 0.4402\nEpoch 16/100\n29/29 - 5s - 158ms/step - loss: 299.7659 - mae: 13.7602 - mse: 298.4659 - qwk: 0.4509 - val_loss: 314.0219 - val_mae: 14.0879 - val_mse: 312.7215 - val_qwk: 0.4296\nEpoch 17/100\n29/29 - 5s - 156ms/step - loss: 297.3360 - mae: 13.5631 - mse: 296.0348 - qwk: 0.4497 - val_loss: 313.2235 - val_mae: 14.0624 - val_mse: 311.9215 - val_qwk: 0.4414\nEpoch 18/100\n29/29 - 5s - 158ms/step - loss: 289.0807 - mae: 13.5460 - mse: 287.7777 - qwk: 0.4667 - val_loss: 314.2762 - val_mae: 14.0791 - val_mse: 312.9726 - val_qwk: 0.4403\nEpoch 19/100\n29/29 - 5s - 160ms/step - loss: 296.6425 - mae: 13.8145 - mse: 295.3386 - qwk: 0.4527 - val_loss: 313.9229 - val_mae: 14.0873 - val_mse: 312.6190 - val_qwk: 0.4415\nEpoch 20/100\n29/29 - 5s - 160ms/step - loss: 295.3196 - mae: 13.6683 - mse: 294.0150 - qwk: 0.4566 - val_loss: 313.1032 - val_mae: 14.0665 - val_mse: 311.7981 - val_qwk: 0.4416\nEpoch 21/100\n29/29 - 5s - 162ms/step - loss: 294.0644 - mae: 13.6774 - mse: 292.7589 - qwk: 0.4519 - val_loss: 312.7851 - val_mae: 14.0433 - val_mse: 311.4790 - val_qwk: 0.4385\nEpoch 22/100\n29/29 - 5s - 163ms/step - loss: 291.1105 - mae: 13.5903 - mse: 289.8042 - qwk: 0.4733 - val_loss: 315.4143 - val_mae: 14.1173 - val_mse: 314.1074 - val_qwk: 0.4362\nEpoch 23/100\n29/29 - 5s - 156ms/step - loss: 288.3090 - mae: 13.6110 - mse: 287.0019 - qwk: 0.4662 - val_loss: 315.7834 - val_mae: 14.1365 - val_mse: 314.4759 - val_qwk: 0.4356\nEpoch 24/100\n29/29 - 5s - 158ms/step - loss: 288.2837 - mae: 13.5906 - mse: 286.9758 - qwk: 0.4696 - val_loss: 318.2420 - val_mae: 14.2100 - val_mse: 316.9338 - val_qwk: 0.4289\nEpoch 25/100\n29/29 - 5s - 162ms/step - loss: 287.5210 - mae: 13.6032 - mse: 286.2124 - qwk: 0.4707 - val_loss: 316.5377 - val_mae: 14.1668 - val_mse: 315.2288 - val_qwk: 0.4397\nEpoch 26/100\n29/29 - 5s - 161ms/step - loss: 293.0797 - mae: 13.6470 - mse: 291.7706 - qwk: 0.4672 - val_loss: 317.0683 - val_mae: 14.2275 - val_mse: 315.7590 - val_qwk: 0.4388\nEpoch 27/100\n29/29 - 5s - 161ms/step - loss: 286.2116 - mae: 13.5184 - mse: 284.9019 - qwk: 0.4792 - val_loss: 318.5106 - val_mae: 14.2550 - val_mse: 317.2003 - val_qwk: 0.4303\nEpoch 28/100\n29/29 - 5s - 159ms/step - loss: 286.6992 - mae: 13.4732 - mse: 285.3885 - qwk: 0.4785 - val_loss: 321.7210 - val_mae: 14.3108 - val_mse: 320.4101 - val_qwk: 0.4328\nEpoch 29/100\n29/29 - 5s - 159ms/step - loss: 288.3876 - mae: 13.5892 - mse: 287.0766 - qwk: 0.4638 - val_loss: 317.2706 - val_mae: 14.1834 - val_mse: 315.9594 - val_qwk: 0.4471\nEpoch 30/100\n29/29 - 5s - 158ms/step - loss: 286.4632 - mae: 13.5242 - mse: 285.1517 - qwk: 0.4710 - val_loss: 316.7686 - val_mae: 14.1759 - val_mse: 315.4570 - val_qwk: 0.4351\nEpoch 31/100\n29/29 - 5s - 159ms/step - loss: 273.1026 - mae: 13.2368 - mse: 271.7909 - qwk: 0.4999 - val_loss: 316.6512 - val_mae: 14.1804 - val_mse: 315.3394 - val_qwk: 0.4312\n29/29 - 1s - 28ms/step - loss: 312.7851 - mae: 14.0433 - mse: 311.4790 - qwk: 0.4341\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/100\n29/29 - 5s - 162ms/step - loss: 1146.2744 - mae: 27.6866 - mse: 1144.9677 - qwk: -1.5313e-17 - val_loss: 1297.4226 - val_mae: 31.8296 - val_mse: 1296.1106 - val_qwk: 0.0186\nEpoch 2/100\n29/29 - 5s - 162ms/step - loss: 1071.1973 - mae: 27.2657 - mse: 1069.8806 - qwk: 0.0126 - val_loss: 1172.8213 - val_mae: 30.0496 - val_mse: 1171.5007 - val_qwk: 0.0338\nEpoch 3/100\n29/29 - 5s - 158ms/step - loss: 1008.3448 - mae: 26.6040 - mse: 1007.0232 - qwk: 0.0436 - val_loss: 1077.3888 - val_mae: 28.4060 - val_mse: 1076.0665 - val_qwk: 0.0631\nEpoch 4/100\n29/29 - 5s - 159ms/step - loss: 930.0975 - mae: 25.5661 - mse: 928.7753 - qwk: 0.0650 - val_loss: 931.2839 - val_mae: 25.9914 - val_mse: 929.9623 - val_qwk: 0.0891\nEpoch 5/100\n29/29 - 5s - 156ms/step - loss: 850.7662 - mae: 24.2423 - mse: 849.4453 - qwk: 0.0938 - val_loss: 816.3024 - val_mae: 23.9863 - val_mse: 814.9823 - val_qwk: 0.1261\nEpoch 6/100\n29/29 - 5s - 158ms/step - loss: 761.7563 - mae: 22.7214 - mse: 760.4371 - qwk: 0.1303 - val_loss: 693.5901 - val_mae: 21.7132 - val_mse: 692.2712 - val_qwk: 0.1371\nEpoch 7/100\n29/29 - 5s - 161ms/step - loss: 667.7332 - mae: 20.7921 - mse: 666.4145 - qwk: 0.1611 - val_loss: 594.0592 - val_mae: 19.6037 - val_mse: 592.7401 - val_qwk: 0.1634\nEpoch 8/100\n29/29 - 5s - 160ms/step - loss: 574.5177 - mae: 18.9161 - mse: 573.1978 - qwk: 0.2103 - val_loss: 492.1705 - val_mae: 17.4469 - val_mse: 490.8503 - val_qwk: 0.2234\nEpoch 9/100\n29/29 - 5s - 161ms/step - loss: 479.6992 - mae: 17.0441 - mse: 478.3778 - qwk: 0.2826 - val_loss: 408.0377 - val_mae: 15.6170 - val_mse: 406.7155 - val_qwk: 0.3161\nEpoch 10/100\n29/29 - 5s - 159ms/step - loss: 413.7801 - mae: 15.7878 - mse: 412.4565 - qwk: 0.3344 - val_loss: 351.2400 - val_mae: 14.5107 - val_mse: 349.9160 - val_qwk: 0.3569\nEpoch 11/100\n29/29 - 5s - 164ms/step - loss: 369.4883 - mae: 14.9158 - mse: 368.1637 - qwk: 0.3829 - val_loss: 316.9033 - val_mae: 13.8412 - val_mse: 315.5782 - val_qwk: 0.4018\nEpoch 12/100\n29/29 - 5s - 160ms/step - loss: 343.4723 - mae: 14.4890 - mse: 342.1462 - qwk: 0.4061 - val_loss: 301.1745 - val_mae: 13.6080 - val_mse: 299.8479 - val_qwk: 0.4312\nEpoch 13/100\n29/29 - 5s - 158ms/step - loss: 324.5766 - mae: 14.1831 - mse: 323.2492 - qwk: 0.4367 - val_loss: 288.4722 - val_mae: 13.4421 - val_mse: 287.1448 - val_qwk: 0.4401\nEpoch 14/100\n29/29 - 5s - 157ms/step - loss: 314.1471 - mae: 14.0610 - mse: 312.8190 - qwk: 0.4476 - val_loss: 286.7497 - val_mae: 13.4156 - val_mse: 285.4210 - val_qwk: 0.4614\nEpoch 15/100\n29/29 - 5s - 166ms/step - loss: 310.2772 - mae: 13.9559 - mse: 308.9477 - qwk: 0.4593 - val_loss: 282.0511 - val_mae: 13.3473 - val_mse: 280.7212 - val_qwk: 0.4388\nEpoch 16/100\n29/29 - 5s - 163ms/step - loss: 314.6443 - mae: 14.0736 - mse: 313.3134 - qwk: 0.4542 - val_loss: 283.8572 - val_mae: 13.3813 - val_mse: 282.5258 - val_qwk: 0.4632\nEpoch 17/100\n29/29 - 5s - 160ms/step - loss: 315.2696 - mae: 14.0711 - mse: 313.9375 - qwk: 0.4441 - val_loss: 279.5175 - val_mae: 13.3023 - val_mse: 278.1851 - val_qwk: 0.4521\nEpoch 18/100\n29/29 - 5s - 157ms/step - loss: 312.5466 - mae: 14.1196 - mse: 311.2137 - qwk: 0.4538 - val_loss: 279.5380 - val_mae: 13.2824 - val_mse: 278.2050 - val_qwk: 0.4553\nEpoch 19/100\n29/29 - 5s - 157ms/step - loss: 302.0204 - mae: 13.8275 - mse: 300.6872 - qwk: 0.4774 - val_loss: 279.1082 - val_mae: 13.2972 - val_mse: 277.7748 - val_qwk: 0.4492\nEpoch 20/100\n29/29 - 5s - 157ms/step - loss: 312.6207 - mae: 14.1646 - mse: 311.2869 - qwk: 0.4582 - val_loss: 278.3115 - val_mae: 13.2902 - val_mse: 276.9774 - val_qwk: 0.4533\nEpoch 21/100\n29/29 - 5s - 161ms/step - loss: 303.0221 - mae: 13.8850 - mse: 301.6873 - qwk: 0.4792 - val_loss: 278.8908 - val_mae: 13.2695 - val_mse: 277.5557 - val_qwk: 0.4599\nEpoch 22/100\n29/29 - 5s - 160ms/step - loss: 303.5178 - mae: 13.7782 - mse: 302.1824 - qwk: 0.4772 - val_loss: 282.1919 - val_mae: 13.3858 - val_mse: 280.8565 - val_qwk: 0.4563\nEpoch 23/100\n29/29 - 5s - 157ms/step - loss: 300.7245 - mae: 13.8397 - mse: 299.3888 - qwk: 0.4819 - val_loss: 281.5109 - val_mae: 13.3979 - val_mse: 280.1750 - val_qwk: 0.4530\nEpoch 24/100\n29/29 - 5s - 157ms/step - loss: 293.5711 - mae: 13.6603 - mse: 292.2350 - qwk: 0.4965 - val_loss: 281.3392 - val_mae: 13.3832 - val_mse: 280.0030 - val_qwk: 0.4551\nEpoch 25/100\n29/29 - 5s - 158ms/step - loss: 306.1966 - mae: 13.9010 - mse: 304.8603 - qwk: 0.4839 - val_loss: 279.6558 - val_mae: 13.3481 - val_mse: 278.3195 - val_qwk: 0.4526\nEpoch 26/100\n29/29 - 5s - 159ms/step - loss: 301.8783 - mae: 13.8325 - mse: 300.5416 - qwk: 0.4812 - val_loss: 281.0607 - val_mae: 13.3609 - val_mse: 279.7240 - val_qwk: 0.4585\nEpoch 27/100\n29/29 - 5s - 159ms/step - loss: 302.3922 - mae: 13.9590 - mse: 301.0553 - qwk: 0.4682 - val_loss: 281.4917 - val_mae: 13.3858 - val_mse: 280.1548 - val_qwk: 0.4708\nEpoch 28/100\n29/29 - 5s - 160ms/step - loss: 291.4431 - mae: 13.4859 - mse: 290.1058 - qwk: 0.4965 - val_loss: 281.6236 - val_mae: 13.3996 - val_mse: 280.2863 - val_qwk: 0.4399\nEpoch 29/100\n29/29 - 5s - 164ms/step - loss: 292.2199 - mae: 13.6619 - mse: 290.8823 - qwk: 0.4977 - val_loss: 281.7483 - val_mae: 13.4186 - val_mse: 280.4105 - val_qwk: 0.4712\nEpoch 30/100\n29/29 - 5s - 159ms/step - loss: 297.8041 - mae: 13.6784 - mse: 296.4662 - qwk: 0.4854 - val_loss: 281.5285 - val_mae: 13.3739 - val_mse: 280.1909 - val_qwk: 0.4562\n29/29 - 1s - 28ms/step - loss: 278.3115 - mae: 13.2902 - mse: 276.9774 - qwk: 0.4477\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/100\n29/29 - 5s - 162ms/step - loss: 1111.8162 - mae: 27.3839 - mse: 1110.5288 - qwk: -2.6798e-17 - val_loss: 1360.2101 - val_mae: 32.4591 - val_mse: 1358.9200 - val_qwk: 0.0301\nEpoch 2/100\n29/29 - 5s - 162ms/step - loss: 1035.7301 - mae: 26.9415 - mse: 1034.4366 - qwk: 0.0144 - val_loss: 1242.3643 - val_mae: 30.8424 - val_mse: 1241.0673 - val_qwk: 0.0568\nEpoch 3/100\n29/29 - 5s - 160ms/step - loss: 969.4570 - mae: 26.1649 - mse: 968.1583 - qwk: 0.0465 - val_loss: 1167.9365 - val_mae: 29.6373 - val_mse: 1166.6364 - val_qwk: 0.1061\nEpoch 4/100\n29/29 - 5s - 164ms/step - loss: 897.5557 - mae: 25.1523 - mse: 896.2554 - qwk: 0.0581 - val_loss: 998.4490 - val_mae: 27.0007 - val_mse: 997.1494 - val_qwk: 0.0992\nEpoch 5/100\n29/29 - 5s - 160ms/step - loss: 813.1877 - mae: 23.7017 - mse: 811.8890 - qwk: 0.0707 - val_loss: 879.2838 - val_mae: 25.0338 - val_mse: 877.9862 - val_qwk: 0.1160\nEpoch 6/100\n29/29 - 5s - 164ms/step - loss: 716.8769 - mae: 21.9318 - mse: 715.5802 - qwk: 0.1135 - val_loss: 745.7190 - val_mae: 22.6510 - val_mse: 744.4233 - val_qwk: 0.1431\nEpoch 7/100\n29/29 - 5s - 159ms/step - loss: 625.9091 - mae: 20.0524 - mse: 624.6136 - qwk: 0.1639 - val_loss: 611.7632 - val_mae: 20.0040 - val_mse: 610.4674 - val_qwk: 0.1801\nEpoch 8/100\n29/29 - 5s - 161ms/step - loss: 525.4780 - mae: 18.0255 - mse: 524.1822 - qwk: 0.2057 - val_loss: 520.0220 - val_mae: 18.1061 - val_mse: 518.7262 - val_qwk: 0.2418\nEpoch 9/100\n29/29 - 5s - 163ms/step - loss: 450.3354 - mae: 16.4673 - mse: 449.0394 - qwk: 0.2646 - val_loss: 440.0330 - val_mae: 16.4124 - val_mse: 438.7361 - val_qwk: 0.3284\nEpoch 10/100\n29/29 - 5s - 164ms/step - loss: 387.3395 - mae: 15.2324 - mse: 386.0423 - qwk: 0.3454 - val_loss: 385.6707 - val_mae: 15.3247 - val_mse: 384.3732 - val_qwk: 0.3891\nEpoch 11/100\n29/29 - 5s - 163ms/step - loss: 338.2181 - mae: 14.2182 - mse: 336.9200 - qwk: 0.3974 - val_loss: 353.8337 - val_mae: 14.7727 - val_mse: 352.5350 - val_qwk: 0.4289\nEpoch 12/100\n29/29 - 5s - 168ms/step - loss: 319.4363 - mae: 13.9294 - mse: 318.1379 - qwk: 0.4074 - val_loss: 335.4723 - val_mae: 14.4951 - val_mse: 334.1740 - val_qwk: 0.4427\nEpoch 13/100\n29/29 - 5s - 160ms/step - loss: 303.2313 - mae: 13.6956 - mse: 301.9328 - qwk: 0.4346 - val_loss: 324.9188 - val_mae: 14.3355 - val_mse: 323.6201 - val_qwk: 0.4486\nEpoch 14/100\n29/29 - 5s - 164ms/step - loss: 297.9814 - mae: 13.5110 - mse: 296.6826 - qwk: 0.4456 - val_loss: 321.4003 - val_mae: 14.3087 - val_mse: 320.1012 - val_qwk: 0.4410\nEpoch 15/100\n29/29 - 5s - 164ms/step - loss: 298.5508 - mae: 13.6895 - mse: 297.2516 - qwk: 0.4403 - val_loss: 318.4482 - val_mae: 14.2521 - val_mse: 317.1486 - val_qwk: 0.4568\nEpoch 16/100\n29/29 - 5s - 164ms/step - loss: 289.3572 - mae: 13.5563 - mse: 288.0578 - qwk: 0.4559 - val_loss: 317.8393 - val_mae: 14.3102 - val_mse: 316.5401 - val_qwk: 0.4362\nEpoch 17/100\n29/29 - 5s - 159ms/step - loss: 287.9084 - mae: 13.4315 - mse: 286.6091 - qwk: 0.4638 - val_loss: 319.1830 - val_mae: 14.3210 - val_mse: 317.8836 - val_qwk: 0.4506\nEpoch 18/100\n29/29 - 5s - 159ms/step - loss: 289.7066 - mae: 13.5052 - mse: 288.4073 - qwk: 0.4586 - val_loss: 320.2914 - val_mae: 14.3343 - val_mse: 318.9919 - val_qwk: 0.4622\nEpoch 19/100\n29/29 - 5s - 163ms/step - loss: 281.9149 - mae: 13.3479 - mse: 280.6153 - qwk: 0.4688 - val_loss: 317.9778 - val_mae: 14.2757 - val_mse: 316.6781 - val_qwk: 0.4516\nEpoch 20/100\n29/29 - 5s - 160ms/step - loss: 280.3936 - mae: 13.2915 - mse: 279.0939 - qwk: 0.4713 - val_loss: 321.2451 - val_mae: 14.3450 - val_mse: 319.9452 - val_qwk: 0.4600\nEpoch 21/100\n29/29 - 5s - 161ms/step - loss: 285.9087 - mae: 13.3556 - mse: 284.6090 - qwk: 0.4611 - val_loss: 323.3817 - val_mae: 14.3685 - val_mse: 322.0819 - val_qwk: 0.4536\nEpoch 22/100\n29/29 - 5s - 160ms/step - loss: 283.2285 - mae: 13.3487 - mse: 281.9290 - qwk: 0.4738 - val_loss: 318.7168 - val_mae: 14.2677 - val_mse: 317.4178 - val_qwk: 0.4451\nEpoch 23/100\n29/29 - 5s - 162ms/step - loss: 283.3298 - mae: 13.3722 - mse: 282.0312 - qwk: 0.4712 - val_loss: 322.5591 - val_mae: 14.3367 - val_mse: 321.2607 - val_qwk: 0.4484\nEpoch 24/100\n29/29 - 5s - 163ms/step - loss: 280.6597 - mae: 13.3228 - mse: 279.3615 - qwk: 0.4683 - val_loss: 321.2632 - val_mae: 14.3234 - val_mse: 319.9653 - val_qwk: 0.4514\nEpoch 25/100\n29/29 - 5s - 161ms/step - loss: 277.3700 - mae: 13.2657 - mse: 276.0722 - qwk: 0.4883 - val_loss: 321.2595 - val_mae: 14.3199 - val_mse: 319.9620 - val_qwk: 0.4621\nEpoch 26/100\n29/29 - 5s - 163ms/step - loss: 268.3603 - mae: 13.0181 - mse: 267.0630 - qwk: 0.4991 - val_loss: 322.0539 - val_mae: 14.3137 - val_mse: 320.7570 - val_qwk: 0.4587\n29/29 - 1s - 29ms/step - loss: 317.8392 - mae: 14.3102 - mse: 316.5401 - qwk: 0.4269\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-c89fd6e20557>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Calculate the average validation loss across all folds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mavg_val_qwk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_qwk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Average Validation QWK: {avg_val_qwk}, Average Validation Loss: {avg_val_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'avg_val_loss' is not defined"],"ename":"NameError","evalue":"name 'avg_val_loss' is not defined","output_type":"error"}],"execution_count":37},{"cell_type":"code","source":"# Graph most recent model history\n'''\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nval_loss, val_mae, val_mse, val_qwk = model.evaluate(X_train_v, y_train_v)\nprint(f\"Validation loss: {val_loss:.4f}, Validation MAE: {val_mae:.4f}\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T07:07:06.031996Z","iopub.execute_input":"2025-01-16T07:07:06.032389Z","iopub.status.idle":"2025-01-16T07:07:07.287909Z","shell.execute_reply.started":"2025-01-16T07:07:06.032362Z","shell.execute_reply":"2025-01-16T07:07:07.286576Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 324.8459 - mae: 14.3970 - mse: 323.5468 - qwk: 0.4129\nValidation loss: 317.8392, Validation MAE: 14.3102\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"'''\nplt.plot(history.history['qwk'])\nplt.plot(history.history['val_qwk'])\nplt.title('Model QWK')\nplt.xlabel('Epoch')\nplt.ylabel('QWK')\nplt.legend(['qwk','val_qwk'])\nplt.show()\n\nval_loss, val_mae, val_mse, val_qwk = model.evaluate(X_train_v, y_train_v)\nprint(f\"Validation QWK: {val_qwk:.4f}\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T07:07:16.538200Z","iopub.execute_input":"2025-01-16T07:07:16.538668Z","iopub.status.idle":"2025-01-16T07:07:17.728787Z","shell.execute_reply.started":"2025-01-16T07:07:16.538610Z","shell.execute_reply":"2025-01-16T07:07:17.727490Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 324.8459 - mae: 14.3970 - mse: 323.5468 - qwk: 0.4129\nValidation QWK: 0.4269\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"#model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T07:07:27.174817Z","iopub.execute_input":"2025-01-16T07:07:27.175191Z","iopub.status.idle":"2025-01-16T07:07:27.212797Z","shell.execute_reply.started":"2025-01-16T07:07:27.175166Z","shell.execute_reply":"2025-01-16T07:07:27.211739Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_3\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │           \u001b[38;5;34m4,480\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_12               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_12 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_13               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_13 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_14               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_14 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m1,056\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_15               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_15 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,480</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_12               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_13               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_14               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_15               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ activation_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m49,765\u001b[0m (194.40 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,765</span> (194.40 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,417\u001b[0m (64.13 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,417</span> (64.13 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m32,836\u001b[0m (128.27 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,836</span> (128.27 KB)\n</pre>\n"},"metadata":{}}],"execution_count":40},{"cell_type":"markdown","source":"We have way too many parameters considering we only have 2700 rows of data.\n\nIf rule of thumb is we need 10x as much data as params...\n\nWe can try a much smaller DNN, as well as augment the data. Or do PCA to have fewer neurons in the input layer.\n\nI am willing to augment the data once or twice (duplicating with noise). More than \nthat and I think the risk of overfitting to the training data is too high, even with \nthe added noise in the augmentation.\n\nIf we augment once, there are ~5400 rows. If we augment twice, there are ~8100 rows.\n\nSo we are actually looking for a DNN with only 500-900 trainable params?","metadata":{}},{"cell_type":"markdown","source":"# Augment data","metadata":{}},{"cell_type":"code","source":"# Get standard deviations of each column in X and y\nstd_X = np.std(X_train_labelled, axis=0)\nstd_y = np.std(y_train_labelled, axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:32:56.620521Z","iopub.execute_input":"2025-01-17T07:32:56.621013Z","iopub.status.idle":"2025-01-17T07:32:56.627456Z","shell.execute_reply.started":"2025-01-17T07:32:56.620973Z","shell.execute_reply":"2025-01-17T07:32:56.626267Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"X_train_labelled.shape, y_train_labelled.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:32:58.494082Z","iopub.execute_input":"2025-01-17T07:32:58.494456Z","iopub.status.idle":"2025-01-17T07:32:58.500788Z","shell.execute_reply.started":"2025-01-17T07:32:58.494425Z","shell.execute_reply":"2025-01-17T07:32:58.499619Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"((2736, 34), (2736,))"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# Create augmented datasets\n\nX_noise_multiplier=0.1\ny_noise_multiplier=0.1\n\n# Not doing this in a for loop because we will not augment more than twice\n# And we may choose to add different noisiness to each augmentation\nX_train_labelled_noisy = X_train_labelled + (X_noise_multiplier * np.random.normal(0, std_X, X_train_labelled.shape))\ny_train_labelled_noisy = y_train_labelled + (y_noise_multiplier * np.random.normal(0, std_y, y_train_labelled.shape))\nX_train_labelled_noisy.shape, y_train_labelled_noisy.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:33:01.421591Z","iopub.execute_input":"2025-01-17T07:33:01.421998Z","iopub.status.idle":"2025-01-17T07:33:01.447284Z","shell.execute_reply.started":"2025-01-17T07:33:01.421959Z","shell.execute_reply":"2025-01-17T07:33:01.446082Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"((2736, 34), (2736,))"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"X_train_labelled_aug1 = pd.concat([X_train_labelled,X_train_labelled_noisy], ignore_index=True)\ny_train_labelled_aug1 = pd.concat([y_train_labelled,y_train_labelled_noisy], ignore_index=True)\nX_train_labelled_aug1.shape, y_train_labelled_aug1.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:33:02.791359Z","iopub.execute_input":"2025-01-17T07:33:02.791803Z","iopub.status.idle":"2025-01-17T07:33:02.801251Z","shell.execute_reply.started":"2025-01-17T07:33:02.791766Z","shell.execute_reply":"2025-01-17T07:33:02.800083Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"((5472, 34), (5472,))"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"# V3: Smaller model and data augmented once","metadata":{}},{"cell_type":"code","source":"def create_model_v3():\n    model = keras.models.Sequential([\n        keras.layers.Dense(20, input_shape=(X_train_labelled.shape[1],), activation=\"selu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(20, activation=\"selu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(1, activation=\"linear\")\n    ])\n\n    model.compile(optimizer=Adam(learning_rate=0.005), loss='mean_squared_error', metrics=['mae','mse',qwk],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T07:41:25.077122Z","iopub.execute_input":"2025-01-16T07:41:25.077707Z","iopub.status.idle":"2025-01-16T07:41:25.085651Z","shell.execute_reply.started":"2025-01-16T07:41:25.077658Z","shell.execute_reply":"2025-01-16T07:41:25.084260Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"model = create_model_v3()\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T07:41:26.963320Z","iopub.execute_input":"2025-01-16T07:41:26.963685Z","iopub.status.idle":"2025-01-16T07:41:27.022991Z","shell.execute_reply.started":"2025-01-16T07:41:26.963656Z","shell.execute_reply":"2025-01-16T07:41:27.021806Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_14\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_14\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ dense_45 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │             \u001b[38;5;34m700\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_31 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_46 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │             \u001b[38;5;34m420\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_32 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_47 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m21\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ dense_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">420</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,141\u001b[0m (4.46 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,141</span> (4.46 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,141\u001b[0m (4.46 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,141</span> (4.46 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"'''\n# Prepare to store results\nvalidation_qwk = []\nvalidation_losses = []\nmodels = []\n\n# Loop over each fold\nfor train_index, val_index in kf.split(X_train_labelled_aug1):\n    # Split the data into training and validation sets for the current fold\n    X_train_t, X_train_v = X_train_labelled_aug1.loc[train_index], X_train_labelled_aug1.loc[val_index]\n    y_train_t, y_train_v = y_train_labelled_aug1.loc[train_index], y_train_labelled_aug1.loc[val_index]\n    \n    # Build a new model for each fold\n    model = create_model_v3()\n    \n    # Define early stopping to avoid overfitting\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n    \n    # Train the model on the training set and evaluate on the validation set\n    history = model.fit(\n        X_train_t, y_train_t,\n        validation_data=(X_train_v, y_train_v),\n        epochs=100,\n        batch_size=64,\n        callbacks=[early_stopping],\n        verbose=2\n    )\n    \n    # Evaluate the model on the validation set\n    val_loss, val_mae, val_mse, val_qwk = model.evaluate(X_train_v, y_train_v, verbose=2)\n    #val_qwk = model.evaluate(X_train_v, y_train_v, verbose=2)\n    validation_qwk.append(val_qwk)\n    validation_losses.append(val_loss)\n    models.append(model)\n\n# Calculate the average validation loss across all folds\navg_val_qwk = np.mean(validation_qwk)\navg_val_loss = np.mean(validation_losses)\nprint(f\"Average Validation QWK: {avg_val_qwk}, Average Validation Loss: {avg_val_loss}\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T07:41:55.337769Z","iopub.execute_input":"2025-01-16T07:41:55.338243Z","iopub.status.idle":"2025-01-16T07:56:26.232265Z","shell.execute_reply.started":"2025-01-16T07:41:55.338193Z","shell.execute_reply":"2025-01-16T07:56:26.231139Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"57/57 - 4s - 65ms/step - loss: 898.3621 - mae: 23.8596 - mse: 898.3621 - qwk: 0.0740 - val_loss: 526.4337 - val_mae: 18.1952 - val_mse: 526.4337 - val_qwk: 0.2203\nEpoch 2/100\n57/57 - 4s - 65ms/step - loss: 477.4351 - mae: 17.2207 - mse: 477.4351 - qwk: 0.2459 - val_loss: 346.6423 - val_mae: 14.9520 - val_mse: 346.6423 - val_qwk: 0.3752\nEpoch 3/100\n57/57 - 4s - 65ms/step - loss: 385.7579 - mae: 15.5516 - mse: 385.7579 - qwk: 0.3329 - val_loss: 319.2467 - val_mae: 14.3667 - val_mse: 319.2467 - val_qwk: 0.3778\nEpoch 4/100\n57/57 - 4s - 63ms/step - loss: 350.6621 - mae: 14.8094 - mse: 350.6621 - qwk: 0.3676 - val_loss: 312.0309 - val_mae: 14.2030 - val_mse: 312.0309 - val_qwk: 0.3960\nEpoch 5/100\n57/57 - 4s - 63ms/step - loss: 333.0366 - mae: 14.4139 - mse: 333.0366 - qwk: 0.3903 - val_loss: 307.3542 - val_mae: 14.1465 - val_mse: 307.3542 - val_qwk: 0.4017\nEpoch 6/100\n57/57 - 4s - 64ms/step - loss: 331.7460 - mae: 14.3608 - mse: 331.7460 - qwk: 0.3837 - val_loss: 310.3259 - val_mae: 14.1446 - val_mse: 310.3259 - val_qwk: 0.3817\nEpoch 7/100\n57/57 - 4s - 63ms/step - loss: 325.7187 - mae: 14.2945 - mse: 325.7187 - qwk: 0.3922 - val_loss: 309.4647 - val_mae: 14.1473 - val_mse: 309.4647 - val_qwk: 0.3818\nEpoch 8/100\n57/57 - 4s - 64ms/step - loss: 318.5798 - mae: 14.1734 - mse: 318.5798 - qwk: 0.4024 - val_loss: 305.1906 - val_mae: 14.0794 - val_mse: 305.1906 - val_qwk: 0.3978\nEpoch 9/100\n57/57 - 4s - 65ms/step - loss: 313.9019 - mae: 14.1271 - mse: 313.9019 - qwk: 0.4111 - val_loss: 304.2690 - val_mae: 14.0581 - val_mse: 304.2690 - val_qwk: 0.4033\nEpoch 10/100\n57/57 - 4s - 63ms/step - loss: 315.3240 - mae: 14.1741 - mse: 315.3240 - qwk: 0.4067 - val_loss: 303.2416 - val_mae: 14.0633 - val_mse: 303.2416 - val_qwk: 0.4013\nEpoch 11/100\n57/57 - 4s - 64ms/step - loss: 312.3747 - mae: 14.0824 - mse: 312.3747 - qwk: 0.4103 - val_loss: 303.6149 - val_mae: 14.0719 - val_mse: 303.6149 - val_qwk: 0.3992\nEpoch 12/100\n57/57 - 4s - 65ms/step - loss: 315.2824 - mae: 14.0893 - mse: 315.2824 - qwk: 0.4028 - val_loss: 302.7462 - val_mae: 14.0386 - val_mse: 302.7462 - val_qwk: 0.4034\nEpoch 13/100\n57/57 - 4s - 63ms/step - loss: 306.4908 - mae: 13.9391 - mse: 306.4908 - qwk: 0.4180 - val_loss: 300.4532 - val_mae: 14.0083 - val_mse: 300.4532 - val_qwk: 0.4075\nEpoch 14/100\n57/57 - 4s - 64ms/step - loss: 306.3204 - mae: 13.8699 - mse: 306.3204 - qwk: 0.4153 - val_loss: 300.4941 - val_mae: 14.0062 - val_mse: 300.4941 - val_qwk: 0.4083\nEpoch 15/100\n57/57 - 4s - 64ms/step - loss: 306.0295 - mae: 13.8404 - mse: 306.0295 - qwk: 0.4250 - val_loss: 299.6231 - val_mae: 13.9965 - val_mse: 299.6231 - val_qwk: 0.4050\nEpoch 16/100\n57/57 - 4s - 63ms/step - loss: 310.0069 - mae: 13.9357 - mse: 310.0069 - qwk: 0.4132 - val_loss: 300.8474 - val_mae: 13.9752 - val_mse: 300.8474 - val_qwk: 0.4131\nEpoch 17/100\n57/57 - 4s - 64ms/step - loss: 309.6854 - mae: 13.9367 - mse: 309.6854 - qwk: 0.4119 - val_loss: 300.8817 - val_mae: 13.9928 - val_mse: 300.8817 - val_qwk: 0.4014\nEpoch 18/100\n57/57 - 4s - 64ms/step - loss: 306.4080 - mae: 13.7532 - mse: 306.4080 - qwk: 0.4155 - val_loss: 299.2993 - val_mae: 13.9604 - val_mse: 299.2993 - val_qwk: 0.4120\nEpoch 19/100\n57/57 - 4s - 65ms/step - loss: 303.8918 - mae: 13.8449 - mse: 303.8918 - qwk: 0.4187 - val_loss: 300.6488 - val_mae: 13.9920 - val_mse: 300.6488 - val_qwk: 0.4020\nEpoch 20/100\n57/57 - 4s - 65ms/step - loss: 303.6841 - mae: 13.7960 - mse: 303.6841 - qwk: 0.4264 - val_loss: 298.8440 - val_mae: 13.9444 - val_mse: 298.8440 - val_qwk: 0.4235\nEpoch 21/100\n57/57 - 4s - 63ms/step - loss: 298.9649 - mae: 13.7282 - mse: 298.9649 - qwk: 0.4371 - val_loss: 297.0149 - val_mae: 13.9213 - val_mse: 297.0149 - val_qwk: 0.4226\nEpoch 22/100\n57/57 - 4s - 64ms/step - loss: 301.2770 - mae: 13.7346 - mse: 301.2770 - qwk: 0.4315 - val_loss: 298.3444 - val_mae: 13.9604 - val_mse: 298.3444 - val_qwk: 0.4171\nEpoch 23/100\n57/57 - 4s - 63ms/step - loss: 300.3948 - mae: 13.6489 - mse: 300.3948 - qwk: 0.4382 - val_loss: 298.5052 - val_mae: 13.9571 - val_mse: 298.5052 - val_qwk: 0.4138\nEpoch 24/100\n57/57 - 4s - 63ms/step - loss: 299.9691 - mae: 13.6708 - mse: 299.9691 - qwk: 0.4400 - val_loss: 300.7871 - val_mae: 13.9914 - val_mse: 300.7871 - val_qwk: 0.4125\nEpoch 25/100\n57/57 - 4s - 63ms/step - loss: 295.7216 - mae: 13.6055 - mse: 295.7216 - qwk: 0.4574 - val_loss: 299.9138 - val_mae: 13.9889 - val_mse: 299.9138 - val_qwk: 0.4179\nEpoch 26/100\n57/57 - 4s - 63ms/step - loss: 298.1149 - mae: 13.6532 - mse: 298.1149 - qwk: 0.4386 - val_loss: 297.8510 - val_mae: 13.9495 - val_mse: 297.8510 - val_qwk: 0.4189\nEpoch 27/100\n57/57 - 4s - 63ms/step - loss: 295.9504 - mae: 13.6220 - mse: 295.9504 - qwk: 0.4440 - val_loss: 297.7669 - val_mae: 13.9523 - val_mse: 297.7669 - val_qwk: 0.4303\nEpoch 28/100\n57/57 - 4s - 64ms/step - loss: 298.4948 - mae: 13.7506 - mse: 298.4948 - qwk: 0.4436 - val_loss: 297.5066 - val_mae: 13.9593 - val_mse: 297.5066 - val_qwk: 0.4282\nEpoch 29/100\n57/57 - 4s - 64ms/step - loss: 292.9763 - mae: 13.6303 - mse: 292.9763 - qwk: 0.4480 - val_loss: 296.7924 - val_mae: 13.9350 - val_mse: 296.7924 - val_qwk: 0.4352\nEpoch 30/100\n57/57 - 4s - 63ms/step - loss: 297.8377 - mae: 13.7022 - mse: 297.8377 - qwk: 0.4277 - val_loss: 296.5779 - val_mae: 13.9241 - val_mse: 296.5779 - val_qwk: 0.4341\nEpoch 31/100\n57/57 - 4s - 63ms/step - loss: 301.9343 - mae: 13.8064 - mse: 301.9343 - qwk: 0.4300 - val_loss: 295.2345 - val_mae: 13.8948 - val_mse: 295.2345 - val_qwk: 0.4326\nEpoch 32/100\n57/57 - 4s - 64ms/step - loss: 294.1733 - mae: 13.5015 - mse: 294.1733 - qwk: 0.4464 - val_loss: 296.8438 - val_mae: 13.9169 - val_mse: 296.8438 - val_qwk: 0.4231\nEpoch 33/100\n57/57 - 4s - 64ms/step - loss: 291.3381 - mae: 13.4944 - mse: 291.3381 - qwk: 0.4567 - val_loss: 296.5177 - val_mae: 13.9189 - val_mse: 296.5177 - val_qwk: 0.4222\nEpoch 34/100\n57/57 - 4s - 63ms/step - loss: 293.8431 - mae: 13.5686 - mse: 293.8431 - qwk: 0.4506 - val_loss: 298.3705 - val_mae: 13.9338 - val_mse: 298.3705 - val_qwk: 0.4213\nEpoch 35/100\n57/57 - 4s - 65ms/step - loss: 296.3128 - mae: 13.6220 - mse: 296.3128 - qwk: 0.4462 - val_loss: 297.0027 - val_mae: 13.9176 - val_mse: 297.0027 - val_qwk: 0.4253\nEpoch 36/100\n57/57 - 4s - 64ms/step - loss: 287.2862 - mae: 13.5118 - mse: 287.2862 - qwk: 0.4659 - val_loss: 295.3065 - val_mae: 13.8803 - val_mse: 295.3065 - val_qwk: 0.4377\nEpoch 37/100\n57/57 - 4s - 63ms/step - loss: 289.1896 - mae: 13.4328 - mse: 289.1896 - qwk: 0.4626 - val_loss: 299.8174 - val_mae: 13.9557 - val_mse: 299.8174 - val_qwk: 0.4170\nEpoch 38/100\n57/57 - 4s - 65ms/step - loss: 288.4250 - mae: 13.4069 - mse: 288.4250 - qwk: 0.4632 - val_loss: 296.9681 - val_mae: 13.9322 - val_mse: 296.9681 - val_qwk: 0.4320\nEpoch 39/100\n57/57 - 4s - 64ms/step - loss: 293.1389 - mae: 13.4989 - mse: 293.1389 - qwk: 0.4508 - val_loss: 295.4182 - val_mae: 13.8763 - val_mse: 295.4182 - val_qwk: 0.4389\nEpoch 40/100\n57/57 - 4s - 64ms/step - loss: 292.0790 - mae: 13.5943 - mse: 292.0790 - qwk: 0.4518 - val_loss: 296.4119 - val_mae: 13.9351 - val_mse: 296.4119 - val_qwk: 0.4226\nEpoch 41/100\n57/57 - 4s - 64ms/step - loss: 291.1366 - mae: 13.4522 - mse: 291.1366 - qwk: 0.4575 - val_loss: 294.6995 - val_mae: 13.8841 - val_mse: 294.6995 - val_qwk: 0.4481\nEpoch 42/100\n57/57 - 4s - 64ms/step - loss: 287.2969 - mae: 13.4282 - mse: 287.2969 - qwk: 0.4673 - val_loss: 296.4531 - val_mae: 13.9359 - val_mse: 296.4531 - val_qwk: 0.4293\nEpoch 43/100\n57/57 - 4s - 63ms/step - loss: 290.8780 - mae: 13.5121 - mse: 290.8780 - qwk: 0.4614 - val_loss: 295.4659 - val_mae: 13.8907 - val_mse: 295.4659 - val_qwk: 0.4395\nEpoch 44/100\n57/57 - 4s - 64ms/step - loss: 289.0263 - mae: 13.4368 - mse: 289.0263 - qwk: 0.4695 - val_loss: 296.9358 - val_mae: 13.8859 - val_mse: 296.9358 - val_qwk: 0.4384\nEpoch 45/100\n57/57 - 4s - 63ms/step - loss: 292.4872 - mae: 13.5439 - mse: 292.4872 - qwk: 0.4526 - val_loss: 294.8470 - val_mae: 13.8317 - val_mse: 294.8470 - val_qwk: 0.4560\nEpoch 46/100\n57/57 - 4s - 63ms/step - loss: 285.1108 - mae: 13.3568 - mse: 285.1108 - qwk: 0.4738 - val_loss: 294.8059 - val_mae: 13.8795 - val_mse: 294.8059 - val_qwk: 0.4508\nEpoch 47/100\n57/57 - 4s - 66ms/step - loss: 286.3036 - mae: 13.3588 - mse: 286.3036 - qwk: 0.4709 - val_loss: 296.7048 - val_mae: 13.9506 - val_mse: 296.7048 - val_qwk: 0.4450\nEpoch 48/100\n57/57 - 4s - 64ms/step - loss: 283.7130 - mae: 13.3869 - mse: 283.7130 - qwk: 0.4787 - val_loss: 296.1313 - val_mae: 13.9158 - val_mse: 296.1313 - val_qwk: 0.4392\nEpoch 49/100\n57/57 - 4s - 64ms/step - loss: 285.5154 - mae: 13.3737 - mse: 285.5154 - qwk: 0.4765 - val_loss: 297.0621 - val_mae: 13.9156 - val_mse: 297.0621 - val_qwk: 0.4318\nEpoch 50/100\n57/57 - 4s - 63ms/step - loss: 292.8796 - mae: 13.5289 - mse: 292.8796 - qwk: 0.4560 - val_loss: 296.9211 - val_mae: 13.8672 - val_mse: 296.9211 - val_qwk: 0.4281\nEpoch 51/100\n57/57 - 4s - 64ms/step - loss: 284.8437 - mae: 13.4044 - mse: 284.8437 - qwk: 0.4657 - val_loss: 294.6197 - val_mae: 13.8535 - val_mse: 294.6197 - val_qwk: 0.4424\nEpoch 52/100\n57/57 - 4s - 65ms/step - loss: 285.3459 - mae: 13.3387 - mse: 285.3459 - qwk: 0.4777 - val_loss: 297.5770 - val_mae: 13.9032 - val_mse: 297.5770 - val_qwk: 0.4331\nEpoch 53/100\n57/57 - 4s - 63ms/step - loss: 284.2549 - mae: 13.3550 - mse: 284.2549 - qwk: 0.4727 - val_loss: 296.0250 - val_mae: 13.8574 - val_mse: 296.0250 - val_qwk: 0.4434\nEpoch 54/100\n57/57 - 4s - 63ms/step - loss: 284.1751 - mae: 13.3133 - mse: 284.1751 - qwk: 0.4746 - val_loss: 295.6005 - val_mae: 13.8447 - val_mse: 295.6005 - val_qwk: 0.4346\nEpoch 55/100\n57/57 - 4s - 64ms/step - loss: 281.2440 - mae: 13.2515 - mse: 281.2440 - qwk: 0.4732 - val_loss: 294.0416 - val_mae: 13.8478 - val_mse: 294.0416 - val_qwk: 0.4365\nEpoch 56/100\n57/57 - 4s - 63ms/step - loss: 277.9695 - mae: 13.1702 - mse: 277.9695 - qwk: 0.4944 - val_loss: 293.2077 - val_mae: 13.8028 - val_mse: 293.2077 - val_qwk: 0.4464\nEpoch 57/100\n57/57 - 4s - 63ms/step - loss: 277.6357 - mae: 13.1202 - mse: 277.6357 - qwk: 0.4933 - val_loss: 295.3703 - val_mae: 13.8511 - val_mse: 295.3703 - val_qwk: 0.4438\nEpoch 58/100\n57/57 - 4s - 63ms/step - loss: 282.3375 - mae: 13.3123 - mse: 282.3375 - qwk: 0.4693 - val_loss: 294.1021 - val_mae: 13.8637 - val_mse: 294.1021 - val_qwk: 0.4403\nEpoch 59/100\n57/57 - 4s - 63ms/step - loss: 281.7322 - mae: 13.2651 - mse: 281.7322 - qwk: 0.4737 - val_loss: 293.2930 - val_mae: 13.8070 - val_mse: 293.2930 - val_qwk: 0.4488\nEpoch 60/100\n57/57 - 4s - 63ms/step - loss: 285.6350 - mae: 13.3032 - mse: 285.6350 - qwk: 0.4693 - val_loss: 295.2696 - val_mae: 13.8679 - val_mse: 295.2696 - val_qwk: 0.4391\nEpoch 61/100\n57/57 - 4s - 65ms/step - loss: 280.6582 - mae: 13.2530 - mse: 280.6582 - qwk: 0.4797 - val_loss: 292.4960 - val_mae: 13.8395 - val_mse: 292.4960 - val_qwk: 0.4505\nEpoch 62/100\n57/57 - 4s - 64ms/step - loss: 281.9350 - mae: 13.2909 - mse: 281.9350 - qwk: 0.4741 - val_loss: 294.4753 - val_mae: 13.8433 - val_mse: 294.4753 - val_qwk: 0.4503\nEpoch 63/100\n57/57 - 4s - 65ms/step - loss: 279.6370 - mae: 13.2463 - mse: 279.6370 - qwk: 0.4875 - val_loss: 296.1781 - val_mae: 13.8713 - val_mse: 296.1781 - val_qwk: 0.4483\nEpoch 64/100\n57/57 - 4s - 66ms/step - loss: 282.5658 - mae: 13.2708 - mse: 282.5658 - qwk: 0.4847 - val_loss: 296.8028 - val_mae: 13.9136 - val_mse: 296.8028 - val_qwk: 0.4365\nEpoch 65/100\n57/57 - 4s - 65ms/step - loss: 277.1876 - mae: 13.1726 - mse: 277.1876 - qwk: 0.4857 - val_loss: 294.0167 - val_mae: 13.8154 - val_mse: 294.0167 - val_qwk: 0.4479\nEpoch 66/100\n57/57 - 4s - 65ms/step - loss: 277.0877 - mae: 13.2237 - mse: 277.0877 - qwk: 0.4901 - val_loss: 294.0483 - val_mae: 13.8328 - val_mse: 294.0483 - val_qwk: 0.4306\nEpoch 67/100\n57/57 - 4s - 64ms/step - loss: 278.3141 - mae: 13.2258 - mse: 278.3141 - qwk: 0.4883 - val_loss: 294.7324 - val_mae: 13.8472 - val_mse: 294.7324 - val_qwk: 0.4424\nEpoch 68/100\n57/57 - 4s - 66ms/step - loss: 278.2627 - mae: 13.0803 - mse: 278.2627 - qwk: 0.4984 - val_loss: 294.6121 - val_mae: 13.8298 - val_mse: 294.6121 - val_qwk: 0.4369\nEpoch 69/100\n57/57 - 4s - 65ms/step - loss: 277.5443 - mae: 13.1858 - mse: 277.5443 - qwk: 0.4836 - val_loss: 291.8466 - val_mae: 13.7702 - val_mse: 291.8466 - val_qwk: 0.4551\nEpoch 70/100\n57/57 - 4s - 64ms/step - loss: 285.2041 - mae: 13.3585 - mse: 285.2041 - qwk: 0.4710 - val_loss: 295.3078 - val_mae: 13.8654 - val_mse: 295.3078 - val_qwk: 0.4326\nEpoch 71/100\n57/57 - 4s - 65ms/step - loss: 276.3584 - mae: 13.1518 - mse: 276.3584 - qwk: 0.4940 - val_loss: 292.2559 - val_mae: 13.7932 - val_mse: 292.2559 - val_qwk: 0.4499\nEpoch 72/100\n57/57 - 4s - 68ms/step - loss: 276.0683 - mae: 13.2223 - mse: 276.0683 - qwk: 0.4820 - val_loss: 293.9699 - val_mae: 13.7994 - val_mse: 293.9699 - val_qwk: 0.4430\nEpoch 73/100\n57/57 - 4s - 67ms/step - loss: 271.5344 - mae: 12.9960 - mse: 271.5344 - qwk: 0.5044 - val_loss: 292.4795 - val_mae: 13.7983 - val_mse: 292.4795 - val_qwk: 0.4513\nEpoch 74/100\n57/57 - 4s - 66ms/step - loss: 280.9867 - mae: 13.2286 - mse: 280.9867 - qwk: 0.4826 - val_loss: 294.9492 - val_mae: 13.8629 - val_mse: 294.9492 - val_qwk: 0.4371\nEpoch 75/100\n57/57 - 4s - 64ms/step - loss: 275.0648 - mae: 13.0554 - mse: 275.0648 - qwk: 0.4951 - val_loss: 292.6676 - val_mae: 13.8006 - val_mse: 292.6676 - val_qwk: 0.4402\nEpoch 76/100\n57/57 - 4s - 65ms/step - loss: 275.6618 - mae: 13.1575 - mse: 275.6618 - qwk: 0.4985 - val_loss: 294.2090 - val_mae: 13.8245 - val_mse: 294.2090 - val_qwk: 0.4395\nEpoch 77/100\n57/57 - 4s - 65ms/step - loss: 276.7245 - mae: 13.0951 - mse: 276.7245 - qwk: 0.4877 - val_loss: 293.8428 - val_mae: 13.8302 - val_mse: 293.8428 - val_qwk: 0.4363\nEpoch 78/100\n57/57 - 4s - 64ms/step - loss: 275.0366 - mae: 13.0307 - mse: 275.0366 - qwk: 0.4929 - val_loss: 293.8946 - val_mae: 13.8193 - val_mse: 293.8946 - val_qwk: 0.4367\nEpoch 79/100\n57/57 - 4s - 65ms/step - loss: 276.4921 - mae: 13.1092 - mse: 276.4921 - qwk: 0.4969 - val_loss: 293.2204 - val_mae: 13.7547 - val_mse: 293.2204 - val_qwk: 0.4489\n57/57 - 1s - 17ms/step - loss: 291.8466 - mae: 13.7702 - mse: 291.8466 - qwk: 0.4524\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"57/57 - 4s - 70ms/step - loss: 889.8527 - mae: 23.7747 - mse: 889.8527 - qwk: 0.1030 - val_loss: 547.2053 - val_mae: 18.5421 - val_mse: 547.2053 - val_qwk: 0.1441\nEpoch 2/100\n57/57 - 4s - 69ms/step - loss: 481.1210 - mae: 17.3578 - mse: 481.1210 - qwk: 0.2443 - val_loss: 332.5453 - val_mae: 14.4595 - val_mse: 332.5453 - val_qwk: 0.3981\nEpoch 3/100\n57/57 - 4s - 67ms/step - loss: 378.9996 - mae: 15.4240 - mse: 378.9996 - qwk: 0.3517 - val_loss: 301.6948 - val_mae: 13.8148 - val_mse: 301.6948 - val_qwk: 0.4135\nEpoch 4/100\n57/57 - 4s - 67ms/step - loss: 358.5028 - mae: 15.0887 - mse: 358.5028 - qwk: 0.3543 - val_loss: 299.0734 - val_mae: 13.7279 - val_mse: 299.0734 - val_qwk: 0.4327\nEpoch 5/100\n57/57 - 4s - 67ms/step - loss: 340.5253 - mae: 14.7681 - mse: 340.5253 - qwk: 0.3775 - val_loss: 293.5031 - val_mae: 13.5715 - val_mse: 293.5031 - val_qwk: 0.4373\nEpoch 6/100\n57/57 - 4s - 64ms/step - loss: 337.3664 - mae: 14.7112 - mse: 337.3664 - qwk: 0.3956 - val_loss: 293.9720 - val_mae: 13.5813 - val_mse: 293.9720 - val_qwk: 0.4494\nEpoch 7/100\n57/57 - 4s - 64ms/step - loss: 328.6751 - mae: 14.4739 - mse: 328.6751 - qwk: 0.3965 - val_loss: 292.3468 - val_mae: 13.5489 - val_mse: 292.3468 - val_qwk: 0.4439\nEpoch 8/100\n57/57 - 4s - 64ms/step - loss: 323.9898 - mae: 14.4085 - mse: 323.9898 - qwk: 0.4021 - val_loss: 292.2815 - val_mae: 13.5615 - val_mse: 292.2815 - val_qwk: 0.4451\nEpoch 9/100\n57/57 - 4s - 66ms/step - loss: 327.5283 - mae: 14.5263 - mse: 327.5283 - qwk: 0.3917 - val_loss: 290.3343 - val_mae: 13.4954 - val_mse: 290.3343 - val_qwk: 0.4457\nEpoch 10/100\n57/57 - 4s - 66ms/step - loss: 323.8087 - mae: 14.4115 - mse: 323.8087 - qwk: 0.4091 - val_loss: 291.1745 - val_mae: 13.5076 - val_mse: 291.1745 - val_qwk: 0.4457\nEpoch 11/100\n57/57 - 4s - 65ms/step - loss: 316.7715 - mae: 14.2175 - mse: 316.7715 - qwk: 0.4147 - val_loss: 289.9525 - val_mae: 13.4684 - val_mse: 289.9525 - val_qwk: 0.4441\nEpoch 12/100\n57/57 - 4s - 65ms/step - loss: 325.6412 - mae: 14.5031 - mse: 325.6412 - qwk: 0.3952 - val_loss: 291.2405 - val_mae: 13.4808 - val_mse: 291.2405 - val_qwk: 0.4516\nEpoch 13/100\n57/57 - 4s - 64ms/step - loss: 320.2077 - mae: 14.2520 - mse: 320.2077 - qwk: 0.4070 - val_loss: 290.1324 - val_mae: 13.4783 - val_mse: 290.1324 - val_qwk: 0.4614\nEpoch 14/100\n57/57 - 4s - 64ms/step - loss: 315.3597 - mae: 14.2407 - mse: 315.3597 - qwk: 0.4235 - val_loss: 289.6273 - val_mae: 13.4471 - val_mse: 289.6273 - val_qwk: 0.4399\nEpoch 15/100\n57/57 - 4s - 63ms/step - loss: 315.6169 - mae: 14.2147 - mse: 315.6169 - qwk: 0.4148 - val_loss: 290.8319 - val_mae: 13.4965 - val_mse: 290.8319 - val_qwk: 0.4523\nEpoch 16/100\n57/57 - 4s - 64ms/step - loss: 312.0143 - mae: 14.0588 - mse: 312.0143 - qwk: 0.4192 - val_loss: 289.3069 - val_mae: 13.4627 - val_mse: 289.3069 - val_qwk: 0.4626\nEpoch 17/100\n57/57 - 4s - 64ms/step - loss: 313.4101 - mae: 14.1515 - mse: 313.4101 - qwk: 0.4212 - val_loss: 288.1188 - val_mae: 13.4331 - val_mse: 288.1188 - val_qwk: 0.4660\nEpoch 18/100\n57/57 - 4s - 64ms/step - loss: 311.1783 - mae: 14.1056 - mse: 311.1783 - qwk: 0.4367 - val_loss: 288.7963 - val_mae: 13.4419 - val_mse: 288.7963 - val_qwk: 0.4597\nEpoch 19/100\n57/57 - 4s - 68ms/step - loss: 307.9767 - mae: 14.0354 - mse: 307.9767 - qwk: 0.4362 - val_loss: 288.6131 - val_mae: 13.4347 - val_mse: 288.6131 - val_qwk: 0.4606\nEpoch 20/100\n57/57 - 4s - 66ms/step - loss: 312.5804 - mae: 14.1648 - mse: 312.5804 - qwk: 0.4221 - val_loss: 288.8367 - val_mae: 13.4276 - val_mse: 288.8367 - val_qwk: 0.4633\nEpoch 21/100\n57/57 - 4s - 66ms/step - loss: 308.1204 - mae: 14.0706 - mse: 308.1204 - qwk: 0.4354 - val_loss: 288.4053 - val_mae: 13.4539 - val_mse: 288.4053 - val_qwk: 0.4640\nEpoch 22/100\n57/57 - 4s - 65ms/step - loss: 306.6828 - mae: 14.0199 - mse: 306.6828 - qwk: 0.4354 - val_loss: 287.2041 - val_mae: 13.4008 - val_mse: 287.2041 - val_qwk: 0.4660\nEpoch 23/100\n57/57 - 4s - 63ms/step - loss: 312.9091 - mae: 14.1212 - mse: 312.9091 - qwk: 0.4215 - val_loss: 288.9866 - val_mae: 13.4511 - val_mse: 288.9866 - val_qwk: 0.4530\nEpoch 24/100\n57/57 - 4s - 63ms/step - loss: 309.5682 - mae: 14.1059 - mse: 309.5682 - qwk: 0.4269 - val_loss: 287.4947 - val_mae: 13.4015 - val_mse: 287.4947 - val_qwk: 0.4627\nEpoch 25/100\n57/57 - 4s - 65ms/step - loss: 298.8954 - mae: 13.8001 - mse: 298.8954 - qwk: 0.4552 - val_loss: 287.4380 - val_mae: 13.3797 - val_mse: 287.4380 - val_qwk: 0.4657\nEpoch 26/100\n57/57 - 4s - 65ms/step - loss: 311.6082 - mae: 14.1091 - mse: 311.6082 - qwk: 0.4296 - val_loss: 287.9973 - val_mae: 13.4307 - val_mse: 287.9973 - val_qwk: 0.4723\nEpoch 27/100\n57/57 - 4s - 67ms/step - loss: 306.3998 - mae: 14.0984 - mse: 306.3998 - qwk: 0.4364 - val_loss: 286.2546 - val_mae: 13.3812 - val_mse: 286.2546 - val_qwk: 0.4700\nEpoch 28/100\n57/57 - 4s - 66ms/step - loss: 312.5625 - mae: 14.1553 - mse: 312.5625 - qwk: 0.4200 - val_loss: 287.1492 - val_mae: 13.4154 - val_mse: 287.1492 - val_qwk: 0.4700\nEpoch 29/100\n57/57 - 4s - 64ms/step - loss: 307.9034 - mae: 13.9981 - mse: 307.9034 - qwk: 0.4297 - val_loss: 287.1107 - val_mae: 13.4149 - val_mse: 287.1107 - val_qwk: 0.4709\nEpoch 30/100\n57/57 - 4s - 64ms/step - loss: 303.6146 - mae: 13.9548 - mse: 303.6146 - qwk: 0.4360 - val_loss: 286.5792 - val_mae: 13.3846 - val_mse: 286.5792 - val_qwk: 0.4709\nEpoch 31/100\n57/57 - 4s - 63ms/step - loss: 304.6176 - mae: 13.9971 - mse: 304.6176 - qwk: 0.4414 - val_loss: 284.9424 - val_mae: 13.3323 - val_mse: 284.9424 - val_qwk: 0.4698\nEpoch 32/100\n57/57 - 4s - 65ms/step - loss: 300.7802 - mae: 13.8592 - mse: 300.7802 - qwk: 0.4509 - val_loss: 285.9678 - val_mae: 13.3566 - val_mse: 285.9678 - val_qwk: 0.4760\nEpoch 33/100\n57/57 - 4s - 65ms/step - loss: 301.5821 - mae: 13.9343 - mse: 301.5821 - qwk: 0.4486 - val_loss: 286.4686 - val_mae: 13.3820 - val_mse: 286.4686 - val_qwk: 0.4752\nEpoch 34/100\n57/57 - 4s - 65ms/step - loss: 306.6538 - mae: 13.9990 - mse: 306.6538 - qwk: 0.4316 - val_loss: 285.5992 - val_mae: 13.3376 - val_mse: 285.5992 - val_qwk: 0.4691\nEpoch 35/100\n57/57 - 4s - 65ms/step - loss: 299.5997 - mae: 13.7692 - mse: 299.5997 - qwk: 0.4540 - val_loss: 285.8754 - val_mae: 13.3870 - val_mse: 285.8754 - val_qwk: 0.4672\nEpoch 36/100\n57/57 - 4s - 66ms/step - loss: 304.5996 - mae: 13.9946 - mse: 304.5996 - qwk: 0.4376 - val_loss: 284.7116 - val_mae: 13.3363 - val_mse: 284.7116 - val_qwk: 0.4738\nEpoch 37/100\n57/57 - 4s - 67ms/step - loss: 299.6268 - mae: 13.8576 - mse: 299.6268 - qwk: 0.4484 - val_loss: 286.1139 - val_mae: 13.3759 - val_mse: 286.1139 - val_qwk: 0.4743\nEpoch 38/100\n57/57 - 4s - 67ms/step - loss: 300.6826 - mae: 13.9051 - mse: 300.6826 - qwk: 0.4473 - val_loss: 286.6817 - val_mae: 13.4171 - val_mse: 286.6817 - val_qwk: 0.4753\nEpoch 39/100\n57/57 - 4s - 65ms/step - loss: 301.6248 - mae: 13.8966 - mse: 301.6248 - qwk: 0.4465 - val_loss: 285.2269 - val_mae: 13.3813 - val_mse: 285.2269 - val_qwk: 0.4721\nEpoch 40/100\n57/57 - 4s - 62ms/step - loss: 297.2172 - mae: 13.7792 - mse: 297.2172 - qwk: 0.4624 - val_loss: 287.2187 - val_mae: 13.3948 - val_mse: 287.2187 - val_qwk: 0.4738\nEpoch 41/100\n57/57 - 4s - 65ms/step - loss: 292.7796 - mae: 13.6123 - mse: 292.7796 - qwk: 0.4665 - val_loss: 284.9136 - val_mae: 13.3468 - val_mse: 284.9136 - val_qwk: 0.4642\nEpoch 42/100\n57/57 - 4s - 63ms/step - loss: 300.8596 - mae: 13.9363 - mse: 300.8596 - qwk: 0.4501 - val_loss: 284.5437 - val_mae: 13.3629 - val_mse: 284.5437 - val_qwk: 0.4768\nEpoch 43/100\n57/57 - 4s - 64ms/step - loss: 296.9485 - mae: 13.8303 - mse: 296.9485 - qwk: 0.4594 - val_loss: 283.9329 - val_mae: 13.3320 - val_mse: 283.9329 - val_qwk: 0.4759\nEpoch 44/100\n57/57 - 4s - 65ms/step - loss: 301.5977 - mae: 13.8795 - mse: 301.5977 - qwk: 0.4475 - val_loss: 283.8824 - val_mae: 13.3063 - val_mse: 283.8824 - val_qwk: 0.4732\nEpoch 45/100\n57/57 - 4s - 64ms/step - loss: 300.4483 - mae: 13.8089 - mse: 300.4483 - qwk: 0.4510 - val_loss: 284.1384 - val_mae: 13.3088 - val_mse: 284.1384 - val_qwk: 0.4677\nEpoch 46/100\n57/57 - 4s - 65ms/step - loss: 300.0650 - mae: 13.8437 - mse: 300.0650 - qwk: 0.4509 - val_loss: 282.9556 - val_mae: 13.2696 - val_mse: 282.9556 - val_qwk: 0.4699\nEpoch 47/100\n57/57 - 4s - 63ms/step - loss: 297.2431 - mae: 13.7113 - mse: 297.2431 - qwk: 0.4591 - val_loss: 285.0983 - val_mae: 13.3451 - val_mse: 285.0983 - val_qwk: 0.4734\nEpoch 48/100\n57/57 - 4s - 62ms/step - loss: 290.9782 - mae: 13.6145 - mse: 290.9782 - qwk: 0.4646 - val_loss: 285.1901 - val_mae: 13.3510 - val_mse: 285.1901 - val_qwk: 0.4635\nEpoch 49/100\n57/57 - 4s - 63ms/step - loss: 298.9607 - mae: 13.8671 - mse: 298.9607 - qwk: 0.4501 - val_loss: 283.7753 - val_mae: 13.3132 - val_mse: 283.7753 - val_qwk: 0.4666\nEpoch 50/100\n57/57 - 4s - 64ms/step - loss: 298.3254 - mae: 13.8251 - mse: 298.3254 - qwk: 0.4509 - val_loss: 283.7304 - val_mae: 13.2898 - val_mse: 283.7304 - val_qwk: 0.4702\nEpoch 51/100\n57/57 - 4s - 64ms/step - loss: 295.8727 - mae: 13.7484 - mse: 295.8727 - qwk: 0.4538 - val_loss: 284.1735 - val_mae: 13.3215 - val_mse: 284.1735 - val_qwk: 0.4630\nEpoch 52/100\n57/57 - 4s - 63ms/step - loss: 289.1378 - mae: 13.6193 - mse: 289.1378 - qwk: 0.4731 - val_loss: 284.8139 - val_mae: 13.3308 - val_mse: 284.8139 - val_qwk: 0.4625\nEpoch 53/100\n57/57 - 4s - 67ms/step - loss: 295.2601 - mae: 13.7286 - mse: 295.2601 - qwk: 0.4640 - val_loss: 284.5686 - val_mae: 13.3367 - val_mse: 284.5686 - val_qwk: 0.4591\nEpoch 54/100\n57/57 - 4s - 64ms/step - loss: 292.5357 - mae: 13.7225 - mse: 292.5357 - qwk: 0.4695 - val_loss: 285.1196 - val_mae: 13.3657 - val_mse: 285.1196 - val_qwk: 0.4618\nEpoch 55/100\n57/57 - 4s - 64ms/step - loss: 289.4106 - mae: 13.6294 - mse: 289.4106 - qwk: 0.4738 - val_loss: 287.0961 - val_mae: 13.4066 - val_mse: 287.0961 - val_qwk: 0.4783\nEpoch 56/100\n57/57 - 4s - 64ms/step - loss: 292.7825 - mae: 13.6943 - mse: 292.7825 - qwk: 0.4738 - val_loss: 283.6909 - val_mae: 13.3346 - val_mse: 283.6909 - val_qwk: 0.4740\n57/57 - 1s - 16ms/step - loss: 282.9556 - mae: 13.2696 - mse: 282.9556 - qwk: 0.4627\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"57/57 - 4s - 66ms/step - loss: 819.2934 - mae: 22.6132 - mse: 819.2934 - qwk: 0.0754 - val_loss: 480.3055 - val_mae: 17.5422 - val_mse: 480.3055 - val_qwk: 0.1909\nEpoch 2/100\n57/57 - 4s - 65ms/step - loss: 444.0466 - mae: 16.6560 - mse: 444.0466 - qwk: 0.2728 - val_loss: 344.5447 - val_mae: 14.7211 - val_mse: 344.5447 - val_qwk: 0.3506\nEpoch 3/100\n57/57 - 4s - 65ms/step - loss: 374.9289 - mae: 15.4759 - mse: 374.9289 - qwk: 0.3493 - val_loss: 317.2442 - val_mae: 14.0974 - val_mse: 317.2442 - val_qwk: 0.3768\nEpoch 4/100\n57/57 - 4s - 65ms/step - loss: 349.8003 - mae: 14.8364 - mse: 349.8003 - qwk: 0.3722 - val_loss: 306.7350 - val_mae: 13.9245 - val_mse: 306.7350 - val_qwk: 0.4009\nEpoch 5/100\n57/57 - 4s - 66ms/step - loss: 342.9285 - mae: 14.6479 - mse: 342.9285 - qwk: 0.3815 - val_loss: 304.2454 - val_mae: 13.8700 - val_mse: 304.2454 - val_qwk: 0.3987\nEpoch 6/100\n57/57 - 4s - 66ms/step - loss: 334.2870 - mae: 14.5462 - mse: 334.2870 - qwk: 0.3848 - val_loss: 302.8159 - val_mae: 13.8422 - val_mse: 302.8159 - val_qwk: 0.4058\nEpoch 7/100\n57/57 - 4s - 64ms/step - loss: 325.6520 - mae: 14.3173 - mse: 325.6520 - qwk: 0.3977 - val_loss: 298.9784 - val_mae: 13.7689 - val_mse: 298.9784 - val_qwk: 0.4309\nEpoch 8/100\n57/57 - 4s - 63ms/step - loss: 323.8528 - mae: 14.3324 - mse: 323.8528 - qwk: 0.4027 - val_loss: 298.7377 - val_mae: 13.7211 - val_mse: 298.7377 - val_qwk: 0.4169\nEpoch 9/100\n57/57 - 4s - 65ms/step - loss: 318.9791 - mae: 14.2389 - mse: 318.9791 - qwk: 0.4008 - val_loss: 297.8097 - val_mae: 13.7399 - val_mse: 297.8097 - val_qwk: 0.4279\nEpoch 10/100\n57/57 - 4s - 67ms/step - loss: 315.6168 - mae: 14.1654 - mse: 315.6168 - qwk: 0.4175 - val_loss: 297.4285 - val_mae: 13.7010 - val_mse: 297.4285 - val_qwk: 0.4259\nEpoch 11/100\n57/57 - 4s - 66ms/step - loss: 314.9404 - mae: 14.1492 - mse: 314.9404 - qwk: 0.4147 - val_loss: 298.9926 - val_mae: 13.6817 - val_mse: 298.9926 - val_qwk: 0.4191\nEpoch 12/100\n57/57 - 4s - 66ms/step - loss: 313.7453 - mae: 14.1070 - mse: 313.7453 - qwk: 0.4176 - val_loss: 295.6918 - val_mae: 13.6411 - val_mse: 295.6918 - val_qwk: 0.4235\nEpoch 13/100\n57/57 - 4s - 65ms/step - loss: 307.5773 - mae: 13.9540 - mse: 307.5773 - qwk: 0.4337 - val_loss: 295.9843 - val_mae: 13.6108 - val_mse: 295.9843 - val_qwk: 0.4253\nEpoch 14/100\n57/57 - 4s - 67ms/step - loss: 312.0299 - mae: 14.0499 - mse: 312.0299 - qwk: 0.4251 - val_loss: 293.9949 - val_mae: 13.6078 - val_mse: 293.9949 - val_qwk: 0.4280\nEpoch 15/100\n57/57 - 4s - 65ms/step - loss: 311.7051 - mae: 14.1372 - mse: 311.7051 - qwk: 0.4206 - val_loss: 294.9498 - val_mae: 13.6232 - val_mse: 294.9498 - val_qwk: 0.4303\nEpoch 16/100\n57/57 - 4s - 64ms/step - loss: 309.8016 - mae: 14.0346 - mse: 309.8016 - qwk: 0.4229 - val_loss: 296.8617 - val_mae: 13.6468 - val_mse: 296.8617 - val_qwk: 0.4162\nEpoch 17/100\n57/57 - 4s - 64ms/step - loss: 304.0471 - mae: 13.9173 - mse: 304.0471 - qwk: 0.4353 - val_loss: 292.5083 - val_mae: 13.5973 - val_mse: 292.5083 - val_qwk: 0.4366\nEpoch 18/100\n57/57 - 4s - 64ms/step - loss: 305.6598 - mae: 13.9641 - mse: 305.6598 - qwk: 0.4378 - val_loss: 292.0040 - val_mae: 13.5826 - val_mse: 292.0040 - val_qwk: 0.4336\nEpoch 19/100\n57/57 - 4s - 64ms/step - loss: 306.9945 - mae: 13.8883 - mse: 306.9945 - qwk: 0.4315 - val_loss: 291.9427 - val_mae: 13.5616 - val_mse: 291.9427 - val_qwk: 0.4414\nEpoch 20/100\n57/57 - 4s - 64ms/step - loss: 302.7173 - mae: 13.8810 - mse: 302.7173 - qwk: 0.4336 - val_loss: 292.3339 - val_mae: 13.5683 - val_mse: 292.3339 - val_qwk: 0.4324\nEpoch 21/100\n57/57 - 4s - 64ms/step - loss: 300.6006 - mae: 13.7662 - mse: 300.6006 - qwk: 0.4424 - val_loss: 292.5309 - val_mae: 13.5728 - val_mse: 292.5309 - val_qwk: 0.4397\nEpoch 22/100\n57/57 - 4s - 65ms/step - loss: 303.9333 - mae: 13.9028 - mse: 303.9333 - qwk: 0.4381 - val_loss: 291.7594 - val_mae: 13.6048 - val_mse: 291.7594 - val_qwk: 0.4418\nEpoch 23/100\n57/57 - 4s - 65ms/step - loss: 300.5328 - mae: 13.8603 - mse: 300.5328 - qwk: 0.4450 - val_loss: 292.1593 - val_mae: 13.5843 - val_mse: 292.1593 - val_qwk: 0.4409\nEpoch 24/100\n57/57 - 4s - 65ms/step - loss: 305.2328 - mae: 13.8858 - mse: 305.2328 - qwk: 0.4317 - val_loss: 290.9380 - val_mae: 13.5709 - val_mse: 290.9380 - val_qwk: 0.4427\nEpoch 25/100\n57/57 - 4s - 64ms/step - loss: 298.6425 - mae: 13.7299 - mse: 298.6425 - qwk: 0.4477 - val_loss: 292.2210 - val_mae: 13.5859 - val_mse: 292.2210 - val_qwk: 0.4357\nEpoch 26/100\n57/57 - 4s - 67ms/step - loss: 302.2626 - mae: 13.8750 - mse: 302.2626 - qwk: 0.4404 - val_loss: 292.6618 - val_mae: 13.5784 - val_mse: 292.6618 - val_qwk: 0.4346\nEpoch 27/100\n57/57 - 4s - 64ms/step - loss: 299.3404 - mae: 13.8330 - mse: 299.3404 - qwk: 0.4467 - val_loss: 293.0089 - val_mae: 13.5890 - val_mse: 293.0089 - val_qwk: 0.4234\nEpoch 28/100\n57/57 - 4s - 66ms/step - loss: 303.0356 - mae: 13.9154 - mse: 303.0356 - qwk: 0.4425 - val_loss: 291.8594 - val_mae: 13.5840 - val_mse: 291.8594 - val_qwk: 0.4321\nEpoch 29/100\n57/57 - 4s - 65ms/step - loss: 300.1574 - mae: 13.7927 - mse: 300.1574 - qwk: 0.4483 - val_loss: 290.4317 - val_mae: 13.5344 - val_mse: 290.4317 - val_qwk: 0.4385\nEpoch 30/100\n57/57 - 4s - 64ms/step - loss: 296.2642 - mae: 13.7096 - mse: 296.2642 - qwk: 0.4566 - val_loss: 290.9748 - val_mae: 13.5370 - val_mse: 290.9748 - val_qwk: 0.4287\nEpoch 31/100\n57/57 - 4s - 67ms/step - loss: 300.4074 - mae: 13.7911 - mse: 300.4074 - qwk: 0.4511 - val_loss: 291.5728 - val_mae: 13.5715 - val_mse: 291.5728 - val_qwk: 0.4321\nEpoch 32/100\n57/57 - 4s - 66ms/step - loss: 297.2549 - mae: 13.7267 - mse: 297.2549 - qwk: 0.4529 - val_loss: 294.1530 - val_mae: 13.5742 - val_mse: 294.1530 - val_qwk: 0.4216\nEpoch 33/100\n57/57 - 4s - 66ms/step - loss: 302.1421 - mae: 13.8076 - mse: 302.1421 - qwk: 0.4431 - val_loss: 290.4552 - val_mae: 13.5589 - val_mse: 290.4552 - val_qwk: 0.4424\nEpoch 34/100\n57/57 - 4s - 68ms/step - loss: 295.3449 - mae: 13.7089 - mse: 295.3449 - qwk: 0.4563 - val_loss: 291.0169 - val_mae: 13.5533 - val_mse: 291.0169 - val_qwk: 0.4466\nEpoch 35/100\n57/57 - 4s - 64ms/step - loss: 292.5304 - mae: 13.6195 - mse: 292.5304 - qwk: 0.4647 - val_loss: 291.2134 - val_mae: 13.5329 - val_mse: 291.2134 - val_qwk: 0.4360\nEpoch 36/100\n57/57 - 4s - 66ms/step - loss: 294.7958 - mae: 13.5737 - mse: 294.7958 - qwk: 0.4576 - val_loss: 289.9405 - val_mae: 13.5190 - val_mse: 289.9405 - val_qwk: 0.4525\nEpoch 37/100\n57/57 - 4s - 65ms/step - loss: 298.3671 - mae: 13.7700 - mse: 298.3671 - qwk: 0.4423 - val_loss: 290.4813 - val_mae: 13.5276 - val_mse: 290.4813 - val_qwk: 0.4411\nEpoch 38/100\n57/57 - 4s - 64ms/step - loss: 296.0390 - mae: 13.6825 - mse: 296.0390 - qwk: 0.4609 - val_loss: 290.3301 - val_mae: 13.5049 - val_mse: 290.3301 - val_qwk: 0.4374\nEpoch 39/100\n57/57 - 4s - 63ms/step - loss: 293.8157 - mae: 13.6553 - mse: 293.8157 - qwk: 0.4685 - val_loss: 291.5597 - val_mae: 13.5332 - val_mse: 291.5597 - val_qwk: 0.4311\nEpoch 40/100\n57/57 - 4s - 65ms/step - loss: 299.6833 - mae: 13.7233 - mse: 299.6833 - qwk: 0.4378 - val_loss: 290.3323 - val_mae: 13.5131 - val_mse: 290.3323 - val_qwk: 0.4337\nEpoch 41/100\n57/57 - 4s - 64ms/step - loss: 297.0020 - mae: 13.6656 - mse: 297.0020 - qwk: 0.4566 - val_loss: 290.9344 - val_mae: 13.5295 - val_mse: 290.9344 - val_qwk: 0.4389\nEpoch 42/100\n57/57 - 4s - 64ms/step - loss: 295.5663 - mae: 13.7029 - mse: 295.5663 - qwk: 0.4550 - val_loss: 289.0941 - val_mae: 13.5167 - val_mse: 289.0941 - val_qwk: 0.4497\nEpoch 43/100\n57/57 - 4s - 64ms/step - loss: 292.6205 - mae: 13.6251 - mse: 292.6205 - qwk: 0.4634 - val_loss: 289.4540 - val_mae: 13.4747 - val_mse: 289.4540 - val_qwk: 0.4466\nEpoch 44/100\n57/57 - 4s - 65ms/step - loss: 295.0923 - mae: 13.6747 - mse: 295.0923 - qwk: 0.4559 - val_loss: 288.4882 - val_mae: 13.4610 - val_mse: 288.4882 - val_qwk: 0.4489\nEpoch 45/100\n57/57 - 4s - 65ms/step - loss: 295.6019 - mae: 13.7080 - mse: 295.6019 - qwk: 0.4604 - val_loss: 290.5046 - val_mae: 13.4804 - val_mse: 290.5046 - val_qwk: 0.4307\nEpoch 46/100\n57/57 - 4s - 65ms/step - loss: 290.4171 - mae: 13.4791 - mse: 290.4171 - qwk: 0.4684 - val_loss: 290.4545 - val_mae: 13.4993 - val_mse: 290.4545 - val_qwk: 0.4359\nEpoch 47/100\n57/57 - 4s - 63ms/step - loss: 293.3024 - mae: 13.6914 - mse: 293.3024 - qwk: 0.4714 - val_loss: 288.6915 - val_mae: 13.4537 - val_mse: 288.6915 - val_qwk: 0.4419\nEpoch 48/100\n57/57 - 4s - 66ms/step - loss: 292.3868 - mae: 13.5599 - mse: 292.3868 - qwk: 0.4618 - val_loss: 288.0182 - val_mae: 13.4343 - val_mse: 288.0182 - val_qwk: 0.4548\nEpoch 49/100\n57/57 - 4s - 64ms/step - loss: 286.9787 - mae: 13.4818 - mse: 286.9787 - qwk: 0.4766 - val_loss: 287.6696 - val_mae: 13.4539 - val_mse: 287.6696 - val_qwk: 0.4467\nEpoch 50/100\n57/57 - 4s - 65ms/step - loss: 289.4036 - mae: 13.5482 - mse: 289.4036 - qwk: 0.4746 - val_loss: 288.4218 - val_mae: 13.4668 - val_mse: 288.4218 - val_qwk: 0.4519\nEpoch 51/100\n57/57 - 4s - 64ms/step - loss: 289.5458 - mae: 13.5765 - mse: 289.5458 - qwk: 0.4781 - val_loss: 288.0254 - val_mae: 13.4663 - val_mse: 288.0254 - val_qwk: 0.4489\nEpoch 52/100\n57/57 - 4s - 65ms/step - loss: 290.6725 - mae: 13.5534 - mse: 290.6725 - qwk: 0.4715 - val_loss: 287.9037 - val_mae: 13.4336 - val_mse: 287.9037 - val_qwk: 0.4518\nEpoch 53/100\n57/57 - 4s - 64ms/step - loss: 286.5972 - mae: 13.4698 - mse: 286.5972 - qwk: 0.4730 - val_loss: 288.2371 - val_mae: 13.4700 - val_mse: 288.2371 - val_qwk: 0.4494\nEpoch 54/100\n57/57 - 4s - 64ms/step - loss: 284.8141 - mae: 13.3488 - mse: 284.8141 - qwk: 0.4754 - val_loss: 287.0964 - val_mae: 13.4561 - val_mse: 287.0964 - val_qwk: 0.4552\nEpoch 55/100\n57/57 - 4s - 63ms/step - loss: 290.3039 - mae: 13.6602 - mse: 290.3039 - qwk: 0.4698 - val_loss: 288.8045 - val_mae: 13.4648 - val_mse: 288.8045 - val_qwk: 0.4403\nEpoch 56/100\n57/57 - 4s - 63ms/step - loss: 286.2668 - mae: 13.4766 - mse: 286.2668 - qwk: 0.4705 - val_loss: 287.9137 - val_mae: 13.4681 - val_mse: 287.9137 - val_qwk: 0.4525\nEpoch 57/100\n57/57 - 4s - 67ms/step - loss: 284.2318 - mae: 13.4441 - mse: 284.2318 - qwk: 0.4875 - val_loss: 289.5419 - val_mae: 13.4632 - val_mse: 289.5419 - val_qwk: 0.4431\nEpoch 58/100\n57/57 - 4s - 63ms/step - loss: 288.3012 - mae: 13.4366 - mse: 288.3012 - qwk: 0.4747 - val_loss: 288.3852 - val_mae: 13.4664 - val_mse: 288.3852 - val_qwk: 0.4504\nEpoch 59/100\n57/57 - 4s - 65ms/step - loss: 287.5662 - mae: 13.5320 - mse: 287.5662 - qwk: 0.4752 - val_loss: 287.5471 - val_mae: 13.4715 - val_mse: 287.5471 - val_qwk: 0.4507\nEpoch 60/100\n57/57 - 4s - 65ms/step - loss: 289.4880 - mae: 13.4990 - mse: 289.4880 - qwk: 0.4738 - val_loss: 286.2671 - val_mae: 13.4274 - val_mse: 286.2671 - val_qwk: 0.4513\nEpoch 61/100\n57/57 - 4s - 64ms/step - loss: 288.7163 - mae: 13.5281 - mse: 288.7163 - qwk: 0.4717 - val_loss: 287.5629 - val_mae: 13.4581 - val_mse: 287.5629 - val_qwk: 0.4522\nEpoch 62/100\n57/57 - 4s - 64ms/step - loss: 282.4829 - mae: 13.4219 - mse: 282.4829 - qwk: 0.4766 - val_loss: 288.2410 - val_mae: 13.4397 - val_mse: 288.2410 - val_qwk: 0.4504\nEpoch 63/100\n57/57 - 4s - 65ms/step - loss: 278.6048 - mae: 13.2800 - mse: 278.6048 - qwk: 0.4981 - val_loss: 286.9969 - val_mae: 13.4078 - val_mse: 286.9969 - val_qwk: 0.4547\nEpoch 64/100\n57/57 - 4s - 63ms/step - loss: 287.1292 - mae: 13.5310 - mse: 287.1292 - qwk: 0.4806 - val_loss: 287.1408 - val_mae: 13.3956 - val_mse: 287.1408 - val_qwk: 0.4562\nEpoch 65/100\n57/57 - 4s - 63ms/step - loss: 284.4665 - mae: 13.4452 - mse: 284.4665 - qwk: 0.4789 - val_loss: 286.9771 - val_mae: 13.4346 - val_mse: 286.9771 - val_qwk: 0.4638\nEpoch 66/100\n57/57 - 4s - 68ms/step - loss: 281.3976 - mae: 13.4211 - mse: 281.3976 - qwk: 0.4901 - val_loss: 287.7821 - val_mae: 13.4444 - val_mse: 287.7821 - val_qwk: 0.4589\nEpoch 67/100\n57/57 - 4s - 66ms/step - loss: 283.4380 - mae: 13.4643 - mse: 283.4380 - qwk: 0.4849 - val_loss: 286.2416 - val_mae: 13.4169 - val_mse: 286.2416 - val_qwk: 0.4503\nEpoch 68/100\n57/57 - 4s - 65ms/step - loss: 277.4767 - mae: 13.2554 - mse: 277.4767 - qwk: 0.4967 - val_loss: 286.7564 - val_mae: 13.4067 - val_mse: 286.7564 - val_qwk: 0.4516\nEpoch 69/100\n57/57 - 4s - 66ms/step - loss: 281.3240 - mae: 13.3035 - mse: 281.3240 - qwk: 0.4863 - val_loss: 286.9800 - val_mae: 13.4425 - val_mse: 286.9800 - val_qwk: 0.4511\nEpoch 70/100\n57/57 - 4s - 73ms/step - loss: 289.2746 - mae: 13.4263 - mse: 289.2746 - qwk: 0.4762 - val_loss: 287.0442 - val_mae: 13.4287 - val_mse: 287.0442 - val_qwk: 0.4554\nEpoch 71/100\n57/57 - 4s - 70ms/step - loss: 283.4243 - mae: 13.3763 - mse: 283.4243 - qwk: 0.4849 - val_loss: 286.4321 - val_mae: 13.4079 - val_mse: 286.4321 - val_qwk: 0.4526\nEpoch 72/100\n57/57 - 4s - 74ms/step - loss: 284.2336 - mae: 13.4483 - mse: 284.2336 - qwk: 0.4805 - val_loss: 286.3030 - val_mae: 13.3885 - val_mse: 286.3030 - val_qwk: 0.4532\nEpoch 73/100\n57/57 - 4s - 65ms/step - loss: 285.3966 - mae: 13.4772 - mse: 285.3966 - qwk: 0.4844 - val_loss: 286.7597 - val_mae: 13.3964 - val_mse: 286.7597 - val_qwk: 0.4498\nEpoch 74/100\n57/57 - 4s - 66ms/step - loss: 273.2921 - mae: 13.0900 - mse: 273.2921 - qwk: 0.5036 - val_loss: 288.3798 - val_mae: 13.4490 - val_mse: 288.3798 - val_qwk: 0.4459\nEpoch 75/100\n57/57 - 4s - 64ms/step - loss: 285.1027 - mae: 13.4782 - mse: 285.1027 - qwk: 0.4833 - val_loss: 286.5635 - val_mae: 13.3995 - val_mse: 286.5635 - val_qwk: 0.4652\nEpoch 76/100\n57/57 - 4s - 71ms/step - loss: 279.0680 - mae: 13.2716 - mse: 279.0680 - qwk: 0.4975 - val_loss: 285.5401 - val_mae: 13.3893 - val_mse: 285.5401 - val_qwk: 0.4536\nEpoch 77/100\n57/57 - 4s - 76ms/step - loss: 278.5335 - mae: 13.2956 - mse: 278.5335 - qwk: 0.4937 - val_loss: 285.1932 - val_mae: 13.3688 - val_mse: 285.1932 - val_qwk: 0.4566\nEpoch 78/100\n57/57 - 4s - 73ms/step - loss: 281.2585 - mae: 13.4435 - mse: 281.2585 - qwk: 0.4924 - val_loss: 286.0419 - val_mae: 13.3632 - val_mse: 286.0419 - val_qwk: 0.4443\nEpoch 79/100\n57/57 - 4s - 66ms/step - loss: 281.4326 - mae: 13.3787 - mse: 281.4326 - qwk: 0.4822 - val_loss: 288.3837 - val_mae: 13.4134 - val_mse: 288.3837 - val_qwk: 0.4497\nEpoch 80/100\n57/57 - 4s - 65ms/step - loss: 277.0044 - mae: 13.2773 - mse: 277.0044 - qwk: 0.4988 - val_loss: 285.2592 - val_mae: 13.3408 - val_mse: 285.2592 - val_qwk: 0.4459\nEpoch 81/100\n57/57 - 4s - 64ms/step - loss: 280.3737 - mae: 13.3485 - mse: 280.3737 - qwk: 0.4844 - val_loss: 285.6176 - val_mae: 13.3815 - val_mse: 285.6176 - val_qwk: 0.4518\nEpoch 82/100\n57/57 - 4s - 65ms/step - loss: 277.8289 - mae: 13.3253 - mse: 277.8289 - qwk: 0.4932 - val_loss: 284.4826 - val_mae: 13.3635 - val_mse: 284.4826 - val_qwk: 0.4526\nEpoch 83/100\n57/57 - 4s - 64ms/step - loss: 275.8595 - mae: 13.2515 - mse: 275.8595 - qwk: 0.4971 - val_loss: 284.7887 - val_mae: 13.3728 - val_mse: 284.7887 - val_qwk: 0.4530\nEpoch 84/100\n57/57 - 4s - 64ms/step - loss: 272.2629 - mae: 13.1341 - mse: 272.2629 - qwk: 0.5117 - val_loss: 284.1043 - val_mae: 13.3803 - val_mse: 284.1043 - val_qwk: 0.4497\nEpoch 85/100\n57/57 - 4s - 66ms/step - loss: 272.3175 - mae: 13.1549 - mse: 272.3175 - qwk: 0.5125 - val_loss: 283.4333 - val_mae: 13.3278 - val_mse: 283.4333 - val_qwk: 0.4536\nEpoch 86/100\n57/57 - 4s - 63ms/step - loss: 275.6829 - mae: 13.1861 - mse: 275.6829 - qwk: 0.4974 - val_loss: 283.4326 - val_mae: 13.3631 - val_mse: 283.4326 - val_qwk: 0.4557\nEpoch 87/100\n57/57 - 4s - 63ms/step - loss: 277.9341 - mae: 13.3151 - mse: 277.9341 - qwk: 0.4992 - val_loss: 283.9625 - val_mae: 13.3763 - val_mse: 283.9625 - val_qwk: 0.4601\nEpoch 88/100\n57/57 - 4s - 64ms/step - loss: 277.2950 - mae: 13.2873 - mse: 277.2950 - qwk: 0.5012 - val_loss: 284.2978 - val_mae: 13.3793 - val_mse: 284.2978 - val_qwk: 0.4589\nEpoch 89/100\n57/57 - 4s - 65ms/step - loss: 276.9808 - mae: 13.3077 - mse: 276.9808 - qwk: 0.5018 - val_loss: 282.9716 - val_mae: 13.3691 - val_mse: 282.9716 - val_qwk: 0.4501\nEpoch 90/100\n57/57 - 4s - 65ms/step - loss: 272.9077 - mae: 13.2342 - mse: 272.9077 - qwk: 0.5014 - val_loss: 283.4276 - val_mae: 13.3589 - val_mse: 283.4276 - val_qwk: 0.4585\nEpoch 91/100\n57/57 - 4s - 65ms/step - loss: 275.1926 - mae: 13.2025 - mse: 275.1926 - qwk: 0.4920 - val_loss: 284.3218 - val_mae: 13.3663 - val_mse: 284.3218 - val_qwk: 0.4410\nEpoch 92/100\n57/57 - 4s - 64ms/step - loss: 272.8232 - mae: 13.2150 - mse: 272.8232 - qwk: 0.5045 - val_loss: 283.7081 - val_mae: 13.3478 - val_mse: 283.7081 - val_qwk: 0.4625\nEpoch 93/100\n57/57 - 4s - 68ms/step - loss: 270.7158 - mae: 13.0613 - mse: 270.7158 - qwk: 0.5109 - val_loss: 283.5478 - val_mae: 13.3524 - val_mse: 283.5478 - val_qwk: 0.4658\nEpoch 94/100\n57/57 - 4s - 73ms/step - loss: 271.4676 - mae: 13.0756 - mse: 271.4676 - qwk: 0.5135 - val_loss: 283.3794 - val_mae: 13.3092 - val_mse: 283.3794 - val_qwk: 0.4613\nEpoch 95/100\n57/57 - 4s - 70ms/step - loss: 275.5932 - mae: 13.2636 - mse: 275.5932 - qwk: 0.5005 - val_loss: 284.3329 - val_mae: 13.3562 - val_mse: 284.3329 - val_qwk: 0.4631\nEpoch 96/100\n57/57 - 4s - 67ms/step - loss: 273.0796 - mae: 13.1748 - mse: 273.0796 - qwk: 0.5067 - val_loss: 286.6118 - val_mae: 13.3878 - val_mse: 286.6118 - val_qwk: 0.4574\nEpoch 97/100\n57/57 - 4s - 65ms/step - loss: 275.6324 - mae: 13.2591 - mse: 275.6324 - qwk: 0.5079 - val_loss: 283.2276 - val_mae: 13.3585 - val_mse: 283.2276 - val_qwk: 0.4568\nEpoch 98/100\n57/57 - 4s - 64ms/step - loss: 265.3574 - mae: 13.0126 - mse: 265.3574 - qwk: 0.5113 - val_loss: 285.8282 - val_mae: 13.3655 - val_mse: 285.8282 - val_qwk: 0.4583\nEpoch 99/100\n57/57 - 4s - 71ms/step - loss: 276.0817 - mae: 13.1931 - mse: 276.0817 - qwk: 0.5026 - val_loss: 285.9586 - val_mae: 13.3714 - val_mse: 285.9586 - val_qwk: 0.4673\n57/57 - 1s - 17ms/step - loss: 282.9716 - mae: 13.3691 - mse: 282.9716 - qwk: 0.4501\nAverage Validation QWK: 0.45508497953414917, Average Validation Loss: 285.9246012369792\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"**Score: 0.386, 0.366**","metadata":{}},{"cell_type":"markdown","source":"**Thoughts**:\n* Small improvement on V1 basic model\n* The way we augmented the data was maybe weird, augmenting before splitting into train and val. Perhaps we should split first and then augment the train set only, i.e., write an augment function to use in the cv loop\n* We still technically had <10x the number of rows as params. So we can consider augmenting again, or doing PCA","metadata":{}},{"cell_type":"markdown","source":"# V4: Using augmentation function","metadata":{}},{"cell_type":"code","source":"def aug_data(X_train, y_train, noise_factor_X, noise_factor_y):\n    std_X = np.std(X_train, axis=0)\n    std_y = np.std(y_train, axis=0)\n    X_train_noisy = X_train + (noise_factor_X * np.random.normal(0, std_X, X_train.shape))\n    y_train_noisy = y_train + (noise_factor_y * np.random.normal(0, std_y, y_train.shape))\n    X_train_aug = pd.concat([X_train,X_train_noisy], ignore_index=True)\n    y_train_aug = pd.concat([y_train,y_train_noisy], ignore_index=True)\n    return (X_train_aug, y_train_aug)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T07:33:48.424078Z","iopub.execute_input":"2025-01-17T07:33:48.424463Z","iopub.status.idle":"2025-01-17T07:33:48.431108Z","shell.execute_reply.started":"2025-01-17T07:33:48.424430Z","shell.execute_reply":"2025-01-17T07:33:48.429704Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def create_model_v4():\n    model = keras.models.Sequential([\n        keras.layers.Dense(20, input_shape=(X_train_labelled.shape[1],), activation=\"selu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(20, activation=\"selu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(1, activation=\"linear\")\n    ])\n\n    model.compile(optimizer=Adam(learning_rate=0.005), loss='mean_squared_error', metrics=['mae','mse',qwk],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T01:52:34.366371Z","iopub.execute_input":"2025-01-17T01:52:34.366757Z","iopub.status.idle":"2025-01-17T01:52:34.372635Z","shell.execute_reply.started":"2025-01-17T01:52:34.366728Z","shell.execute_reply":"2025-01-17T01:52:34.371589Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"model = create_model_v4()\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T01:52:39.225446Z","iopub.execute_input":"2025-01-17T01:52:39.225873Z","iopub.status.idle":"2025-01-17T01:52:39.357342Z","shell.execute_reply.started":"2025-01-17T01:52:39.225839Z","shell.execute_reply":"2025-01-17T01:52:39.356355Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │             \u001b[38;5;34m700\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │             \u001b[38;5;34m420\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m21\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">420</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,141\u001b[0m (4.46 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,141</span> (4.46 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,141\u001b[0m (4.46 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,141</span> (4.46 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"'''\n# Prepare to store results\nvalidation_qwk = []\nvalidation_losses = []\nmodels = []\n\n# Loop over each fold\nfor train_index, val_index in kf.split(X_train_labelled):\n    # Split the data into training and validation sets for the current fold\n    X_train_t, X_train_v = X_train_labelled.loc[train_index], X_train_labelled.loc[val_index]\n    y_train_t, y_train_v = y_train_labelled.loc[train_index], y_train_labelled.loc[val_index]\n\n    print(X_train_t.shape, y_train_t.shape, X_train_v.shape, y_train_v.shape)\n    X_train_t, y_train_t = aug_data(X_train_t, y_train_t, 0.1, 0.1)\n    print(X_train_t.shape, y_train_t.shape, X_train_v.shape, y_train_v.shape)    \n    \n    # Build a new model for each fold\n    model = create_model_v4()\n    \n    # Define early stopping to avoid overfitting\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n    \n    # Train the model on the training set and evaluate on the validation set\n    history = model.fit(\n        X_train_t, y_train_t,\n        validation_data=(X_train_v, y_train_v),\n        epochs=100,\n        batch_size=64,\n        callbacks=[early_stopping],\n        verbose=2\n    )\n    \n    # Evaluate the model on the validation set\n    val_loss, val_mae, val_mse, val_qwk = model.evaluate(X_train_v, y_train_v, verbose=2)\n    #val_qwk = model.evaluate(X_train_v, y_train_v, verbose=2)\n    validation_qwk.append(val_qwk)\n    validation_losses.append(val_loss)\n    models.append(model)\n\n# Calculate the average validation loss across all folds\navg_val_qwk = np.mean(validation_qwk)\navg_val_loss = np.mean(validation_losses)\nprint(f\"Average Validation QWK: {avg_val_qwk}, Average Validation Loss: {avg_val_loss}\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T08:41:01.022493Z","iopub.execute_input":"2025-01-16T08:41:01.023084Z","iopub.status.idle":"2025-01-16T08:44:57.240361Z","shell.execute_reply.started":"2025-01-16T08:41:01.023038Z","shell.execute_reply":"2025-01-16T08:44:57.239098Z"}},"outputs":[{"name":"stdout","text":"(1824, 34) (1824,) (912, 34) (912,)\n(3648, 34) (3648,) (912, 34) (912,)\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"57/57 - 5s - 91ms/step - loss: 875.7547 - mae: 23.7453 - mse: 875.7547 - qwk: 0.0610 - val_loss: 465.3753 - val_mae: 16.9110 - val_mse: 465.3753 - val_qwk: 0.2709\nEpoch 2/100\n57/57 - 4s - 67ms/step - loss: 438.3978 - mae: 16.5049 - mse: 438.3978 - qwk: 0.2541 - val_loss: 333.6129 - val_mae: 14.6263 - val_mse: 333.6129 - val_qwk: 0.3566\nEpoch 3/100\n57/57 - 4s - 63ms/step - loss: 355.6319 - mae: 14.9176 - mse: 355.6319 - qwk: 0.3361 - val_loss: 322.2124 - val_mae: 14.4199 - val_mse: 322.2124 - val_qwk: 0.3947\nEpoch 4/100\n57/57 - 4s - 63ms/step - loss: 323.6082 - mae: 14.3479 - mse: 323.6082 - qwk: 0.3798 - val_loss: 316.6915 - val_mae: 14.3302 - val_mse: 316.6915 - val_qwk: 0.4011\nEpoch 5/100\n57/57 - 4s - 64ms/step - loss: 320.1348 - mae: 14.1951 - mse: 320.1348 - qwk: 0.3803 - val_loss: 315.9749 - val_mae: 14.3012 - val_mse: 315.9749 - val_qwk: 0.3996\nEpoch 6/100\n57/57 - 4s - 62ms/step - loss: 311.1091 - mae: 14.0760 - mse: 311.1091 - qwk: 0.4001 - val_loss: 315.2975 - val_mae: 14.2568 - val_mse: 315.2975 - val_qwk: 0.4120\nEpoch 7/100\n57/57 - 4s - 64ms/step - loss: 310.0119 - mae: 14.0859 - mse: 310.0119 - qwk: 0.4070 - val_loss: 316.8084 - val_mae: 14.2843 - val_mse: 316.8084 - val_qwk: 0.4223\nEpoch 8/100\n57/57 - 4s - 62ms/step - loss: 301.6998 - mae: 13.8855 - mse: 301.6998 - qwk: 0.4220 - val_loss: 314.4651 - val_mae: 14.2160 - val_mse: 314.4651 - val_qwk: 0.4138\nEpoch 9/100\n57/57 - 4s - 63ms/step - loss: 299.2221 - mae: 13.7870 - mse: 299.2221 - qwk: 0.4233 - val_loss: 315.9269 - val_mae: 14.2409 - val_mse: 315.9269 - val_qwk: 0.4227\nEpoch 10/100\n57/57 - 4s - 62ms/step - loss: 300.5024 - mae: 13.9109 - mse: 300.5024 - qwk: 0.4209 - val_loss: 312.6992 - val_mae: 14.1947 - val_mse: 312.6992 - val_qwk: 0.4115\nEpoch 11/100\n57/57 - 3s - 61ms/step - loss: 299.3711 - mae: 13.7927 - mse: 299.3711 - qwk: 0.4178 - val_loss: 314.6974 - val_mae: 14.2190 - val_mse: 314.6974 - val_qwk: 0.4147\nEpoch 12/100\n57/57 - 4s - 62ms/step - loss: 293.8838 - mae: 13.6810 - mse: 293.8838 - qwk: 0.4341 - val_loss: 314.5938 - val_mae: 14.2052 - val_mse: 314.5938 - val_qwk: 0.4028\nEpoch 13/100\n57/57 - 4s - 63ms/step - loss: 292.4850 - mae: 13.5566 - mse: 292.4850 - qwk: 0.4362 - val_loss: 314.1861 - val_mae: 14.2097 - val_mse: 314.1861 - val_qwk: 0.4064\nEpoch 14/100\n57/57 - 4s - 63ms/step - loss: 293.8505 - mae: 13.6718 - mse: 293.8505 - qwk: 0.4357 - val_loss: 315.2572 - val_mae: 14.2493 - val_mse: 315.2572 - val_qwk: 0.4126\nEpoch 15/100\n57/57 - 3s - 61ms/step - loss: 294.0511 - mae: 13.6891 - mse: 294.0511 - qwk: 0.4429 - val_loss: 313.1117 - val_mae: 14.2058 - val_mse: 313.1117 - val_qwk: 0.4157\nEpoch 16/100\n57/57 - 4s - 65ms/step - loss: 289.8007 - mae: 13.5893 - mse: 289.8007 - qwk: 0.4488 - val_loss: 313.2843 - val_mae: 14.2140 - val_mse: 313.2843 - val_qwk: 0.4128\nEpoch 17/100\n57/57 - 4s - 65ms/step - loss: 292.4888 - mae: 13.6575 - mse: 292.4888 - qwk: 0.4401 - val_loss: 315.0567 - val_mae: 14.2719 - val_mse: 315.0567 - val_qwk: 0.4086\nEpoch 18/100\n57/57 - 4s - 64ms/step - loss: 283.6186 - mae: 13.3990 - mse: 283.6186 - qwk: 0.4591 - val_loss: 316.7162 - val_mae: 14.2664 - val_mse: 316.7162 - val_qwk: 0.4165\nEpoch 19/100\n57/57 - 4s - 62ms/step - loss: 283.4271 - mae: 13.4017 - mse: 283.4271 - qwk: 0.4614 - val_loss: 314.1787 - val_mae: 14.2261 - val_mse: 314.1787 - val_qwk: 0.4100\nEpoch 20/100\n57/57 - 4s - 61ms/step - loss: 291.5519 - mae: 13.6166 - mse: 291.5519 - qwk: 0.4373 - val_loss: 318.2734 - val_mae: 14.3152 - val_mse: 318.2734 - val_qwk: 0.4072\n29/29 - 1s - 19ms/step - loss: 312.6992 - mae: 14.1947 - mse: 312.6992 - qwk: 0.4042\n(1824, 34) (1824,) (912, 34) (912,)\n(3648, 34) (3648,) (912, 34) (912,)\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"57/57 - 4s - 64ms/step - loss: 895.6153 - mae: 23.7749 - mse: 895.6153 - qwk: 0.0646 - val_loss: 513.0127 - val_mae: 18.0062 - val_mse: 513.0127 - val_qwk: 0.1641\nEpoch 2/100\n57/57 - 4s - 64ms/step - loss: 462.5610 - mae: 17.0213 - mse: 462.5610 - qwk: 0.2618 - val_loss: 304.9149 - val_mae: 13.8764 - val_mse: 304.9149 - val_qwk: 0.3893\nEpoch 3/100\n57/57 - 4s - 65ms/step - loss: 365.8646 - mae: 15.2522 - mse: 365.8646 - qwk: 0.3613 - val_loss: 281.2878 - val_mae: 13.3819 - val_mse: 281.2878 - val_qwk: 0.4247\nEpoch 4/100\n57/57 - 4s - 63ms/step - loss: 351.6534 - mae: 15.0265 - mse: 351.6534 - qwk: 0.3771 - val_loss: 281.0340 - val_mae: 13.3591 - val_mse: 281.0340 - val_qwk: 0.4039\nEpoch 5/100\n57/57 - 4s - 62ms/step - loss: 332.1867 - mae: 14.5845 - mse: 332.1867 - qwk: 0.4113 - val_loss: 279.9257 - val_mae: 13.3035 - val_mse: 279.9257 - val_qwk: 0.4004\nEpoch 6/100\n57/57 - 4s - 63ms/step - loss: 332.7411 - mae: 14.5762 - mse: 332.7411 - qwk: 0.4036 - val_loss: 276.6351 - val_mae: 13.2790 - val_mse: 276.6351 - val_qwk: 0.4302\nEpoch 7/100\n57/57 - 4s - 62ms/step - loss: 328.6779 - mae: 14.4613 - mse: 328.6779 - qwk: 0.4046 - val_loss: 274.5875 - val_mae: 13.2089 - val_mse: 274.5875 - val_qwk: 0.4172\nEpoch 8/100\n57/57 - 4s - 62ms/step - loss: 325.8683 - mae: 14.4966 - mse: 325.8683 - qwk: 0.4030 - val_loss: 275.0509 - val_mae: 13.2385 - val_mse: 275.0509 - val_qwk: 0.4293\nEpoch 9/100\n57/57 - 4s - 64ms/step - loss: 322.8295 - mae: 14.3643 - mse: 322.8295 - qwk: 0.4236 - val_loss: 274.2522 - val_mae: 13.2025 - val_mse: 274.2522 - val_qwk: 0.4269\nEpoch 10/100\n57/57 - 4s - 63ms/step - loss: 318.9991 - mae: 14.2603 - mse: 318.9991 - qwk: 0.4227 - val_loss: 274.7600 - val_mae: 13.2259 - val_mse: 274.7600 - val_qwk: 0.4188\nEpoch 11/100\n57/57 - 4s - 65ms/step - loss: 324.3019 - mae: 14.4021 - mse: 324.3019 - qwk: 0.4126 - val_loss: 275.1070 - val_mae: 13.2540 - val_mse: 275.1070 - val_qwk: 0.4149\nEpoch 12/100\n57/57 - 4s - 63ms/step - loss: 314.6916 - mae: 14.1658 - mse: 314.6916 - qwk: 0.4305 - val_loss: 275.1132 - val_mae: 13.2304 - val_mse: 275.1132 - val_qwk: 0.4184\nEpoch 13/100\n57/57 - 4s - 65ms/step - loss: 316.8787 - mae: 14.2687 - mse: 316.8787 - qwk: 0.4266 - val_loss: 273.0191 - val_mae: 13.1994 - val_mse: 273.0191 - val_qwk: 0.4244\nEpoch 14/100\n57/57 - 4s - 65ms/step - loss: 313.7790 - mae: 14.1297 - mse: 313.7790 - qwk: 0.4367 - val_loss: 275.5848 - val_mae: 13.2592 - val_mse: 275.5848 - val_qwk: 0.4069\nEpoch 15/100\n57/57 - 4s - 63ms/step - loss: 312.5024 - mae: 14.1280 - mse: 312.5024 - qwk: 0.4422 - val_loss: 275.7450 - val_mae: 13.2315 - val_mse: 275.7450 - val_qwk: 0.4176\nEpoch 16/100\n57/57 - 4s - 64ms/step - loss: 310.5783 - mae: 14.1390 - mse: 310.5783 - qwk: 0.4481 - val_loss: 275.7125 - val_mae: 13.2614 - val_mse: 275.7125 - val_qwk: 0.4178\nEpoch 17/100\n57/57 - 4s - 63ms/step - loss: 313.4706 - mae: 14.1661 - mse: 313.4706 - qwk: 0.4383 - val_loss: 275.5770 - val_mae: 13.2608 - val_mse: 275.5770 - val_qwk: 0.4272\nEpoch 18/100\n57/57 - 4s - 64ms/step - loss: 311.3261 - mae: 14.0831 - mse: 311.3261 - qwk: 0.4459 - val_loss: 277.6389 - val_mae: 13.2927 - val_mse: 277.6389 - val_qwk: 0.4229\nEpoch 19/100\n57/57 - 4s - 62ms/step - loss: 307.5390 - mae: 14.0533 - mse: 307.5390 - qwk: 0.4517 - val_loss: 278.3111 - val_mae: 13.2662 - val_mse: 278.3111 - val_qwk: 0.4178\nEpoch 20/100\n57/57 - 4s - 68ms/step - loss: 307.0132 - mae: 14.0718 - mse: 307.0132 - qwk: 0.4515 - val_loss: 277.6205 - val_mae: 13.2991 - val_mse: 277.6205 - val_qwk: 0.4125\nEpoch 21/100\n57/57 - 4s - 66ms/step - loss: 308.4372 - mae: 14.0218 - mse: 308.4372 - qwk: 0.4441 - val_loss: 278.8413 - val_mae: 13.3506 - val_mse: 278.8413 - val_qwk: 0.4142\nEpoch 22/100\n57/57 - 4s - 66ms/step - loss: 302.6072 - mae: 13.9597 - mse: 302.6072 - qwk: 0.4596 - val_loss: 277.9416 - val_mae: 13.2982 - val_mse: 277.9416 - val_qwk: 0.4285\nEpoch 23/100\n57/57 - 4s - 63ms/step - loss: 310.3154 - mae: 14.1660 - mse: 310.3154 - qwk: 0.4430 - val_loss: 280.0088 - val_mae: 13.3418 - val_mse: 280.0088 - val_qwk: 0.4231\n29/29 - 1s - 18ms/step - loss: 273.0191 - mae: 13.1994 - mse: 273.0191 - qwk: 0.4167\n(1824, 34) (1824,) (912, 34) (912,)\n(3648, 34) (3648,) (912, 34) (912,)\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"57/57 - 4s - 64ms/step - loss: 710.7382 - mae: 21.1324 - mse: 710.7382 - qwk: 0.1302 - val_loss: 469.7377 - val_mae: 17.2835 - val_mse: 469.7377 - val_qwk: 0.2752\nEpoch 2/100\n57/57 - 4s - 63ms/step - loss: 420.1433 - mae: 16.1372 - mse: 420.1433 - qwk: 0.3008 - val_loss: 343.6344 - val_mae: 14.8401 - val_mse: 343.6344 - val_qwk: 0.3760\nEpoch 3/100\n57/57 - 4s - 64ms/step - loss: 355.8070 - mae: 14.9431 - mse: 355.8070 - qwk: 0.3573 - val_loss: 329.2273 - val_mae: 14.5956 - val_mse: 329.2273 - val_qwk: 0.3831\nEpoch 4/100\n57/57 - 4s - 63ms/step - loss: 336.7700 - mae: 14.5493 - mse: 336.7700 - qwk: 0.3675 - val_loss: 320.1608 - val_mae: 14.3850 - val_mse: 320.1608 - val_qwk: 0.4032\nEpoch 5/100\n57/57 - 4s - 62ms/step - loss: 311.5491 - mae: 13.9891 - mse: 311.5491 - qwk: 0.4125 - val_loss: 316.6977 - val_mae: 14.2872 - val_mse: 316.6977 - val_qwk: 0.4367\nEpoch 6/100\n57/57 - 4s - 64ms/step - loss: 309.6068 - mae: 14.0128 - mse: 309.6068 - qwk: 0.4172 - val_loss: 318.7800 - val_mae: 14.2702 - val_mse: 318.7800 - val_qwk: 0.4005\nEpoch 7/100\n57/57 - 4s - 64ms/step - loss: 308.8650 - mae: 13.9061 - mse: 308.8650 - qwk: 0.4141 - val_loss: 316.4743 - val_mae: 14.2475 - val_mse: 316.4743 - val_qwk: 0.4323\nEpoch 8/100\n57/57 - 4s - 63ms/step - loss: 306.4349 - mae: 13.9451 - mse: 306.4349 - qwk: 0.4099 - val_loss: 314.7869 - val_mae: 14.2371 - val_mse: 314.7869 - val_qwk: 0.4155\nEpoch 9/100\n57/57 - 4s - 62ms/step - loss: 306.8379 - mae: 13.9446 - mse: 306.8379 - qwk: 0.4067 - val_loss: 316.9609 - val_mae: 14.2650 - val_mse: 316.9609 - val_qwk: 0.4137\nEpoch 10/100\n57/57 - 4s - 63ms/step - loss: 300.0617 - mae: 13.7716 - mse: 300.0617 - qwk: 0.4280 - val_loss: 316.8178 - val_mae: 14.2471 - val_mse: 316.8178 - val_qwk: 0.4373\nEpoch 11/100\n57/57 - 4s - 64ms/step - loss: 299.4550 - mae: 13.7735 - mse: 299.4550 - qwk: 0.4307 - val_loss: 314.4766 - val_mae: 14.2213 - val_mse: 314.4766 - val_qwk: 0.4247\nEpoch 12/100\n57/57 - 4s - 62ms/step - loss: 300.3793 - mae: 13.8085 - mse: 300.3793 - qwk: 0.4240 - val_loss: 317.1660 - val_mae: 14.2305 - val_mse: 317.1660 - val_qwk: 0.4207\nEpoch 13/100\n57/57 - 4s - 64ms/step - loss: 298.1805 - mae: 13.6591 - mse: 298.1805 - qwk: 0.4285 - val_loss: 316.6467 - val_mae: 14.2430 - val_mse: 316.6467 - val_qwk: 0.4231\nEpoch 14/100\n57/57 - 4s - 65ms/step - loss: 292.6460 - mae: 13.5632 - mse: 292.6460 - qwk: 0.4408 - val_loss: 315.4810 - val_mae: 14.1956 - val_mse: 315.4810 - val_qwk: 0.4451\nEpoch 15/100\n57/57 - 4s - 66ms/step - loss: 288.4968 - mae: 13.4280 - mse: 288.4968 - qwk: 0.4495 - val_loss: 316.0149 - val_mae: 14.2438 - val_mse: 316.0149 - val_qwk: 0.4340\nEpoch 16/100\n57/57 - 4s - 64ms/step - loss: 288.9579 - mae: 13.4583 - mse: 288.9579 - qwk: 0.4476 - val_loss: 317.1076 - val_mae: 14.2508 - val_mse: 317.1076 - val_qwk: 0.4363\nEpoch 17/100\n57/57 - 4s - 65ms/step - loss: 292.3553 - mae: 13.5911 - mse: 292.3553 - qwk: 0.4445 - val_loss: 316.8515 - val_mae: 14.2562 - val_mse: 316.8515 - val_qwk: 0.4330\nEpoch 18/100\n57/57 - 4s - 64ms/step - loss: 284.9930 - mae: 13.4376 - mse: 284.9930 - qwk: 0.4600 - val_loss: 318.6124 - val_mae: 14.3255 - val_mse: 318.6124 - val_qwk: 0.4426\nEpoch 19/100\n57/57 - 4s - 65ms/step - loss: 291.3081 - mae: 13.5278 - mse: 291.3081 - qwk: 0.4458 - val_loss: 318.2527 - val_mae: 14.3111 - val_mse: 318.2527 - val_qwk: 0.4329\nEpoch 20/100\n57/57 - 4s - 66ms/step - loss: 285.8773 - mae: 13.4009 - mse: 285.8773 - qwk: 0.4538 - val_loss: 319.8341 - val_mae: 14.3285 - val_mse: 319.8341 - val_qwk: 0.4149\nEpoch 21/100\n57/57 - 4s - 64ms/step - loss: 284.8369 - mae: 13.3411 - mse: 284.8369 - qwk: 0.4528 - val_loss: 321.2278 - val_mae: 14.4021 - val_mse: 321.2278 - val_qwk: 0.4178\n29/29 - 1s - 18ms/step - loss: 314.4766 - mae: 14.2213 - mse: 314.4766 - qwk: 0.4150\nAverage Validation QWK: 0.41196342309316, Average Validation Loss: 300.0649719238281\n","output_type":"stream"}],"execution_count":77},{"cell_type":"markdown","source":"**Score: 0.375, 0.364**\n\nGot worse. Maybe we can go back to augmenting the data before the split","metadata":{}},{"cell_type":"markdown","source":"# V5: Applying PCA","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=0.95)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T01:53:05.888235Z","iopub.execute_input":"2025-01-17T01:53:05.888663Z","iopub.status.idle":"2025-01-17T01:53:05.893314Z","shell.execute_reply.started":"2025-01-17T01:53:05.888626Z","shell.execute_reply":"2025-01-17T01:53:05.892002Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"X_train_labelled_pca = pd.DataFrame(pca.fit_transform(X_train_labelled))\nX_train_labelled_pca.shape, X_train_labelled.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T01:53:06.926867Z","iopub.execute_input":"2025-01-17T01:53:06.927231Z","iopub.status.idle":"2025-01-17T01:53:06.941602Z","shell.execute_reply.started":"2025-01-17T01:53:06.927201Z","shell.execute_reply":"2025-01-17T01:53:06.940239Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"((2736, 18), (2736, 34))"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# Just 15 components can explain 90% of the variance of the labelled data? 18:95%\nX_test_fimpute_pca = pd.DataFrame(pca.transform(X_test_fimpute))\nX_test_fimpute_pca.shape, X_test_fimpute.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T01:53:43.068982Z","iopub.execute_input":"2025-01-17T01:53:43.069401Z","iopub.status.idle":"2025-01-17T01:53:43.081875Z","shell.execute_reply.started":"2025-01-17T01:53:43.069368Z","shell.execute_reply":"2025-01-17T01:53:43.080560Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"((20, 18), (20, 34))"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"def create_model_v5():\n    model = keras.models.Sequential([\n        keras.layers.Dense(24, input_shape=(X_train_labelled_pca.shape[1],), activation=\"relu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(12, activation=\"relu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(1, activation=\"linear\")\n    ])\n\n    model.compile(optimizer=Adam(learning_rate=0.005), loss='mean_squared_error', metrics=['mae','mse',qwk],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T03:13:23.427965Z","iopub.execute_input":"2025-01-17T03:13:23.428442Z","iopub.status.idle":"2025-01-17T03:13:23.435188Z","shell.execute_reply.started":"2025-01-17T03:13:23.428409Z","shell.execute_reply":"2025-01-17T03:13:23.434018Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"model = create_model_v5()\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T03:13:24.521362Z","iopub.execute_input":"2025-01-17T03:13:24.521817Z","iopub.status.idle":"2025-01-17T03:13:24.587995Z","shell.execute_reply.started":"2025-01-17T03:13:24.521779Z","shell.execute_reply":"2025-01-17T03:13:24.586900Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_28\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_28\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ dense_85 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)                  │             \u001b[38;5;34m456\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_56 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_86 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)                  │             \u001b[38;5;34m300\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_57 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_87 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m13\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ dense_85 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">456</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_86 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m769\u001b[0m (3.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">769</span> (3.00 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m769\u001b[0m (3.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">769</span> (3.00 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":69},{"cell_type":"code","source":"X_train_labelled_pca_aug, y_train_labelled_aug = aug_data(X_train_labelled_pca, y_train_labelled, 0.1, 0.1)\nX_train_labelled_pca_aug.shape, y_train_labelled_aug.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T03:13:32.258289Z","iopub.execute_input":"2025-01-17T03:13:32.258684Z","iopub.status.idle":"2025-01-17T03:13:32.271930Z","shell.execute_reply.started":"2025-01-17T03:13:32.258648Z","shell.execute_reply":"2025-01-17T03:13:32.270815Z"}},"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"((5472, 18), (5472,))"},"metadata":{}}],"execution_count":70},{"cell_type":"code","source":"'''\n# Prepare to store results\nvalidation_qwk = []\nvalidation_losses = []\nmodels = []\n\n# Loop over each fold\nfor train_index, val_index in kf.split(X_train_labelled_pca_aug):    \n    # Split the data into training and validation sets for the current fold\n    X_train_t, X_train_v = X_train_labelled_pca_aug.loc[train_index], X_train_labelled_pca_aug.loc[val_index]\n    y_train_t, y_train_v = y_train_labelled_aug.loc[train_index], y_train_labelled_aug.loc[val_index]\n\n    #print(X_train_t.shape, y_train_t.shape, X_train_v.shape, y_train_v.shape)\n    #X_train_t, y_train_t = aug_data(X_train_t, y_train_t, 0.1, 0.1)\n    #print(X_train_t.shape, y_train_t.shape, X_train_v.shape, y_train_v.shape)    \n    \n    # Build a new model for each fold\n    model = create_model_v5()\n    \n    # Define early stopping to avoid overfitting\n    #early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n    early_stopping = EarlyStopping(monitor='val_qwk', mode='max', patience=20, restore_best_weights=True)\n\n    \n    # Train the model on the training set and evaluate on the validation set\n    history = model.fit(\n        X_train_t, y_train_t,\n        validation_data=(X_train_v, y_train_v),\n        epochs=100,\n        batch_size=64,\n        callbacks=[early_stopping],\n        verbose=2\n    )\n    \n    # Evaluate the model on the validation set\n    val_loss, val_mae, val_mse, val_qwk = model.evaluate(X_train_v, y_train_v, verbose=2)\n    #val_qwk = model.evaluate(X_train_v, y_train_v, verbose=2)\n    validation_qwk.append(val_qwk)\n    validation_losses.append(val_loss)\n    models.append(model)\n\n# Calculate the average validation loss across all folds\navg_val_qwk = np.mean(validation_qwk)\navg_val_loss = np.mean(validation_losses)\nprint(f\"Average Validation QWK: {avg_val_qwk}, Average Validation Loss: {avg_val_loss}\")\n'''\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T04:03:44.245347Z","iopub.execute_input":"2025-01-17T04:03:44.245716Z","iopub.status.idle":"2025-01-17T04:03:44.333776Z","shell.execute_reply.started":"2025-01-17T04:03:44.245681Z","shell.execute_reply":"2025-01-17T04:03:44.332306Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-22723853830e>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Loop over each fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_labelled_pca_aug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Split the data into training and validation sets for the current fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mX_train_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_labelled_pca_aug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_labelled_pca_aug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'kf' is not defined"],"ename":"NameError","evalue":"name 'kf' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"**Score:**\n* early stopping with val_loss: **0.305, 0.348**\n* early stopping with val_qwk: **0.308, 0.342**","metadata":{}},{"cell_type":"markdown","source":"Thoughts:\n* Given small data, we tried augmenting once and used a smaller model to avoid overfitting. Probably 2 or max 3 hidden layers only\n* We tried PCA to allow for more neurons in our model without leading to too many params. In theory, I believe in PCA because 90% of variance was explained with just 15 features, and 95% with 18. So we should probably reduce from 34 since we can.\n* We tried varying earlystopping monitor to be val_qwk rather than val_loss\n* We may still try:\n    * Augmenting twice\n    * Different model architectures\n    * Different regularisation parameters: L1, L2, dropout\n    * Changing loss function to be more reflective of qwk\n    * Adding sample weights based on number of imputed features\n    * Including actigraph data\n    * Targeting sii instead of PCIAT-PCIAT_Total\n\nThe result of this version was so much worse, that for the next version we'll go back to the V3 architecture, data augmented once, no PCA, early stopping monitor val_loss, and we'll just carefully change one thing at a time","metadata":{}},{"cell_type":"markdown","source":"# V6","metadata":{}},{"cell_type":"code","source":"# Custom loss that combines MSE and QWK\ndef combined_loss(y_true, y_pred):\n    mse_loss = tf.reduce_mean(tf.square(y_true - y_pred))  # Mean Squared Error\n    #qwk_loss = tf.py_function(quadratic_weighted_kappa, [y_true, y_pred], tf.float64)  # QWK\n    qwk_loss = qwk(y_true, y_pred)\n    qwk_loss = tf.convert_to_tensor(qwk_loss, dtype=tf.float32)\n    # Add a weighting factor\n    #return mse_loss\n    #return (mse_loss, 10 * qwk_loss)\n    return mse_loss - (50 * qwk_loss) + 100 # Subtract qwk loss as higher qwk is better. Multiplier needs to be chosen by trial and error. Added constant so loss is > 0\n    #return (-1)*qwk_loss # Didn't work (no gradients provided) - seems it needs to be combined with nmse","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T08:05:18.375285Z","iopub.execute_input":"2025-01-17T08:05:18.375745Z","iopub.status.idle":"2025-01-17T08:05:18.381393Z","shell.execute_reply.started":"2025-01-17T08:05:18.375711Z","shell.execute_reply":"2025-01-17T08:05:18.379927Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"a = tf.constant([9.0,54.0,72.0])\nb = tf.constant([6.0,61.0,71.0])\nc = tf.constant([6.0,61.0,65.0])\n\nprint(combined_loss(a,b), combined_loss(a,c))\n#combined_loss(a,c)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T08:05:19.721118Z","iopub.execute_input":"2025-01-17T08:05:19.721486Z","iopub.status.idle":"2025-01-17T08:05:19.734112Z","shell.execute_reply.started":"2025-01-17T08:05:19.721456Z","shell.execute_reply":"2025-01-17T08:05:19.733029Z"}},"outputs":[{"name":"stdout","text":"tf.Tensor(70.57575, shape=(), dtype=float32) tf.Tensor(87.66667, shape=(), dtype=float32)\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"\ndef create_model_v6():\n    model = keras.models.Sequential([\n        keras.layers.Dense(20, input_shape=(X_train_labelled.shape[1],), activation=\"leaky_relu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(20, activation=\"leaky_relu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(1, activation=\"linear\")\n    ])\n\n    #model.compile(optimizer=Adam(learning_rate=0.005), loss='mean_squared_error', metrics=['mae','mse',qwk],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    model.compile(optimizer=Adam(learning_rate=0.005), loss=combined_loss, metrics=['mae','mse',qwk],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T08:19:11.755163Z","iopub.execute_input":"2025-01-17T08:19:11.755600Z","iopub.status.idle":"2025-01-17T08:19:11.762306Z","shell.execute_reply.started":"2025-01-17T08:19:11.755570Z","shell.execute_reply":"2025-01-17T08:19:11.761232Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"model = create_model_v6()\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T08:19:12.978648Z","iopub.execute_input":"2025-01-17T08:19:12.979130Z","iopub.status.idle":"2025-01-17T08:19:13.044048Z","shell.execute_reply.started":"2025-01-17T08:19:12.979098Z","shell.execute_reply":"2025-01-17T08:19:13.043131Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_13\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │             \u001b[38;5;34m700\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_40 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │             \u001b[38;5;34m420\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_27 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m21\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">700</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">420</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,141\u001b[0m (4.46 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,141</span> (4.46 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,141\u001b[0m (4.46 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,141</span> (4.46 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":83},{"cell_type":"code","source":"X_train_labelled_aug, y_train_labelled_aug = aug_data(X_train_labelled, y_train_labelled, 0.1, 0.1)\nX_train_labelled_aug.shape, y_train_labelled_aug.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T08:19:16.915568Z","iopub.execute_input":"2025-01-17T08:19:16.915973Z","iopub.status.idle":"2025-01-17T08:19:16.934441Z","shell.execute_reply.started":"2025-01-17T08:19:16.915932Z","shell.execute_reply":"2025-01-17T08:19:16.933410Z"}},"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"((5472, 34), (5472,))"},"metadata":{}}],"execution_count":84},{"cell_type":"code","source":"\n# Prepare to store results\nvalidation_qwk = []\nvalidation_losses = []\nmodels = []\n\n# Loop over each fold\nfor train_index, val_index in kf.split(X_train_labelled_aug):    \n    # Split the data into training and validation sets for the current fold\n    X_train_t, X_train_v = X_train_labelled_aug.loc[train_index], X_train_labelled_aug.loc[val_index]\n    y_train_t, y_train_v = y_train_labelled_aug.loc[train_index], y_train_labelled_aug.loc[val_index]\n\n    #print(X_train_t.shape, y_train_t.shape, X_train_v.shape, y_train_v.shape)\n    #X_train_t, y_train_t = aug_data(X_train_t, y_train_t, 0.1, 0.1)\n    #print(X_train_t.shape, y_train_t.shape, X_train_v.shape, y_train_v.shape)    \n    \n    # Build a new model for each fold\n    model = create_model_v6()\n    \n    # Define early stopping to avoid overfitting\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n    #early_stopping = EarlyStopping(monitor='val_qwk', mode='max', patience=20, restore_best_weights=True)\n\n    \n    # Train the model on the training set and evaluate on the validation set\n    history = model.fit(\n        X_train_t, y_train_t,\n        validation_data=(X_train_v, y_train_v),\n        epochs=100,\n        batch_size=64,\n        callbacks=[early_stopping],\n        verbose=2\n    )\n    \n    # Evaluate the model on the validation set\n    val_loss, val_mae, val_mse, val_qwk = model.evaluate(X_train_v, y_train_v, verbose=2)\n    #val_qwk = model.evaluate(X_train_v, y_train_v, verbose=2)\n    validation_qwk.append(val_qwk)\n    validation_losses.append(val_loss)\n    models.append(model)\n\n# Calculate the average validation loss across all folds\navg_val_qwk = np.mean(validation_qwk)\navg_val_loss = np.mean(validation_losses)\nprint(f\"Average Validation QWK: {avg_val_qwk}, Average Validation Loss: {avg_val_loss}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T08:19:22.785259Z","iopub.execute_input":"2025-01-17T08:19:22.785623Z","iopub.status.idle":"2025-01-17T08:26:48.728752Z","shell.execute_reply.started":"2025-01-17T08:19:22.785595Z","shell.execute_reply":"2025-01-17T08:26:48.727535Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"57/57 - 4s - 68ms/step - loss: 742.1578 - mae: 21.4545 - mse: 742.1578 - qwk: 0.0732 - val_loss: 441.1319 - val_mae: 16.8392 - val_mse: 441.1319 - val_qwk: 0.2165\nEpoch 2/100\n57/57 - 4s - 70ms/step - loss: 424.8094 - mae: 16.3014 - mse: 424.8094 - qwk: 0.2881 - val_loss: 352.0262 - val_mae: 14.9994 - val_mse: 352.0262 - val_qwk: 0.3675\nEpoch 3/100\n57/57 - 4s - 68ms/step - loss: 377.4055 - mae: 15.3486 - mse: 377.4055 - qwk: 0.3442 - val_loss: 336.1398 - val_mae: 14.7052 - val_mse: 336.1398 - val_qwk: 0.3628\nEpoch 4/100\n57/57 - 4s - 67ms/step - loss: 352.4036 - mae: 14.8379 - mse: 352.4036 - qwk: 0.3782 - val_loss: 324.8921 - val_mae: 14.5009 - val_mse: 324.8921 - val_qwk: 0.3894\nEpoch 5/100\n57/57 - 4s - 71ms/step - loss: 348.9647 - mae: 14.7912 - mse: 348.9647 - qwk: 0.3735 - val_loss: 319.3646 - val_mae: 14.4489 - val_mse: 319.3646 - val_qwk: 0.3938\nEpoch 6/100\n57/57 - 4s - 70ms/step - loss: 348.1029 - mae: 14.7029 - mse: 348.1029 - qwk: 0.3682 - val_loss: 318.3204 - val_mae: 14.4134 - val_mse: 318.3204 - val_qwk: 0.3867\nEpoch 7/100\n57/57 - 4s - 67ms/step - loss: 337.2635 - mae: 14.5253 - mse: 337.2635 - qwk: 0.3848 - val_loss: 314.8896 - val_mae: 14.4169 - val_mse: 314.8896 - val_qwk: 0.3851\nEpoch 8/100\n57/57 - 4s - 64ms/step - loss: 335.0242 - mae: 14.5201 - mse: 335.0242 - qwk: 0.3883 - val_loss: 313.6599 - val_mae: 14.3154 - val_mse: 313.6599 - val_qwk: 0.3979\nEpoch 9/100\n57/57 - 4s - 70ms/step - loss: 327.0955 - mae: 14.2943 - mse: 327.0955 - qwk: 0.4018 - val_loss: 311.3296 - val_mae: 14.2910 - val_mse: 311.3296 - val_qwk: 0.4037\nEpoch 10/100\n57/57 - 4s - 68ms/step - loss: 326.3655 - mae: 14.3294 - mse: 326.3655 - qwk: 0.3975 - val_loss: 311.2600 - val_mae: 14.2769 - val_mse: 311.2600 - val_qwk: 0.3890\nEpoch 11/100\n57/57 - 4s - 67ms/step - loss: 323.3740 - mae: 14.2238 - mse: 323.3740 - qwk: 0.4045 - val_loss: 310.2520 - val_mae: 14.2653 - val_mse: 310.2520 - val_qwk: 0.3934\nEpoch 12/100\n57/57 - 4s - 68ms/step - loss: 327.0407 - mae: 14.3142 - mse: 327.0407 - qwk: 0.3923 - val_loss: 309.9149 - val_mae: 14.2438 - val_mse: 309.9149 - val_qwk: 0.3974\nEpoch 13/100\n57/57 - 4s - 71ms/step - loss: 324.2442 - mae: 14.2797 - mse: 324.2442 - qwk: 0.3967 - val_loss: 310.1237 - val_mae: 14.2699 - val_mse: 310.1237 - val_qwk: 0.3880\nEpoch 14/100\n57/57 - 4s - 67ms/step - loss: 320.4415 - mae: 14.0972 - mse: 320.4415 - qwk: 0.4067 - val_loss: 311.3669 - val_mae: 14.2810 - val_mse: 311.3669 - val_qwk: 0.3894\nEpoch 15/100\n57/57 - 4s - 68ms/step - loss: 319.2749 - mae: 14.1283 - mse: 319.2749 - qwk: 0.4012 - val_loss: 311.0338 - val_mae: 14.2793 - val_mse: 311.0338 - val_qwk: 0.3854\nEpoch 16/100\n57/57 - 4s - 65ms/step - loss: 318.4885 - mae: 14.0950 - mse: 318.4885 - qwk: 0.4073 - val_loss: 308.7012 - val_mae: 14.2093 - val_mse: 308.7012 - val_qwk: 0.4026\nEpoch 17/100\n57/57 - 4s - 67ms/step - loss: 319.7383 - mae: 14.1947 - mse: 319.7383 - qwk: 0.4100 - val_loss: 307.8930 - val_mae: 14.2007 - val_mse: 307.8930 - val_qwk: 0.4013\nEpoch 18/100\n57/57 - 4s - 65ms/step - loss: 319.1398 - mae: 14.1294 - mse: 319.1398 - qwk: 0.3976 - val_loss: 310.7354 - val_mae: 14.2770 - val_mse: 310.7354 - val_qwk: 0.3842\nEpoch 19/100\n57/57 - 4s - 65ms/step - loss: 315.6935 - mae: 14.1138 - mse: 315.6935 - qwk: 0.4125 - val_loss: 308.3076 - val_mae: 14.2426 - val_mse: 308.3076 - val_qwk: 0.3971\nEpoch 20/100\n57/57 - 4s - 67ms/step - loss: 315.5416 - mae: 13.9849 - mse: 315.5416 - qwk: 0.4133 - val_loss: 309.9054 - val_mae: 14.2240 - val_mse: 309.9054 - val_qwk: 0.3932\nEpoch 21/100\n57/57 - 4s - 71ms/step - loss: 316.5820 - mae: 14.0206 - mse: 316.5820 - qwk: 0.3970 - val_loss: 308.1700 - val_mae: 14.2599 - val_mse: 308.1700 - val_qwk: 0.3996\nEpoch 22/100\n57/57 - 4s - 66ms/step - loss: 314.7537 - mae: 14.0356 - mse: 314.7537 - qwk: 0.4086 - val_loss: 306.9559 - val_mae: 14.1961 - val_mse: 306.9559 - val_qwk: 0.4060\nEpoch 23/100\n57/57 - 4s - 66ms/step - loss: 318.0148 - mae: 14.1542 - mse: 318.0148 - qwk: 0.3963 - val_loss: 309.9651 - val_mae: 14.2465 - val_mse: 309.9651 - val_qwk: 0.3903\nEpoch 24/100\n57/57 - 4s - 70ms/step - loss: 313.6253 - mae: 14.0089 - mse: 313.6253 - qwk: 0.4057 - val_loss: 308.9059 - val_mae: 14.2541 - val_mse: 308.9059 - val_qwk: 0.3970\nEpoch 25/100\n57/57 - 4s - 73ms/step - loss: 314.6252 - mae: 14.0334 - mse: 314.6252 - qwk: 0.4004 - val_loss: 310.6103 - val_mae: 14.3012 - val_mse: 310.6103 - val_qwk: 0.3808\nEpoch 26/100\n57/57 - 4s - 70ms/step - loss: 316.8651 - mae: 14.0660 - mse: 316.8651 - qwk: 0.3990 - val_loss: 310.0705 - val_mae: 14.2730 - val_mse: 310.0705 - val_qwk: 0.3924\nEpoch 27/100\n57/57 - 4s - 69ms/step - loss: 311.8698 - mae: 14.0657 - mse: 311.8698 - qwk: 0.4089 - val_loss: 310.2229 - val_mae: 14.2815 - val_mse: 310.2229 - val_qwk: 0.3845\nEpoch 28/100\n57/57 - 4s - 70ms/step - loss: 310.5213 - mae: 13.9549 - mse: 310.5213 - qwk: 0.4158 - val_loss: 308.7824 - val_mae: 14.2579 - val_mse: 308.7824 - val_qwk: 0.3890\nEpoch 29/100\n57/57 - 4s - 73ms/step - loss: 310.4059 - mae: 14.0008 - mse: 310.4059 - qwk: 0.4108 - val_loss: 309.3414 - val_mae: 14.2859 - val_mse: 309.3414 - val_qwk: 0.3927\nEpoch 30/100\n57/57 - 4s - 67ms/step - loss: 310.7310 - mae: 13.9326 - mse: 310.7310 - qwk: 0.4006 - val_loss: 308.0330 - val_mae: 14.2446 - val_mse: 308.0330 - val_qwk: 0.3916\nEpoch 31/100\n57/57 - 4s - 70ms/step - loss: 305.7305 - mae: 13.8675 - mse: 305.7305 - qwk: 0.4262 - val_loss: 308.9894 - val_mae: 14.2604 - val_mse: 308.9894 - val_qwk: 0.3843\nEpoch 32/100\n57/57 - 4s - 71ms/step - loss: 306.4406 - mae: 13.8605 - mse: 306.4406 - qwk: 0.4245 - val_loss: 308.8093 - val_mae: 14.2606 - val_mse: 308.8093 - val_qwk: 0.3866\n57/57 - 1s - 19ms/step - loss: 306.9559 - mae: 14.1961 - mse: 306.9559 - qwk: 0.4025\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"57/57 - 5s - 80ms/step - loss: 764.6979 - mae: 21.7991 - mse: 764.6979 - qwk: 0.0993 - val_loss: 445.2636 - val_mae: 16.8562 - val_mse: 445.2636 - val_qwk: 0.1891\nEpoch 2/100\n57/57 - 4s - 69ms/step - loss: 444.0386 - mae: 16.7292 - mse: 444.0386 - qwk: 0.2788 - val_loss: 343.4548 - val_mae: 14.7467 - val_mse: 343.4548 - val_qwk: 0.3397\nEpoch 3/100\n57/57 - 4s - 70ms/step - loss: 388.6115 - mae: 15.5764 - mse: 388.6115 - qwk: 0.3413 - val_loss: 324.8168 - val_mae: 14.3099 - val_mse: 324.8168 - val_qwk: 0.3896\nEpoch 4/100\n57/57 - 4s - 69ms/step - loss: 374.2978 - mae: 15.3408 - mse: 374.2978 - qwk: 0.3492 - val_loss: 315.3692 - val_mae: 14.0786 - val_mse: 315.3692 - val_qwk: 0.4065\nEpoch 5/100\n57/57 - 4s - 68ms/step - loss: 360.6389 - mae: 15.1819 - mse: 360.6389 - qwk: 0.3634 - val_loss: 311.0690 - val_mae: 13.9706 - val_mse: 311.0690 - val_qwk: 0.4291\nEpoch 6/100\n57/57 - 4s - 66ms/step - loss: 350.0741 - mae: 14.8894 - mse: 350.0741 - qwk: 0.3815 - val_loss: 302.9891 - val_mae: 13.7934 - val_mse: 302.9891 - val_qwk: 0.4208\nEpoch 7/100\n57/57 - 4s - 67ms/step - loss: 342.2919 - mae: 14.7856 - mse: 342.2919 - qwk: 0.3850 - val_loss: 303.4045 - val_mae: 13.7877 - val_mse: 303.4045 - val_qwk: 0.4355\nEpoch 8/100\n57/57 - 4s - 69ms/step - loss: 338.0605 - mae: 14.6797 - mse: 338.0605 - qwk: 0.3922 - val_loss: 302.2310 - val_mae: 13.7298 - val_mse: 302.2310 - val_qwk: 0.4162\nEpoch 9/100\n57/57 - 4s - 65ms/step - loss: 337.0913 - mae: 14.7725 - mse: 337.0913 - qwk: 0.3960 - val_loss: 300.2282 - val_mae: 13.6904 - val_mse: 300.2282 - val_qwk: 0.4207\nEpoch 10/100\n57/57 - 4s - 67ms/step - loss: 340.6068 - mae: 14.7923 - mse: 340.6068 - qwk: 0.3842 - val_loss: 299.6041 - val_mae: 13.6648 - val_mse: 299.6041 - val_qwk: 0.4115\nEpoch 11/100\n57/57 - 4s - 70ms/step - loss: 331.9560 - mae: 14.5850 - mse: 331.9560 - qwk: 0.3869 - val_loss: 297.6471 - val_mae: 13.6412 - val_mse: 297.6471 - val_qwk: 0.4205\nEpoch 12/100\n57/57 - 4s - 71ms/step - loss: 335.0134 - mae: 14.6098 - mse: 335.0134 - qwk: 0.3883 - val_loss: 298.6200 - val_mae: 13.6782 - val_mse: 298.6200 - val_qwk: 0.4321\nEpoch 13/100\n57/57 - 4s - 67ms/step - loss: 329.8911 - mae: 14.4566 - mse: 329.8911 - qwk: 0.3983 - val_loss: 297.7499 - val_mae: 13.6425 - val_mse: 297.7499 - val_qwk: 0.4251\nEpoch 14/100\n57/57 - 4s - 66ms/step - loss: 336.4422 - mae: 14.6835 - mse: 336.4422 - qwk: 0.3827 - val_loss: 297.7557 - val_mae: 13.6561 - val_mse: 297.7557 - val_qwk: 0.4265\nEpoch 15/100\n57/57 - 4s - 70ms/step - loss: 326.5020 - mae: 14.4774 - mse: 326.5020 - qwk: 0.3847 - val_loss: 297.2346 - val_mae: 13.6471 - val_mse: 297.2346 - val_qwk: 0.4347\nEpoch 16/100\n57/57 - 4s - 68ms/step - loss: 326.7595 - mae: 14.4622 - mse: 326.7595 - qwk: 0.3964 - val_loss: 297.4000 - val_mae: 13.6421 - val_mse: 297.4000 - val_qwk: 0.4229\nEpoch 17/100\n57/57 - 4s - 66ms/step - loss: 333.4999 - mae: 14.6421 - mse: 333.4999 - qwk: 0.3874 - val_loss: 297.0876 - val_mae: 13.6115 - val_mse: 297.0876 - val_qwk: 0.4382\nEpoch 18/100\n57/57 - 4s - 68ms/step - loss: 320.0623 - mae: 14.3155 - mse: 320.0623 - qwk: 0.4059 - val_loss: 294.9409 - val_mae: 13.5919 - val_mse: 294.9409 - val_qwk: 0.4291\nEpoch 19/100\n57/57 - 4s - 68ms/step - loss: 326.7759 - mae: 14.4563 - mse: 326.7759 - qwk: 0.3954 - val_loss: 296.1057 - val_mae: 13.5850 - val_mse: 296.1057 - val_qwk: 0.4225\nEpoch 20/100\n57/57 - 4s - 67ms/step - loss: 321.3755 - mae: 14.3813 - mse: 321.3755 - qwk: 0.4061 - val_loss: 294.9172 - val_mae: 13.5446 - val_mse: 294.9172 - val_qwk: 0.4220\nEpoch 21/100\n57/57 - 4s - 67ms/step - loss: 319.1941 - mae: 14.3635 - mse: 319.1941 - qwk: 0.4097 - val_loss: 296.7673 - val_mae: 13.6023 - val_mse: 296.7673 - val_qwk: 0.4284\nEpoch 22/100\n57/57 - 4s - 64ms/step - loss: 320.1176 - mae: 14.4039 - mse: 320.1176 - qwk: 0.4012 - val_loss: 299.1154 - val_mae: 13.6318 - val_mse: 299.1154 - val_qwk: 0.4316\nEpoch 23/100\n57/57 - 4s - 67ms/step - loss: 319.1147 - mae: 14.3071 - mse: 319.1147 - qwk: 0.4087 - val_loss: 294.7705 - val_mae: 13.5652 - val_mse: 294.7705 - val_qwk: 0.4281\nEpoch 24/100\n57/57 - 4s - 67ms/step - loss: 318.7244 - mae: 14.2681 - mse: 318.7244 - qwk: 0.4144 - val_loss: 295.8304 - val_mae: 13.5698 - val_mse: 295.8304 - val_qwk: 0.4293\nEpoch 25/100\n57/57 - 4s - 66ms/step - loss: 318.1195 - mae: 14.2438 - mse: 318.1195 - qwk: 0.4067 - val_loss: 296.6197 - val_mae: 13.5761 - val_mse: 296.6197 - val_qwk: 0.4236\nEpoch 26/100\n57/57 - 4s - 66ms/step - loss: 318.9550 - mae: 14.2996 - mse: 318.9550 - qwk: 0.4087 - val_loss: 297.4094 - val_mae: 13.6146 - val_mse: 297.4094 - val_qwk: 0.4372\nEpoch 27/100\n57/57 - 4s - 66ms/step - loss: 318.5198 - mae: 14.2878 - mse: 318.5198 - qwk: 0.4069 - val_loss: 296.0506 - val_mae: 13.5672 - val_mse: 296.0506 - val_qwk: 0.4275\nEpoch 28/100\n57/57 - 4s - 65ms/step - loss: 315.8607 - mae: 14.2660 - mse: 315.8607 - qwk: 0.4109 - val_loss: 294.9331 - val_mae: 13.5661 - val_mse: 294.9331 - val_qwk: 0.4326\nEpoch 29/100\n57/57 - 4s - 66ms/step - loss: 314.2899 - mae: 14.2166 - mse: 314.2899 - qwk: 0.4139 - val_loss: 294.5485 - val_mae: 13.5753 - val_mse: 294.5485 - val_qwk: 0.4168\nEpoch 30/100\n57/57 - 4s - 65ms/step - loss: 318.4075 - mae: 14.3180 - mse: 318.4075 - qwk: 0.4126 - val_loss: 295.1175 - val_mae: 13.5681 - val_mse: 295.1175 - val_qwk: 0.4216\nEpoch 31/100\n57/57 - 4s - 64ms/step - loss: 314.7730 - mae: 14.2526 - mse: 314.7730 - qwk: 0.4121 - val_loss: 294.8796 - val_mae: 13.5418 - val_mse: 294.8796 - val_qwk: 0.4208\nEpoch 32/100\n57/57 - 4s - 65ms/step - loss: 319.4863 - mae: 14.2869 - mse: 319.4863 - qwk: 0.4018 - val_loss: 293.7023 - val_mae: 13.5558 - val_mse: 293.7023 - val_qwk: 0.4143\nEpoch 33/100\n57/57 - 4s - 65ms/step - loss: 314.3659 - mae: 14.2358 - mse: 314.3659 - qwk: 0.4093 - val_loss: 295.9913 - val_mae: 13.5878 - val_mse: 295.9913 - val_qwk: 0.4370\nEpoch 34/100\n57/57 - 4s - 69ms/step - loss: 316.1198 - mae: 14.1996 - mse: 316.1198 - qwk: 0.4102 - val_loss: 295.0981 - val_mae: 13.5564 - val_mse: 295.0981 - val_qwk: 0.4289\nEpoch 35/100\n57/57 - 4s - 69ms/step - loss: 315.2747 - mae: 14.2294 - mse: 315.2747 - qwk: 0.4129 - val_loss: 294.1536 - val_mae: 13.5692 - val_mse: 294.1536 - val_qwk: 0.4203\nEpoch 36/100\n57/57 - 4s - 67ms/step - loss: 313.5901 - mae: 14.1716 - mse: 313.5901 - qwk: 0.4165 - val_loss: 295.0805 - val_mae: 13.5579 - val_mse: 295.0805 - val_qwk: 0.4285\nEpoch 37/100\n57/57 - 4s - 64ms/step - loss: 312.5994 - mae: 14.1267 - mse: 312.5994 - qwk: 0.4163 - val_loss: 294.0327 - val_mae: 13.5380 - val_mse: 294.0327 - val_qwk: 0.4314\nEpoch 38/100\n57/57 - 4s - 66ms/step - loss: 317.5720 - mae: 14.3066 - mse: 317.5720 - qwk: 0.4074 - val_loss: 294.8198 - val_mae: 13.5610 - val_mse: 294.8198 - val_qwk: 0.4354\nEpoch 39/100\n57/57 - 4s - 65ms/step - loss: 310.8410 - mae: 14.1243 - mse: 310.8410 - qwk: 0.4248 - val_loss: 294.8648 - val_mae: 13.5795 - val_mse: 294.8648 - val_qwk: 0.4447\nEpoch 40/100\n57/57 - 4s - 68ms/step - loss: 315.9785 - mae: 14.2251 - mse: 315.9785 - qwk: 0.4067 - val_loss: 295.8573 - val_mae: 13.6241 - val_mse: 295.8573 - val_qwk: 0.4331\nEpoch 41/100\n57/57 - 4s - 63ms/step - loss: 308.3888 - mae: 13.9697 - mse: 308.3888 - qwk: 0.4338 - val_loss: 295.2336 - val_mae: 13.5647 - val_mse: 295.2336 - val_qwk: 0.4359\nEpoch 42/100\n57/57 - 4s - 66ms/step - loss: 314.1247 - mae: 14.2172 - mse: 314.1247 - qwk: 0.4113 - val_loss: 294.1135 - val_mae: 13.5589 - val_mse: 294.1135 - val_qwk: 0.4202\n57/57 - 1s - 18ms/step - loss: 293.7023 - mae: 13.5558 - mse: 293.7023 - qwk: 0.4048\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"57/57 - 4s - 67ms/step - loss: 901.9189 - mae: 23.7681 - mse: 901.9189 - qwk: 0.0796 - val_loss: 533.9165 - val_mae: 18.3367 - val_mse: 533.9165 - val_qwk: 0.2087\nEpoch 2/100\n57/57 - 4s - 64ms/step - loss: 468.0681 - mae: 16.9938 - mse: 468.0681 - qwk: 0.2666 - val_loss: 370.1227 - val_mae: 15.3357 - val_mse: 370.1227 - val_qwk: 0.3395\nEpoch 3/100\n57/57 - 4s - 64ms/step - loss: 393.3516 - mae: 15.6580 - mse: 393.3516 - qwk: 0.3295 - val_loss: 337.5006 - val_mae: 14.6910 - val_mse: 337.5006 - val_qwk: 0.3842\nEpoch 4/100\n57/57 - 4s - 66ms/step - loss: 371.0048 - mae: 15.2757 - mse: 371.0048 - qwk: 0.3437 - val_loss: 328.1068 - val_mae: 14.4059 - val_mse: 328.1068 - val_qwk: 0.3910\nEpoch 5/100\n57/57 - 4s - 64ms/step - loss: 349.5600 - mae: 14.8324 - mse: 349.5600 - qwk: 0.3812 - val_loss: 319.8004 - val_mae: 14.2388 - val_mse: 319.8004 - val_qwk: 0.3853\nEpoch 6/100\n57/57 - 4s - 64ms/step - loss: 347.7215 - mae: 14.8240 - mse: 347.7215 - qwk: 0.3750 - val_loss: 314.5203 - val_mae: 14.1270 - val_mse: 314.5203 - val_qwk: 0.3985\nEpoch 7/100\n57/57 - 4s - 64ms/step - loss: 339.2355 - mae: 14.6063 - mse: 339.2355 - qwk: 0.3846 - val_loss: 312.8722 - val_mae: 14.0776 - val_mse: 312.8722 - val_qwk: 0.3965\nEpoch 8/100\n57/57 - 4s - 65ms/step - loss: 335.6602 - mae: 14.6233 - mse: 335.6602 - qwk: 0.3875 - val_loss: 313.3080 - val_mae: 14.1080 - val_mse: 313.3080 - val_qwk: 0.3860\nEpoch 9/100\n57/57 - 4s - 67ms/step - loss: 326.0553 - mae: 14.3332 - mse: 326.0553 - qwk: 0.4096 - val_loss: 311.6034 - val_mae: 14.0498 - val_mse: 311.6034 - val_qwk: 0.3881\nEpoch 10/100\n57/57 - 4s - 65ms/step - loss: 325.0533 - mae: 14.3329 - mse: 325.0533 - qwk: 0.4081 - val_loss: 309.1663 - val_mae: 14.0208 - val_mse: 309.1663 - val_qwk: 0.4037\nEpoch 11/100\n57/57 - 4s - 66ms/step - loss: 326.0905 - mae: 14.3492 - mse: 326.0905 - qwk: 0.3980 - val_loss: 307.7952 - val_mae: 13.9984 - val_mse: 307.7952 - val_qwk: 0.4218\nEpoch 12/100\n57/57 - 4s - 67ms/step - loss: 331.6871 - mae: 14.5120 - mse: 331.6871 - qwk: 0.4016 - val_loss: 307.9153 - val_mae: 14.0060 - val_mse: 307.9153 - val_qwk: 0.4096\nEpoch 13/100\n57/57 - 4s - 68ms/step - loss: 324.1593 - mae: 14.3424 - mse: 324.1593 - qwk: 0.4085 - val_loss: 306.9317 - val_mae: 13.9813 - val_mse: 306.9317 - val_qwk: 0.4176\nEpoch 14/100\n57/57 - 4s - 67ms/step - loss: 326.3488 - mae: 14.4334 - mse: 326.3488 - qwk: 0.4029 - val_loss: 306.5605 - val_mae: 13.9723 - val_mse: 306.5605 - val_qwk: 0.4035\nEpoch 15/100\n57/57 - 4s - 66ms/step - loss: 325.3640 - mae: 14.3244 - mse: 325.3640 - qwk: 0.3921 - val_loss: 306.6387 - val_mae: 13.9858 - val_mse: 306.6387 - val_qwk: 0.4077\nEpoch 16/100\n57/57 - 4s - 67ms/step - loss: 318.1210 - mae: 14.2510 - mse: 318.1210 - qwk: 0.4161 - val_loss: 306.3056 - val_mae: 13.9757 - val_mse: 306.3056 - val_qwk: 0.4044\nEpoch 17/100\n57/57 - 4s - 71ms/step - loss: 320.2887 - mae: 14.3648 - mse: 320.2887 - qwk: 0.4072 - val_loss: 307.4391 - val_mae: 13.9533 - val_mse: 307.4391 - val_qwk: 0.4069\nEpoch 18/100\n57/57 - 4s - 65ms/step - loss: 316.4840 - mae: 14.2346 - mse: 316.4840 - qwk: 0.4093 - val_loss: 305.7551 - val_mae: 13.9420 - val_mse: 305.7551 - val_qwk: 0.4157\nEpoch 19/100\n57/57 - 4s - 66ms/step - loss: 316.4866 - mae: 14.2745 - mse: 316.4866 - qwk: 0.4116 - val_loss: 305.8309 - val_mae: 13.9553 - val_mse: 305.8309 - val_qwk: 0.4187\nEpoch 20/100\n57/57 - 4s - 69ms/step - loss: 318.6418 - mae: 14.2266 - mse: 318.6418 - qwk: 0.4110 - val_loss: 306.7278 - val_mae: 13.9515 - val_mse: 306.7278 - val_qwk: 0.4056\nEpoch 21/100\n57/57 - 4s - 66ms/step - loss: 321.7103 - mae: 14.3084 - mse: 321.7103 - qwk: 0.4059 - val_loss: 308.2532 - val_mae: 13.9478 - val_mse: 308.2532 - val_qwk: 0.3975\nEpoch 22/100\n57/57 - 4s - 66ms/step - loss: 322.7198 - mae: 14.4069 - mse: 322.7198 - qwk: 0.4013 - val_loss: 305.3755 - val_mae: 13.9049 - val_mse: 305.3755 - val_qwk: 0.4133\nEpoch 23/100\n57/57 - 4s - 73ms/step - loss: 317.2361 - mae: 14.1977 - mse: 317.2361 - qwk: 0.4114 - val_loss: 304.9623 - val_mae: 13.9180 - val_mse: 304.9623 - val_qwk: 0.4181\nEpoch 24/100\n57/57 - 4s - 69ms/step - loss: 317.9518 - mae: 14.2562 - mse: 317.9518 - qwk: 0.4073 - val_loss: 305.0500 - val_mae: 13.9318 - val_mse: 305.0500 - val_qwk: 0.4282\nEpoch 25/100\n57/57 - 4s - 67ms/step - loss: 320.5145 - mae: 14.2894 - mse: 320.5145 - qwk: 0.4070 - val_loss: 304.4043 - val_mae: 13.9308 - val_mse: 304.4043 - val_qwk: 0.4172\nEpoch 26/100\n57/57 - 4s - 70ms/step - loss: 315.0492 - mae: 14.1440 - mse: 315.0492 - qwk: 0.4115 - val_loss: 305.1619 - val_mae: 13.9339 - val_mse: 305.1619 - val_qwk: 0.4191\nEpoch 27/100\n57/57 - 4s - 69ms/step - loss: 319.9883 - mae: 14.2701 - mse: 319.9883 - qwk: 0.4001 - val_loss: 305.4102 - val_mae: 13.9176 - val_mse: 305.4102 - val_qwk: 0.4084\nEpoch 28/100\n57/57 - 4s - 69ms/step - loss: 317.3063 - mae: 14.2600 - mse: 317.3063 - qwk: 0.4146 - val_loss: 304.2446 - val_mae: 13.9056 - val_mse: 304.2446 - val_qwk: 0.4172\nEpoch 29/100\n57/57 - 4s - 70ms/step - loss: 310.8706 - mae: 14.0836 - mse: 310.8706 - qwk: 0.4229 - val_loss: 304.5262 - val_mae: 13.9080 - val_mse: 304.5262 - val_qwk: 0.4147\nEpoch 30/100\n57/57 - 4s - 65ms/step - loss: 312.2898 - mae: 14.1281 - mse: 312.2898 - qwk: 0.4208 - val_loss: 304.3634 - val_mae: 13.9290 - val_mse: 304.3634 - val_qwk: 0.4152\nEpoch 31/100\n57/57 - 4s - 65ms/step - loss: 311.8734 - mae: 14.1391 - mse: 311.8734 - qwk: 0.4104 - val_loss: 304.1732 - val_mae: 13.9226 - val_mse: 304.1732 - val_qwk: 0.4197\nEpoch 32/100\n57/57 - 4s - 65ms/step - loss: 316.9272 - mae: 14.2554 - mse: 316.9272 - qwk: 0.4009 - val_loss: 304.1988 - val_mae: 13.9104 - val_mse: 304.1988 - val_qwk: 0.4161\nEpoch 33/100\n57/57 - 4s - 67ms/step - loss: 305.1107 - mae: 13.9320 - mse: 305.1107 - qwk: 0.4377 - val_loss: 304.3306 - val_mae: 13.9174 - val_mse: 304.3306 - val_qwk: 0.4212\nEpoch 34/100\n57/57 - 4s - 65ms/step - loss: 314.9258 - mae: 14.1566 - mse: 314.9258 - qwk: 0.4107 - val_loss: 305.1008 - val_mae: 13.9483 - val_mse: 305.1008 - val_qwk: 0.4132\nEpoch 35/100\n57/57 - 4s - 65ms/step - loss: 312.7732 - mae: 14.0427 - mse: 312.7732 - qwk: 0.4155 - val_loss: 305.2180 - val_mae: 13.9251 - val_mse: 305.2180 - val_qwk: 0.4141\nEpoch 36/100\n57/57 - 4s - 68ms/step - loss: 313.1989 - mae: 14.1778 - mse: 313.1989 - qwk: 0.4156 - val_loss: 304.5780 - val_mae: 13.9158 - val_mse: 304.5780 - val_qwk: 0.4127\nEpoch 37/100\n57/57 - 4s - 68ms/step - loss: 314.1210 - mae: 14.2147 - mse: 314.1210 - qwk: 0.4077 - val_loss: 305.0000 - val_mae: 13.9374 - val_mse: 305.0000 - val_qwk: 0.4133\nEpoch 38/100\n57/57 - 4s - 64ms/step - loss: 312.4332 - mae: 14.1009 - mse: 312.4332 - qwk: 0.4187 - val_loss: 305.5077 - val_mae: 13.9369 - val_mse: 305.5077 - val_qwk: 0.4068\nEpoch 39/100\n57/57 - 4s - 66ms/step - loss: 312.2110 - mae: 14.1779 - mse: 312.2110 - qwk: 0.4167 - val_loss: 307.1364 - val_mae: 13.9610 - val_mse: 307.1364 - val_qwk: 0.3969\nEpoch 40/100\n57/57 - 4s - 66ms/step - loss: 308.5789 - mae: 14.0711 - mse: 308.5789 - qwk: 0.4312 - val_loss: 305.2934 - val_mae: 13.9273 - val_mse: 305.2934 - val_qwk: 0.4151\nEpoch 41/100\n57/57 - 4s - 65ms/step - loss: 312.9389 - mae: 14.1270 - mse: 312.9389 - qwk: 0.4197 - val_loss: 306.1735 - val_mae: 13.9489 - val_mse: 306.1735 - val_qwk: 0.4129\n57/57 - 1s - 17ms/step - loss: 304.1732 - mae: 13.9226 - mse: 304.1732 - qwk: 0.4182\nAverage Validation QWK: 0.4085325300693512, Average Validation Loss: 301.61049397786456\n","output_type":"stream"}],"execution_count":85},{"cell_type":"markdown","source":"# Final prediction with model","metadata":{}},{"cell_type":"code","source":"y_test = test_data[['id']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T08:29:17.814489Z","iopub.execute_input":"2025-01-17T08:29:17.814941Z","iopub.status.idle":"2025-01-17T08:29:17.821285Z","shell.execute_reply.started":"2025-01-17T08:29:17.814910Z","shell.execute_reply":"2025-01-17T08:29:17.820173Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"y_test['PCIAT-PCIAT_Total_dnn'] = np.mean([model.predict(X_test_fimpute) for model in models], axis=0)\n#y_test['PCIAT-PCIAT_Total_dnn'] = np.mean([model.predict(X_test_fimpute_pca) for model in models], axis=0)\n#y_test['PCIAT-PCIAT_Total_dnn'] = model.predict(X_test_fimpute)\ny_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T08:29:39.549491Z","iopub.execute_input":"2025-01-17T08:29:39.549889Z","iopub.status.idle":"2025-01-17T08:29:39.861219Z","shell.execute_reply.started":"2025-01-17T08:29:39.549858Z","shell.execute_reply":"2025-01-17T08:29:39.859724Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-88-56e40812541d>:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  y_test['PCIAT-PCIAT_Total_dnn'] = np.mean([model.predict(X_test_fimpute) for model in models], axis=0)\n","output_type":"stream"},{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"         id  PCIAT-PCIAT_Total_dnn\n0  00008ff9              28.629999\n1  000fd460              14.883449\n2  00105258              31.581049\n3  00115b9f              23.590620\n4  0016bb22              39.803635","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>PCIAT-PCIAT_Total_dnn</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>28.629999</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>14.883449</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>31.581049</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>23.590620</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>39.803635</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":88},{"cell_type":"code","source":"y_test['PCIAT-PCIAT_Total'] = y_test.apply(\n    lambda row: (row['PCIAT-PCIAT_Total_dnn']),\naxis=1)\n\ny_test['sii'] = y_test.apply(lambda row: 0 if row['PCIAT-PCIAT_Total']<=30 else \n                             (1 if row['PCIAT-PCIAT_Total']<50 else (\n                                2 if row['PCIAT-PCIAT_Total']<80 else (3)\n                            )), axis=1)\n\ny_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T08:29:45.790026Z","iopub.execute_input":"2025-01-17T08:29:45.790399Z","iopub.status.idle":"2025-01-17T08:29:45.808515Z","shell.execute_reply.started":"2025-01-17T08:29:45.790369Z","shell.execute_reply":"2025-01-17T08:29:45.807297Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-89-9d1fb4b2c713>:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  y_test['PCIAT-PCIAT_Total'] = y_test.apply(\n<ipython-input-89-9d1fb4b2c713>:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  y_test['sii'] = y_test.apply(lambda row: 0 if row['PCIAT-PCIAT_Total']<=30 else\n","output_type":"stream"},{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"         id  PCIAT-PCIAT_Total_dnn  PCIAT-PCIAT_Total  sii\n0  00008ff9              28.629999          28.629999    0\n1  000fd460              14.883449          14.883449    0\n2  00105258              31.581049          31.581049    1\n3  00115b9f              23.590620          23.590620    0\n4  0016bb22              39.803635          39.803635    1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>PCIAT-PCIAT_Total_dnn</th>\n      <th>PCIAT-PCIAT_Total</th>\n      <th>sii</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>28.629999</td>\n      <td>28.629999</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>14.883449</td>\n      <td>14.883449</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>31.581049</td>\n      <td>31.581049</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>23.590620</td>\n      <td>23.590620</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>39.803635</td>\n      <td>39.803635</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":89},{"cell_type":"code","source":"solution = y_test[['id','sii']]\nsolution.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T08:29:48.524106Z","iopub.execute_input":"2025-01-17T08:29:48.524451Z","iopub.status.idle":"2025-01-17T08:29:48.537942Z","shell.execute_reply.started":"2025-01-17T08:29:48.524424Z","shell.execute_reply":"2025-01-17T08:29:48.536781Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}