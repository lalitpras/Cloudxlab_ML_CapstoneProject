{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Continuing previous DNN model optimisation. Now incorporating learnings from working on XGBoost model, e.g., that the QWK score on sii can be the best loss function.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\n\nimport polars as pl\nfrom glob import glob\nfrom tqdm.auto import tqdm\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport random\n\nnp.random.seed(42)\ntf.random.set_seed(42)\nrandom.seed(42)\n\nfrom sklearn.impute import KNNImputer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom tensorflow.keras import layers, models, regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\nfrom sklearn.svm import SVR\n\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import cohen_kappa_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:27:40.655464Z","iopub.execute_input":"2025-01-26T02:27:40.655882Z","iopub.status.idle":"2025-01-26T02:28:00.373402Z","shell.execute_reply.started":"2025-01-26T02:27:40.655855Z","shell.execute_reply":"2025-01-26T02:28:00.372330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data=pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest_data = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:28:00.374822Z","iopub.execute_input":"2025-01-26T02:28:00.375621Z","iopub.status.idle":"2025-01-26T02:28:00.462098Z","shell.execute_reply.started":"2025-01-26T02:28:00.375584Z","shell.execute_reply":"2025-01-26T02:28:00.461088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"INPUT_DIR = \"/kaggle/input/child-mind-institute-problematic-internet-use/\"\n\n# Import aggregate fields from parquet files\n# Modified code from rsakata: https://www.kaggle.com/code/rsakata/cmi-piu-16th-place-solution\n\nfiles_train = glob(INPUT_DIR + \"series_train.parquet/*\")\n#if IS_SUBMIT:\n#    files += glob(INPUT_DIR + \"series_test.parquet/*\")\n\nlist_df_train = []\nfor file in tqdm(files_train):\n    df_series = (\n        pl.read_parquet(file)\n        .with_columns(\n            (\n                (pl.col(\"relative_date_PCIAT\") - pl.col(\"relative_date_PCIAT\").min())*24\n                + (pl.col(\"time_of_day\") // int(1e9)) / 3600\n            ).floor().cast(int).alias(\"total_hours\")\n        )\n        .filter(pl.col(\"non-wear_flag\") != 1)\n        .filter(pl.col(\"step\").count().over(\"total_hours\") == 12 * 60)\n        .group_by(\"total_hours\").agg(\n            pl.col(\"enmo\").std().alias(\"enmo_std\"),\n            pl.col(\"anglez\").std().alias(\"anglez_std\"),\n            pl.col(\"light\").std().alias(\"light_std\")\n        )\n        .with_columns(\n            (pl.col(\"total_hours\") % 24).alias(\"hour\"),\n            pl.lit(file.split(\"/\")[-1][3:]).alias(\"id\")\n        )\n    )\n    list_df_train.append(df_series.to_pandas())\n\ndf_series = pd.concat(list_df_train)\ndf_series[\"enmo_std\"] = np.log(df_series[\"enmo_std\"] + 0.01)\ndf_series[\"anglez_std\"] = np.log(df_series[\"anglez_std\"] + 1)\ndf_series[\"light_std\"] = np.log(df_series[\"light_std\"] + 0.01)\n\ndf_agg_train = df_series.groupby(\"id\")[[\"enmo_std\", \"anglez_std\", \"light_std\"]].agg([\"mean\", \"std\"]).reset_index()\ndf_agg_train.columns = [cols[0] + \"_\" + cols[1] if cols[1] != \"\" else cols[0] for cols in df_agg_train.columns]\ndf_agg_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:28:00.464137Z","iopub.execute_input":"2025-01-26T02:28:00.464421Z","iopub.status.idle":"2025-01-26T02:29:13.269798Z","shell.execute_reply.started":"2025-01-26T02:28:00.464397Z","shell.execute_reply":"2025-01-26T02:29:13.268563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data2 = train_data.merge(df_agg_train, how=\"left\", on=\"id\")\ntrain_data2.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:29:13.270795Z","iopub.execute_input":"2025-01-26T02:29:13.271058Z","iopub.status.idle":"2025-01-26T02:29:13.312704Z","shell.execute_reply.started":"2025-01-26T02:29:13.271035Z","shell.execute_reply":"2025-01-26T02:29:13.311571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"files_test = glob(INPUT_DIR + \"series_test.parquet/*\")\n\n\nlist_df_test = []\nfor file in tqdm(files_test):\n    df_series = (\n        pl.read_parquet(file)\n        .with_columns(\n            (\n                (pl.col(\"relative_date_PCIAT\") - pl.col(\"relative_date_PCIAT\").min())*24\n                + (pl.col(\"time_of_day\") // int(1e9)) / 3600\n            ).floor().cast(int).alias(\"total_hours\")\n        )\n        .filter(pl.col(\"non-wear_flag\") != 1)\n        .filter(pl.col(\"step\").count().over(\"total_hours\") == 12 * 60)\n        .group_by(\"total_hours\").agg(\n            pl.col(\"enmo\").std().alias(\"enmo_std\"),\n            pl.col(\"anglez\").std().alias(\"anglez_std\"),\n            pl.col(\"light\").std().alias(\"light_std\")\n        )\n        .with_columns(\n            (pl.col(\"total_hours\") % 24).alias(\"hour\"),\n            pl.lit(file.split(\"/\")[-1][3:]).alias(\"id\")\n        )\n    )\n    list_df_test.append(df_series.to_pandas())\n\ndf_series = pd.concat(list_df_test)\ndf_series[\"enmo_std\"] = np.log(df_series[\"enmo_std\"] + 0.01)\ndf_series[\"anglez_std\"] = np.log(df_series[\"anglez_std\"] + 1)\ndf_series[\"light_std\"] = np.log(df_series[\"light_std\"] + 0.01)\n\ndf_agg_test = df_series.groupby(\"id\")[[\"enmo_std\", \"anglez_std\", \"light_std\"]].agg([\"mean\", \"std\"]).reset_index()\ndf_agg_test.columns = [cols[0] + \"_\" + cols[1] if cols[1] != \"\" else cols[0] for cols in df_agg_test.columns]\ndf_agg_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:29:13.313619Z","iopub.execute_input":"2025-01-26T02:29:13.313891Z","iopub.status.idle":"2025-01-26T02:29:13.471401Z","shell.execute_reply.started":"2025-01-26T02:29:13.313858Z","shell.execute_reply":"2025-01-26T02:29:13.470077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data2 = test_data.merge(df_agg_test, how=\"left\", on=\"id\")\ntest_data2.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:29:21.983794Z","iopub.execute_input":"2025-01-26T02:29:21.985650Z","iopub.status.idle":"2025-01-26T02:29:22.035232Z","shell.execute_reply.started":"2025-01-26T02:29:21.985596Z","shell.execute_reply":"2025-01-26T02:29:22.033829Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = train_data2[['Basic_Demos-Age',\n                      'Basic_Demos-Sex',\n                      'CGAS-CGAS_Score',\n                      'Physical-BMI',\n                      'BIA-BIA_BMI',\n                      'Physical-Waist_Circumference',\n                      'Physical-Diastolic_BP',\n                      'Physical-HeartRate',\n                      'Physical-Systolic_BP',\n                      'Fitness_Endurance-Max_Stage',\n                      'Fitness_Endurance-Time_Mins',\n                      'Fitness_Endurance-Time_Sec',\n                      'FGC-FGC_CU_Zone',\n                      'FGC-FGC_GSND_Zone',\n                      'FGC-FGC_GSD_Zone',\n                      'FGC-FGC_PU_Zone',\n                      'FGC-FGC_SRL_Zone',\n                      'FGC-FGC_SRR_Zone',\n                      'FGC-FGC_TL_Zone',\n                      'BIA-BIA_Activity_Level_num',\n                      'BIA-BIA_BMC',\n                      'BIA-BIA_BMR',\n                      'BIA-BIA_DEE',\n                      'BIA-BIA_ECW',\n                      'BIA-BIA_FFM',\n                      'BIA-BIA_FFMI',\n                      'BIA-BIA_FMI',\n                      'BIA-BIA_Fat',\n                      'BIA-BIA_ICW',\n                      'BIA-BIA_LDM',\n                      'BIA-BIA_LST',\n                      'BIA-BIA_SMM',\n                      'BIA-BIA_TBW',\n                      'PAQ_A-PAQ_A_Total',\n                      'PAQ_C-PAQ_C_Total',\n                      'SDS-SDS_Total_T',\n                      'PreInt_EduHx-computerinternet_hoursday'\n                       ,\n                      'enmo_std_mean',\n                      'enmo_std_std',\n                      'anglez_std_mean',\n                      'anglez_std_std',\n                      'light_std_mean',\n                      'light_std_std'\n                      ]]\n\ny_train = train_data2['PCIAT-PCIAT_Total']\n\nX_test = test_data2[['Basic_Demos-Age',\n                      'Basic_Demos-Sex',\n                      'CGAS-CGAS_Score',\n                      'Physical-BMI',\n                      'BIA-BIA_BMI',\n                      'Physical-Waist_Circumference',\n                      'Physical-Diastolic_BP',\n                      'Physical-HeartRate',\n                      'Physical-Systolic_BP',\n                      'Fitness_Endurance-Max_Stage',\n                      'Fitness_Endurance-Time_Mins',\n                      'Fitness_Endurance-Time_Sec',\n                      'FGC-FGC_CU_Zone',\n                      'FGC-FGC_GSND_Zone',\n                      'FGC-FGC_GSD_Zone',\n                      'FGC-FGC_PU_Zone',\n                      'FGC-FGC_SRL_Zone',\n                      'FGC-FGC_SRR_Zone',\n                      'FGC-FGC_TL_Zone',\n                      'BIA-BIA_Activity_Level_num',\n                      'BIA-BIA_BMC',\n                      'BIA-BIA_BMR',\n                      'BIA-BIA_DEE',\n                      'BIA-BIA_ECW',\n                      'BIA-BIA_FFM',\n                      'BIA-BIA_FFMI',\n                      'BIA-BIA_FMI',\n                      'BIA-BIA_Fat',\n                      'BIA-BIA_ICW',\n                      'BIA-BIA_LDM',\n                      'BIA-BIA_LST',\n                      'BIA-BIA_SMM',\n                      'BIA-BIA_TBW',\n                      'PAQ_A-PAQ_A_Total',\n                      'PAQ_C-PAQ_C_Total',\n                      'SDS-SDS_Total_T',\n                      'PreInt_EduHx-computerinternet_hoursday'\n                      ,\n                      'enmo_std_mean',\n                      'enmo_std_std',\n                      'anglez_std_mean',\n                      'anglez_std_std',\n                      'light_std_mean',\n                      'light_std_std'\n                   ]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:29:24.259886Z","iopub.execute_input":"2025-01-26T02:29:24.260236Z","iopub.status.idle":"2025-01-26T02:29:24.271508Z","shell.execute_reply.started":"2025-01-26T02:29:24.260209Z","shell.execute_reply":"2025-01-26T02:29:24.270323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add this only if we are not interested in the actigraph data\n\nX_train = X_train.drop(columns=['enmo_std_mean',\n                      'enmo_std_std',\n                      'anglez_std_mean',\n                      'anglez_std_std',\n                      'light_std_mean',\n                      'light_std_std'])\n\nX_test = X_test.drop(columns=['enmo_std_mean',\n                      'enmo_std_std',\n                      'anglez_std_mean',\n                      'anglez_std_std',\n                      'light_std_mean',\n                      'light_std_std'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:29:27.176113Z","iopub.execute_input":"2025-01-26T02:29:27.176451Z","iopub.status.idle":"2025-01-26T02:29:27.183580Z","shell.execute_reply.started":"2025-01-26T02:29:27.176424Z","shell.execute_reply":"2025-01-26T02:29:27.182530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add calculated fields\nX_train['Physical-BMI_Calc'] = X_train.apply(lambda row: row['Physical-BMI'] if row['Physical-BMI']==row['Physical-BMI'] else row['BIA-BIA_BMI'],axis=1)\nX_train['Fitness_Endurance-Time_Sec_Calc'] = X_train.apply(lambda row: row['Fitness_Endurance-Time_Sec'] + (row['Fitness_Endurance-Time_Mins']*60), axis=1)\nX_train['PAQ_Total'] = X_train.apply(lambda row: row['PAQ_A-PAQ_A_Total'] if row['PAQ_A-PAQ_A_Total']==row['PAQ_A-PAQ_A_Total'] else row['PAQ_C-PAQ_C_Total'],axis=1)\n\n\n# Drop fields no longer needed\nX_train = X_train.drop(columns=['PAQ_A-PAQ_A_Total','PAQ_C-PAQ_C_Total',\n                     'Physical-BMI','BIA-BIA_BMI',\n                     'Fitness_Endurance-Time_Mins','Fitness_Endurance-Time_Sec'])\n\n# Remove outliers - may give warnings due to NaN value comparison\nX_train.loc[X_train['CGAS-CGAS_Score']>=100.0,'CGAS-CGAS_Score'] = np.nan\nX_train.loc[X_train['Physical-Systolic_BP']>=180.0,'Physical-Systolic_BP'] = np.nan\nX_train.loc[X_train['Physical-Diastolic_BP']>=120.0,'Physical-Diastolic_BP'] = np.nan\nX_train.loc[X_train['BIA-BIA_DEE']>=6000.0,'BIA-BIA_DEE'] = np.nan\nX_train.loc[(X_train['BIA-BIA_BMC']<=0.0) | (X_train['BIA-BIA_BMC']>=16.0),'BIA-BIA_BMC'] = np.nan\nX_train.loc[(X_train['BIA-BIA_BMR']<=0.0) | (X_train['BIA-BIA_BMR']>=2400.0),'BIA-BIA_BMR'] = np.nan\nX_train.loc[(X_train['BIA-BIA_ECW']<=0.0) | (X_train['BIA-BIA_ECW']>=60.0),'BIA-BIA_ECW'] = np.nan\nX_train.loc[(X_train['BIA-BIA_FFM']<=0.0) | (X_train['BIA-BIA_FFM']>=200.0),'BIA-BIA_FFM'] = np.nan\nX_train.loc[(X_train['BIA-BIA_FFMI']<=0.0) | (X_train['BIA-BIA_FFMI']>=25.0),'BIA-BIA_FFMI'] = np.nan\nX_train.loc[(X_train['BIA-BIA_FMI']<=0.0) | (X_train['BIA-BIA_FMI']>=25.0),'BIA-BIA_FMI'] = np.nan\nX_train.loc[(X_train['BIA-BIA_Fat']<=8.0) | (X_train['BIA-BIA_Fat']>=60.0),'BIA-BIA_Fat'] = np.nan\nX_train.loc[(X_train['BIA-BIA_ICW']<=0.0) | (X_train['BIA-BIA_ICW']>=80.0),'BIA-BIA_ICW'] = np.nan\nX_train.loc[(X_train['BIA-BIA_LDM']<=0.0) | (X_train['BIA-BIA_LDM']>=60.0),'BIA-BIA_LDM'] = np.nan\nX_train.loc[(X_train['BIA-BIA_LST']<=0.0) | (X_train['BIA-BIA_LST']>=150.0),'BIA-BIA_LST'] = np.nan\nX_train.loc[(X_train['BIA-BIA_SMM']<=0.0) | (X_train['BIA-BIA_SMM']>=100.0),'BIA-BIA_SMM'] = np.nan\nX_train.loc[(X_train['BIA-BIA_TBW']<=0.0) | (X_train['BIA-BIA_TBW']>=150.0),'BIA-BIA_TBW'] = np.nan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:29:28.694801Z","iopub.execute_input":"2025-01-26T02:29:28.695152Z","iopub.status.idle":"2025-01-26T02:29:28.862403Z","shell.execute_reply.started":"2025-01-26T02:29:28.695127Z","shell.execute_reply":"2025-01-26T02:29:28.861135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_missing_labelled = X_train.loc[y_train.notna()].isnull().sum(axis=1)/X_train.shape[1]\nweights_labelled = 1 - features_missing_labelled\nweights_labelled.shape\n\nfeatures_missing_labelled2 = X_train.loc[y_train.notna()].isnull().sum(axis=1)\nweights_labelled2 = 1 * ((0.95)**features_missing_labelled2)\n\nweights_labelled3 = np.exp((-2)*features_missing_labelled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:29:31.706033Z","iopub.execute_input":"2025-01-26T02:29:31.706385Z","iopub.status.idle":"2025-01-26T02:29:31.722430Z","shell.execute_reply.started":"2025-01-26T02:29:31.706359Z","shell.execute_reply":"2025-01-26T02:29:31.721195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#MICE\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\niter_imputer = IterativeImputer(max_iter=10, random_state=42)\nX_train_fimpute = pd.DataFrame(iter_imputer.fit_transform(X_train), columns = X_train.columns)\nX_train_fimpute.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:29:33.625847Z","iopub.execute_input":"2025-01-26T02:29:33.626207Z","iopub.status.idle":"2025-01-26T02:29:34.537432Z","shell.execute_reply.started":"2025-01-26T02:29:33.626182Z","shell.execute_reply":"2025-01-26T02:29:34.536336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clip imputed values to original max and min\nfor column in X_train_fimpute.columns:\n    max_val = np.max(X_train[column])\n    min_val = np.min(X_train[column])\n    X_train_fimpute.loc[X_train_fimpute[column]>max_val,column] = max_val\n    X_train_fimpute.loc[X_train_fimpute[column]<min_val, column] = min_val\n\nX_train_fimpute.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:29:34.538847Z","iopub.execute_input":"2025-01-26T02:29:34.539250Z","iopub.status.idle":"2025-01-26T02:29:34.648659Z","shell.execute_reply.started":"2025-01-26T02:29:34.539213Z","shell.execute_reply":"2025-01-26T02:29:34.647592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = StandardScaler()                  \n\nX_train_fimpute[X_train_fimpute.columns] = scaler.fit_transform(X_train_fimpute[X_train_fimpute.columns])\nX_train_fimpute.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:29:35.914963Z","iopub.execute_input":"2025-01-26T02:29:35.915370Z","iopub.status.idle":"2025-01-26T02:29:36.024701Z","shell.execute_reply.started":"2025-01-26T02:29:35.915344Z","shell.execute_reply":"2025-01-26T02:29:36.023625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Repeat the above for X_test\n\n# Add calculated fields\nX_test['Physical-BMI_Calc'] = X_test.apply(lambda row: row['Physical-BMI'] if row['Physical-BMI']==row['Physical-BMI'] else row['BIA-BIA_BMI'],axis=1)\nX_test['Fitness_Endurance-Time_Sec_Calc'] = X_test.apply(lambda row: row['Fitness_Endurance-Time_Sec'] + (row['Fitness_Endurance-Time_Mins']*60), axis=1)\nX_test['PAQ_Total'] = X_test.apply(lambda row: row['PAQ_A-PAQ_A_Total'] if row['PAQ_A-PAQ_A_Total']==row['PAQ_A-PAQ_A_Total'] else row['PAQ_C-PAQ_C_Total'],axis=1)\n\n# Drop fields no longer needed\nX_test = X_test.drop(columns=['PAQ_A-PAQ_A_Total','PAQ_C-PAQ_C_Total',\n                     'Physical-BMI','BIA-BIA_BMI',\n                     'Fitness_Endurance-Time_Mins','Fitness_Endurance-Time_Sec'])\n\n# Remove outliers\nX_test.loc[X_test['CGAS-CGAS_Score']>=100.0,'CGAS-CGAS_Score'] = np.nan\nX_test.loc[X_test['Physical-Systolic_BP']>=180.0,'Physical-Systolic_BP'] = np.nan\nX_test.loc[X_test['Physical-Diastolic_BP']>=120.0,'Physical-Diastolic_BP'] = np.nan\nX_test.loc[X_test['BIA-BIA_DEE']>=6000.0,'BIA-BIA_DEE'] = np.nan\nX_test.loc[(X_test['BIA-BIA_BMC']<=0.0) | (X_test['BIA-BIA_BMC']>=16.0),'BIA-BIA_BMC'] = np.nan\nX_test.loc[(X_test['BIA-BIA_BMR']<=0.0) | (X_test['BIA-BIA_BMR']>=2400.0),'BIA-BIA_BMR'] = np.nan\nX_test.loc[(X_test['BIA-BIA_ECW']<=0.0) | (X_test['BIA-BIA_ECW']>=60.0),'BIA-BIA_ECW'] = np.nan\nX_test.loc[(X_test['BIA-BIA_FFM']<=0.0) | (X_test['BIA-BIA_FFM']>=200.0),'BIA-BIA_FFM'] = np.nan\nX_test.loc[(X_test['BIA-BIA_FFMI']<=0.0) | (X_test['BIA-BIA_FFMI']>=25.0),'BIA-BIA_FFMI'] = np.nan\nX_test.loc[(X_test['BIA-BIA_FMI']<=0.0) | (X_test['BIA-BIA_FMI']>=25.0),'BIA-BIA_FMI'] = np.nan\nX_test.loc[(X_test['BIA-BIA_Fat']<=8.0) | (X_test['BIA-BIA_Fat']>=60.0),'BIA-BIA_Fat'] = np.nan\nX_test.loc[(X_test['BIA-BIA_ICW']<=0.0) | (X_test['BIA-BIA_ICW']>=80.0),'BIA-BIA_ICW'] = np.nan\nX_test.loc[(X_test['BIA-BIA_LDM']<=0.0) | (X_test['BIA-BIA_LDM']>=60.0),'BIA-BIA_LDM'] = np.nan\nX_test.loc[(X_test['BIA-BIA_LST']<=0.0) | (X_test['BIA-BIA_LST']>=150.0),'BIA-BIA_LST'] = np.nan\nX_test.loc[(X_test['BIA-BIA_SMM']<=0.0) | (X_test['BIA-BIA_SMM']>=100.0),'BIA-BIA_SMM'] = np.nan\nX_test.loc[(X_test['BIA-BIA_TBW']<=0.0) | (X_test['BIA-BIA_TBW']>=150.0),'BIA-BIA_TBW'] = np.nan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:29:36.752909Z","iopub.execute_input":"2025-01-26T02:29:36.753317Z","iopub.status.idle":"2025-01-26T02:29:36.787998Z","shell.execute_reply.started":"2025-01-26T02:29:36.753289Z","shell.execute_reply":"2025-01-26T02:29:36.786575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Imputation\nX_test_fimpute = pd.DataFrame(iter_imputer.transform(X_test), columns = X_test.columns)\n\n# Clipping\nfor column in X_test_fimpute.columns:\n    max_val = np.max(X_train[column])\n    min_val = np.min(X_train[column])\n    X_test_fimpute.loc[X_test_fimpute[column]>max_val,column] = max_val\n    X_test_fimpute.loc[X_test_fimpute[column]<min_val, column] = min_val\n\n# Scaling\nX_test_fimpute[X_test_fimpute.columns] = scaler.transform(X_test_fimpute[X_test_fimpute.columns])\n\nX_test_fimpute.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:29:38.783542Z","iopub.execute_input":"2025-01-26T02:29:38.784014Z","iopub.status.idle":"2025-01-26T02:29:38.911709Z","shell.execute_reply.started":"2025-01-26T02:29:38.783975Z","shell.execute_reply":"2025-01-26T02:29:38.910611Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Use labelled data only","metadata":{}},{"cell_type":"code","source":"X_train_labelled = X_train_fimpute.loc[y_train.notna()]\ny_train_labelled = y_train[y_train.notna()]\nprint(\"Size of labelled train data set is: \", (X_train_labelled.shape, y_train_labelled.shape))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:29:40.157813Z","iopub.execute_input":"2025-01-26T02:29:40.158211Z","iopub.status.idle":"2025-01-26T02:29:40.168142Z","shell.execute_reply.started":"2025-01-26T02:29:40.158182Z","shell.execute_reply":"2025-01-26T02:29:40.166660Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Since we are not using the unlabelled data in this notebook, we can drop the indices\n# This will help us prevent any accidents when doing cross-validation when we split by index, don't have to think about loc vs iloc etc.\n\nX_train_labelled = X_train_labelled.reset_index(drop=True)\ny_train_labelled = y_train_labelled.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:29:41.030934Z","iopub.execute_input":"2025-01-26T02:29:41.031377Z","iopub.status.idle":"2025-01-26T02:29:41.039353Z","shell.execute_reply.started":"2025-01-26T02:29:41.031344Z","shell.execute_reply":"2025-01-26T02:29:41.038020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weights_labelled = weights_labelled.reset_index(drop=True)\nweights_labelled2 = weights_labelled2.reset_index(drop=True)\nweights_labelled3 = weights_labelled3.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:29:41.510000Z","iopub.execute_input":"2025-01-26T02:29:41.510470Z","iopub.status.idle":"2025-01-26T02:29:41.516505Z","shell.execute_reply.started":"2025-01-26T02:29:41.510417Z","shell.execute_reply":"2025-01-26T02:29:41.515118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_labelled.shape, y_train_labelled.shape, weights_labelled.shape, weights_labelled2.shape, weights_labelled3.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:29:42.349903Z","iopub.execute_input":"2025-01-26T02:29:42.350247Z","iopub.status.idle":"2025-01-26T02:29:42.357343Z","shell.execute_reply.started":"2025-01-26T02:29:42.350221Z","shell.execute_reply":"2025-01-26T02:29:42.356216Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Set up useful functions and data\n* Augmented data\n* QWK metric\n* QWK loss function\n* Weights","metadata":{}},{"cell_type":"code","source":"# X datasets - X_train_labelled, X_train_labelled_aug1a, X_train_labelled_aug1b, X_train_labelled_aug2a, X_train_labelled_aug2b\n# aug1/aug2 - augment once or twice\n# a/b - 0.1 noise multiplier, 0.15 noise multiplier\n\n# Augmented data\n\n# Get standard deviations of each column in X and y\nstd_X = np.std(X_train_labelled, axis=0)\nstd_y = np.std(y_train_labelled, axis=0)\n\n# Create augmented datasets\n\nX_noise_multiplier=0.1\ny_noise_multiplier=0.1\n\n# Deliberately not doing this in a for loop because we will not augment more than twice\n# For ease of understanding the datasets being created\n# And we may choose to add different noisiness to each augmentation\nX_train_labelled_noisy = X_train_labelled + (X_noise_multiplier * np.random.normal(0, std_X, X_train_labelled.shape))\ny_train_labelled_noisy = y_train_labelled + (y_noise_multiplier * np.random.normal(0, std_y, y_train_labelled.shape))\n\nX_train_labelled_aug1a = pd.concat([X_train_labelled,X_train_labelled_noisy], ignore_index=True)\ny_train_labelled_aug1a = pd.concat([y_train_labelled,y_train_labelled_noisy], ignore_index=True)\nprint(X_train_labelled_aug1a.shape, y_train_labelled_aug1a.shape)\n\n#repeat\nX_train_labelled_noisy = X_train_labelled + (X_noise_multiplier * np.random.normal(0, std_X, X_train_labelled.shape))\ny_train_labelled_noisy = y_train_labelled + (y_noise_multiplier * np.random.normal(0, std_y, y_train_labelled.shape))\n\nX_train_labelled_aug2a = pd.concat([X_train_labelled_aug1a,X_train_labelled_noisy], ignore_index=True)\ny_train_labelled_aug2a = pd.concat([y_train_labelled_aug1a,y_train_labelled_noisy], ignore_index=True)\nprint(X_train_labelled_aug2a.shape, y_train_labelled_aug2a.shape)\n\n\n# Increased noise multiplier\nX_noise_multiplier=0.15\ny_noise_multiplier=0.15\n\nX_train_labelled_noisy = X_train_labelled + (X_noise_multiplier * np.random.normal(0, std_X, X_train_labelled.shape))\ny_train_labelled_noisy = y_train_labelled + (y_noise_multiplier * np.random.normal(0, std_y, y_train_labelled.shape))\n\nX_train_labelled_aug1b = pd.concat([X_train_labelled,X_train_labelled_noisy], ignore_index=True)\ny_train_labelled_aug1b = pd.concat([y_train_labelled,y_train_labelled_noisy], ignore_index=True)\nprint(X_train_labelled_aug1b.shape, y_train_labelled_aug1b.shape)\n\n#repeat\nX_train_labelled_noisy = X_train_labelled + (X_noise_multiplier * np.random.normal(0, std_X, X_train_labelled.shape))\ny_train_labelled_noisy = y_train_labelled + (y_noise_multiplier * np.random.normal(0, std_y, y_train_labelled.shape))\n\nX_train_labelled_aug2b = pd.concat([X_train_labelled_aug1b,X_train_labelled_noisy], ignore_index=True)\ny_train_labelled_aug2b = pd.concat([y_train_labelled_aug1b,y_train_labelled_noisy], ignore_index=True)\nprint(X_train_labelled_aug2b.shape, y_train_labelled_aug2b.shape)\n\n\n\nX_noise_multiplier=0.2\ny_noise_multiplier=0.2\n\nX_train_labelled_noisy = X_train_labelled + (X_noise_multiplier * np.random.normal(0, std_X, X_train_labelled.shape))\ny_train_labelled_noisy = y_train_labelled + (y_noise_multiplier * np.random.normal(0, std_y, y_train_labelled.shape))\n\nX_train_labelled_aug1c = pd.concat([X_train_labelled,X_train_labelled_noisy], ignore_index=True)\ny_train_labelled_aug1c = pd.concat([y_train_labelled,y_train_labelled_noisy], ignore_index=True)\nprint(X_train_labelled_aug1c.shape, y_train_labelled_aug1c.shape)\n\n#repeat\nX_train_labelled_noisy = X_train_labelled + (X_noise_multiplier * np.random.normal(0, std_X, X_train_labelled.shape))\ny_train_labelled_noisy = y_train_labelled + (y_noise_multiplier * np.random.normal(0, std_y, y_train_labelled.shape))\n\nX_train_labelled_aug2c = pd.concat([X_train_labelled_aug1c,X_train_labelled_noisy], ignore_index=True)\ny_train_labelled_aug2c = pd.concat([y_train_labelled_aug1c,y_train_labelled_noisy], ignore_index=True)\nprint(X_train_labelled_aug2c.shape, y_train_labelled_aug2c.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:30:05.652143Z","iopub.execute_input":"2025-01-26T02:30:05.652527Z","iopub.status.idle":"2025-01-26T02:30:05.714570Z","shell.execute_reply.started":"2025-01-26T02:30:05.652501Z","shell.execute_reply":"2025-01-26T02:30:05.713499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# y data: y_train_labelled, y_train_labelled_bin, y_train_labelled_sii\n# For simplicity, just augment without added noise for bin and sii\n\ny_train_labelled_sii = y_train_labelled.copy()\ny_train_labelled_sii.name='sii'\ny_train_labelled_sii = y_train_labelled_sii.apply(lambda row: 0 if row<=30 else \n                             (1 if row<50 else (\n                                2 if row<80 else (3)\n                            )))\n\ny_train_labelled_sii_aug1 = pd.concat([y_train_labelled_sii, y_train_labelled_sii], ignore_index=True)\ny_train_labelled_sii_aug2 = pd.concat([y_train_labelled_sii_aug1, y_train_labelled_sii], ignore_index=True)\n\ny_train_labelled_bin = y_train_labelled.copy()\ny_train_labelled_bin.name='PCIAT_bin'\ny_train_labelled_bin = y_train_labelled_bin.apply(lambda row: 0 if row<=15 else \n                                                  (1 if row<=30 else \n                                                   (2 if row<=40 else \n                                                    (3 if row<50 else \n                                                     (4 if row<=65 else \n                                                      (5 if row<80 else \n                                                       (6 if row<=90 else \n                                                        (7)\n                                                       )\n                                                      )\n                                                     )\n                                                    )\n                                                   )\n                                                  )\n                                                 )\n\ny_train_labelled_bin_aug1 = pd.concat([y_train_labelled_bin, y_train_labelled_bin], ignore_index=True)\ny_train_labelled_bin_aug2 = pd.concat([y_train_labelled_bin_aug1, y_train_labelled_bin], ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:30:08.433923Z","iopub.execute_input":"2025-01-26T02:30:08.434325Z","iopub.status.idle":"2025-01-26T02:30:08.447070Z","shell.execute_reply.started":"2025-01-26T02:30:08.434299Z","shell.execute_reply":"2025-01-26T02:30:08.445692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train_labelled_sii.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:30:10.556426Z","iopub.execute_input":"2025-01-26T02:30:10.556821Z","iopub.status.idle":"2025-01-26T02:30:10.564529Z","shell.execute_reply.started":"2025-01-26T02:30:10.556791Z","shell.execute_reply":"2025-01-26T02:30:10.563369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Weights. We already have defined weights_labelled, weights_labelled2, weights_labelled3\n# Just need to augment\n\nweights_labelled_aug1 = pd.concat([weights_labelled, weights_labelled], ignore_index=True)\nweights_labelled_aug2 = pd.concat([weights_labelled_aug1, weights_labelled], ignore_index=True)\nprint(weights_labelled_aug1.shape, weights_labelled_aug2.shape)\n\nweights_labelled2_aug1 = pd.concat([weights_labelled2, weights_labelled2], ignore_index=True)\nweights_labelled2_aug2 = pd.concat([weights_labelled2_aug1, weights_labelled2], ignore_index=True)\nprint(weights_labelled2_aug1.shape, weights_labelled2_aug2.shape)\n\nweights_labelled3_aug1 = pd.concat([weights_labelled3, weights_labelled3], ignore_index=True)\nweights_labelled3_aug2 = pd.concat([weights_labelled3_aug1, weights_labelled3], ignore_index=True)\nprint(weights_labelled3_aug1.shape, weights_labelled3_aug2.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:30:12.052950Z","iopub.execute_input":"2025-01-26T02:30:12.053379Z","iopub.status.idle":"2025-01-26T02:30:12.065521Z","shell.execute_reply.started":"2025-01-26T02:30:12.053349Z","shell.execute_reply":"2025-01-26T02:30:12.064156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom qwk metric and loss function for pciat_sii\n#Modified from https://medium.com/@nlztrk/quadratic-weighted-kappa-qwk-metric-and-how-to-optimize-it-062cc9121baa\ny = np.array([0,1,2,1,1,2,3,3,2,3,2,1,2,1,0,1,0])\n\nc = 1.47\nd = 0.95\n\ng = np.zeros(4)\nfor i in range(4):\n    g[i] = ((y - i)**2).mean()\n    #g[i] = i\n\nprint(g)\nh = [(x-c)**2 + d for x in [0,1,2,3]]\nprint(h)\n\nplt.plot([0,1,2,3], g, marker=\".\", label=\"actual\")\nplt.plot([0,1,2,3], [(x-c)**2 + d for x in [0,1,2,3]], label=\"fitting\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:30:13.197915Z","iopub.execute_input":"2025-01-26T02:30:13.198264Z","iopub.status.idle":"2025-01-26T02:30:13.462797Z","shell.execute_reply.started":"2025-01-26T02:30:13.198238Z","shell.execute_reply":"2025-01-26T02:30:13.461617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Weight matrix for QWK calculation\nW = np.zeros((4, 4))\nfor i in range(len(W)):\n    for j in range(len(W)):\n        W[i][j] = float(((i-j)**2)/((4)-1)**2)\n\nprint(W)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:30:15.209691Z","iopub.execute_input":"2025-01-26T02:30:15.210039Z","iopub.status.idle":"2025-01-26T02:30:15.217596Z","shell.execute_reply.started":"2025-01-26T02:30:15.210013Z","shell.execute_reply":"2025-01-26T02:30:15.216198Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Given a confusion matrix, compute the expected matrix from the outer product of the row and column frequencies\ndef compute_expected_matrix(conf_matrix):\n    row_sums = tf.reduce_sum(conf_matrix, axis=1, keepdims=True)\n    col_sums = tf.reduce_sum(conf_matrix, axis=0, keepdims=True)\n    \n    # Normalize the row and column sums by the total sum of the confusion matrix\n    total_sum = tf.reduce_sum(conf_matrix)\n    \n    # Compute the expected matrix by multiplying the row and column marginals and normalizing\n    expected_matrix = (row_sums @ col_sums) / total_sum\n    \n    return expected_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:30:18.203195Z","iopub.execute_input":"2025-01-26T02:30:18.203680Z","iopub.status.idle":"2025-01-26T02:30:18.209853Z","shell.execute_reply.started":"2025-01-26T02:30:18.203646Z","shell.execute_reply":"2025-01-26T02:30:18.208361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nt1 = tf.constant([0,1,2,3])\nt2 = tf.constant([0,1,2,3])\n#t1 = tf.cast(t1,tf.float32) + c\n#t2 = tf.cast(t2,tf.float32) + c\n#t1 = t1 + c\n#t2 = t2 + c\n\nt2 = tf.clip_by_value(t2, 0, 3)\n    \n# Round predictions to nearest integer (discrete values)\nt1 = tf.round(t1)\nt2 = tf.round(t2)\n\nprint(t1,t2)\n\n\nconfusion_matrix = tf.math.confusion_matrix(tf.cast(t1, tf.int32), tf.cast(t2, tf.int32), num_classes=4)\n#weight_matrix = tf.cast(\n#    tf.abs(tf.subtract(tf.reshape(tf.range(4), (-1, 1)), tf.reshape(tf.range(4), (1, -1)))), tf.float32\n#)\nweight_matrix = tf.cast(tf.constant(W),tf.float32)\n#weight_matrix = tf.constant(W)/(tf.reduce_sum(tf.constant(W)))\n#weighted_kappa = tf.reduce_sum(weight_matrix * tf.cast(confusion_matrix, tf.float32)) / tf.cast(tf.reduce_sum(confusion_matrix),tf.float32)\n#weighted_kappa\nexpected_matrix = compute_expected_matrix(confusion_matrix)\nconfusion_matrix = confusion_matrix / tf.reduce_sum(confusion_matrix)\n\nprint(confusion_matrix)\nprint(weight_matrix)\n#print(weighted_kappa)\nprint(expected_matrix)\nprint(tf.reduce_sum(weight_matrix))\n\nprint(\"Confusion Matrix shape:\", confusion_matrix.shape)\nprint(\"Weight Matrix shape:\", weight_matrix.shape)\nprint(\"Expected Matrix shape:\", expected_matrix.shape)\n\nweighted_kappa = 1 - (tf.reduce_sum((weight_matrix * tf.cast(confusion_matrix,tf.float32)))) / (tf.reduce_sum((weight_matrix * tf.cast(expected_matrix,tf.float32)))) \n\n#weighted_kappa = tf.reduce_sum(weight_matrix * tf.cast(confusion_matrix, tf.float32))\nprint(weighted_kappa)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:30:20.686046Z","iopub.execute_input":"2025-01-26T02:30:20.686500Z","iopub.status.idle":"2025-01-26T02:30:20.694094Z","shell.execute_reply.started":"2025-01-26T02:30:20.686448Z","shell.execute_reply":"2025-01-26T02:30:20.692880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Metric for tracking model as it is trained\n# Writing own QWK score function. Gradients not necessary as this is not the loss function\ndef qwk_sii(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred, 0, 3)\n    y_true = tf.squeeze(y_true)\n    y_pred = tf.squeeze(y_pred)\n\n    y_true = tf.round(y_true)\n    y_pred = tf.round(y_pred)\n\n    confusion_matrix = tf.math.confusion_matrix(tf.cast(y_true, tf.int32), tf.cast(y_pred, tf.int32), num_classes=4)    \n    expected_matrix = compute_expected_matrix(confusion_matrix)\n    #print(confusion_matrix)\n    confusion_matrix = confusion_matrix / tf.reduce_sum(confusion_matrix)\n    expected_matrix = expected_matrix / tf.reduce_sum(expected_matrix)\n    weight_matrix = tf.cast(tf.constant(W),tf.float32) # W defined for 4 classes\n    numerator = tf.reduce_sum(weight_matrix * tf.cast(confusion_matrix, tf.float32))\n    denominator = tf.reduce_sum(weight_matrix * tf.cast(expected_matrix, tf.float32))\n\n    weighted_kappa = 1 - (numerator / denominator)\n\n    #print(confusion_matrix)\n    #print(expected_matrix)\n    #print(weight_matrix)\n\n    #E = np.outer(W, confusion)\n\n    return weighted_kappa\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:30:23.016145Z","iopub.execute_input":"2025-01-26T02:30:23.016612Z","iopub.status.idle":"2025-01-26T02:30:23.024200Z","shell.execute_reply.started":"2025-01-26T02:30:23.016577Z","shell.execute_reply":"2025-01-26T02:30:23.022502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.custom_gradient\ndef custom_confusion_matrix_loss(y_true, y_pred):\n#def custom_confusion_matrix_loss(y_true, y_pred, sample_weight=None):\n    # Clip predictions to the range [0, 3] to calculate confusion matrix\n    # Not overwriting y_true and y_pred so they can be passed as is to the gradient func\n    y_pred = tf.clip_by_value(y_pred, 0, 3)\n    y_true2 = tf.squeeze(y_true)  # Flatten to 1D, e.g., expecting (64,) instead of (64,1)\n    y_pred2 = tf.squeeze(y_pred)  # Flatten to 1D\n    \n    # Round the true and predicted values: 0, 1, 2, 3\n    y_true2 = tf.round(y_true2)\n    y_pred2 = tf.round(y_pred2)\n\n    # Compute the confusion matrix O\n    confusion_matrix = tf.math.confusion_matrix(tf.cast(y_true2, tf.int32), tf.cast(y_pred2, tf.int32), num_classes=4)\n\n    # Compute the expected matrix E\n    expected_matrix = compute_expected_matrix(confusion_matrix)\n\n    # Normalize confusion matrix and expected matrix\n    confusion_matrix = confusion_matrix / tf.reduce_sum(confusion_matrix)\n    expected_matrix = expected_matrix / tf.reduce_sum(expected_matrix)\n\n    # Multiply by weight matrix\n    numerator = tf.reduce_sum(W * tf.cast(confusion_matrix, tf.float32))\n    denominator = tf.reduce_sum(W * tf.cast(expected_matrix, tf.float32))\n\n    weighted_kappa = 1 - numerator / denominator\n    \n    # Forward pass (return the loss)\n    def grad(dy):\n        # Approximate gradient function using c and d determined previously\n        # Referring to: https://medium.com/@nlztrk/quadratic-weighted-kappa-qwk-metric-and-how-to-optimize-it-062cc9121baa\n        labels = y_true + c\n        preds = y_pred + c\n        #print(labels.shape, preds.shape)\n        preds = tf.clip_by_value(preds, 0, 3)\n        #print(labels.shape, preds.shape)\n        f = 1 / 2 * tf.reduce_sum((preds - labels) ** 2)\n        #print(\"f: \", f)\n        g = 1 / 2 * tf.reduce_sum((preds - c) ** 2 + d)\n        #print(\"g: \", g)\n\n        df = preds - labels\n        #print(\"df: \", df)\n        dg = preds - c\n        #print(\"dg: \", dg)\n        grad = (df / g - f * dg / g ** 2) * tf.cast(tf.size(labels), tf.float32)\n        #print(\"grad: \", grad)\n        #grad = (df/g - f*dg/g**2)*len()\n\n        #if sample_weight is not None:\n        #    sample_weight = tf.convert_to_tensor(sample_weight)\n        #    grad = grad * sample_weight  # Apply sample weight to the gradient\n\n        #if sample_weight is None: # This comparison just didn't work. Maybe have to always pass weights explicitly, even if default\n        #    sample_weight = tf.ones_like(grad) # Default sample weights\n\n        #grad = grad * sample_weight\n\n        grad_loss = dy * (grad) # Multiply by dy for backpropagation\n\n        # Experienced problems with exploding gradients, so clipping\n        # Can clip all gradients to -1,1, or scale them by a consistent factor\n        #grad_loss = tf.clip_by_value(grad_loss, -1, 1)\n        clipping_factor = tf.reduce_max(tf.abs(grad_loss)) # Max gradient change\n        #print(\"clipping factor: \", clipping_factor)\n\n        #if clipping_factor>1:\n        #    grad_loss = grad_loss / clipping_factor\n\n        grad_loss = grad_loss / clipping_factor\n\n        #return grad, grad\n        #print(\"grad loss: \", grad_loss)\n        #print(\"shapes: \", (y_true.shape, y_pred.shape), (grad_loss.shape, grad_loss.shape))\n        return grad_loss, grad_loss  # Gradients for y_true, y_pred\n\n    return (1 - weighted_kappa), grad # (1 - weighted_kappa) to treat as a loss function\n    #return weighted_kappa, grad","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:30:24.939920Z","iopub.execute_input":"2025-01-26T02:30:24.940344Z","iopub.status.idle":"2025-01-26T02:30:24.950240Z","shell.execute_reply.started":"2025-01-26T02:30:24.940314Z","shell.execute_reply":"2025-01-26T02:30:24.948974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def quadratic_weighted_kappa_sii(y_true, y_pred):\n    return custom_confusion_matrix_loss(y_true, y_pred)\n#def quadratic_weighted_kappa_sii(y_true, y_pred, sample_weight=None):\n    #return custom_confusion_matrix_loss(y_true, y_pred, sample_weight)\n\nclass QWKLoss(tf.keras.losses.Loss):\n    def __init__(self, name=\"QWKLoss\"):\n        super().__init__(name=name)\n    \n    def call(self, y_true, y_pred):\n        return quadratic_weighted_kappa_sii(y_true, y_pred)\n        #return soft_quadratic_weighted_kappa(y_true, y_pred)\n    #def call(self, y_true, y_pred, sample_weight=None):\n        #return quadratic_weighted_kappa_sii(y_true, y_pred, sample_weight)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:30:33.257036Z","iopub.execute_input":"2025-01-26T02:30:33.257656Z","iopub.status.idle":"2025-01-26T02:30:33.264187Z","shell.execute_reply.started":"2025-01-26T02:30:33.257627Z","shell.execute_reply":"2025-01-26T02:30:33.263022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#t1 = tf.constant([1,3,0,2,3,1,1,0,2])\n#t2 = tf.constant([3,0,3,0,0,3,3,3,0])\nt1 = tf.constant([0,1,2,3])\nt2 = tf.constant([3,3,0,0])\n\nprint(quadratic_weighted_kappa_sii(t1,t2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T02:30:35.860324Z","iopub.execute_input":"2025-01-26T02:30:35.860775Z","iopub.status.idle":"2025-01-26T02:30:36.025138Z","shell.execute_reply.started":"2025-01-26T02:30:35.860743Z","shell.execute_reply":"2025-01-26T02:30:36.023874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# PCA\n\npca = PCA(n_components=25)\n\nX_train_labelled_pca = pd.DataFrame(pca.fit_transform(X_train_labelled))\nX_test_pca = pd.DataFrame(pca.transform(X_test_fimpute))\nX_train_labelled_pca_aug1b = pd.DataFrame(pca.transform(X_train_labelled_aug1b))\nX_train_labelled_pca_aug2b = pd.DataFrame(pca.transform(X_train_labelled_aug2b))\n\nprint(X_train_labelled_pca.shape, X_test_pca.shape, X_train_labelled_pca_aug1b.shape, X_train_labelled_pca_aug2b.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:28:57.290055Z","iopub.execute_input":"2025-01-26T05:28:57.290621Z","iopub.status.idle":"2025-01-26T05:28:57.349249Z","shell.execute_reply.started":"2025-01-26T05:28:57.290585Z","shell.execute_reply":"2025-01-26T05:28:57.347970Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"k = 3  # Number of folds for cross-validation\nkf = KFold(n_splits=k, shuffle=True, random_state=42)  # Set shuffle=True to randomize data splits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:40:27.857757Z","iopub.execute_input":"2025-01-26T05:40:27.858111Z","iopub.status.idle":"2025-01-26T05:40:27.863833Z","shell.execute_reply.started":"2025-01-26T05:40:27.858085Z","shell.execute_reply":"2025-01-26T05:40:27.862379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_model_v7():\n    model = keras.models.Sequential([\n        keras.layers.Dense(20, input_shape=(X_train_labelled.shape[1],), activation=\"selu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(20, activation=\"selu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(1, activation=\"linear\")\n    ])\n\n    #model.compile(optimizer=Adam(learning_rate=0.005), loss='mean_squared_error', metrics=['mae','mse',quadratic_weighted_kappa_sii],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    #model.compile(optimizer=Adam(learning_rate=0.005), loss=QWKLoss(), metrics=['mae','mse'],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    #model.compile(optimizer=Adam(learning_rate=0.005), loss=soft_quadratic_weighted_kappa, metrics=['mae','mse'],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    #model.compile(optimizer=Adam(learning_rate=0.005), loss='mean_squared_error', metrics=['mae','mse',qwk_sii]) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    model.compile(optimizer=Adam(learning_rate=0.0001), loss=QWKLoss(), metrics=['mae','mse',qwk_sii],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T10:53:18.877552Z","iopub.execute_input":"2025-01-24T10:53:18.877969Z","iopub.status.idle":"2025-01-24T10:53:18.883980Z","shell.execute_reply.started":"2025-01-24T10:53:18.877939Z","shell.execute_reply":"2025-01-24T10:53:18.882907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = create_model_v7()\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T10:53:20.150102Z","iopub.execute_input":"2025-01-24T10:53:20.150508Z","iopub.status.idle":"2025-01-24T10:53:20.238229Z","shell.execute_reply.started":"2025-01-24T10:53:20.150449Z","shell.execute_reply":"2025-01-24T10:53:20.237246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n# Prepare to store results\nvalidation_losses = []\nvalidation_qwk = []\nmodels = []\n\n# Loop over each fold\nfor train_index, val_index in kf.split(X_train_labelled):\n    # Split the data into training and validation sets for the current fold\n    X_train_t, X_train_v = X_train_labelled.loc[train_index], X_train_labelled.loc[val_index]\n    y_train_t, y_train_v = y_train_labelled_sii.loc[train_index], y_train_labelled_sii.loc[val_index]\n    weights_labelled_t, weights_labelled_v = weights_labelled3.loc[train_index], weights_labelled3.loc[val_index]\n\n    # Build a new model for each fold\n    model = create_model_v7()\n    \n    # Define early stopping to avoid overfitting\n    #early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n    early_stopping = EarlyStopping(monitor='val_qwk_sii', mode='max', patience=20, restore_best_weights=True)\n    \n    # Train the model on the training set and evaluate on the validation set\n    history = model.fit(\n        X_train_t, y_train_t,\n        validation_data=(X_train_v, y_train_v),\n        epochs=200,\n        batch_size=64,\n        callbacks=[early_stopping],\n        sample_weight=weights_labelled_t,\n        verbose=2\n    )\n    \n    # Evaluate the model on the validation set\n    val_loss, val_mae, val_mse, val_qwk = model.evaluate(X_train_v, y_train_v, verbose=2)\n    validation_losses.append(val_loss)\n    validation_qwk.append(val_qwk)\n    models.append(model)\n\n# Calculate the average validation loss across all folds\navg_val_loss = np.mean(validation_losses)\navg_val_qwk = np.mean(validation_qwk)\nprint(f\"Average Validation Loss: {avg_val_loss}, Average Validation QWK: {avg_val_qwk}\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T04:58:42.764880Z","iopub.execute_input":"2025-01-24T04:58:42.765343Z","iopub.status.idle":"2025-01-24T05:00:57.766395Z","shell.execute_reply.started":"2025-01-24T04:58:42.765308Z","shell.execute_reply":"2025-01-24T05:00:57.764023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_model_v8():\n    model = keras.models.Sequential([\n        keras.layers.Dense(20, input_shape=(X_train_labelled.shape[1],), activation=\"selu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(20, activation=\"selu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(1, activation=\"linear\")\n    ])   \n\n    #model.compile(optimizer=Adam(learning_rate=0.005), loss='mean_squared_error', metrics=['mae','mse',quadratic_weighted_kappa_sii],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    #model.compile(optimizer=Adam(learning_rate=0.005), loss=QWKLoss(), metrics=['mae','mse'],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    #model.compile(optimizer=Adam(learning_rate=0.005), loss=soft_quadratic_weighted_kappa, metrics=['mae','mse'],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    #model.compile(optimizer=Adam(learning_rate=0.005), loss='mean_squared_error', metrics=['mae','mse',qwk_sii]) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    model.compile(optimizer=Adam(learning_rate=0.0001), loss=QWKLoss(), metrics=['mae','mse',qwk_sii],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T03:04:22.687588Z","iopub.execute_input":"2025-01-26T03:04:22.687996Z","iopub.status.idle":"2025-01-26T03:04:22.694690Z","shell.execute_reply.started":"2025-01-26T03:04:22.687971Z","shell.execute_reply":"2025-01-26T03:04:22.693354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = create_model_v8()\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T03:04:26.809171Z","iopub.execute_input":"2025-01-26T03:04:26.809663Z","iopub.status.idle":"2025-01-26T03:04:26.869057Z","shell.execute_reply.started":"2025-01-26T03:04:26.809627Z","shell.execute_reply":"2025-01-26T03:04:26.868058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n# Prepare to store results\nvalidation_losses = []\nvalidation_qwk = []\nmodels = []\n\n# Loop over each fold\nfor train_index, val_index in kf.split(X_train_labelled):\n    # Split the data into training and validation sets for the current fold\n    X_train_t, X_train_v = X_train_labelled_aug2b.loc[train_index], X_train_labelled_aug2b.loc[val_index]\n    y_train_t, y_train_v = y_train_labelled_sii_aug2.loc[train_index], y_train_labelled_sii_aug2.loc[val_index]\n    weights_labelled_t, weights_labelled_v = weights_labelled_aug2.loc[train_index], weights_labelled_aug2.loc[val_index]\n\n    # Build a new model for each fold\n    model = create_model_v8()\n    \n    # Define early stopping to avoid overfitting\n    #early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n    early_stopping = EarlyStopping(monitor='val_qwk_sii', mode='max', patience=10, restore_best_weights=True)\n    \n    # Train the model on the training set and evaluate on the validation set\n    history = model.fit(\n        X_train_t, y_train_t,\n        validation_data=(X_train_v, y_train_v),\n        epochs=100,\n        batch_size=64,\n        callbacks=[early_stopping],\n        #sample_weight=weights_labelled_t,\n        verbose=2\n    )\n    \n    # Evaluate the model on the validation set\n    val_loss, val_mae, val_mse, val_qwk = model.evaluate(X_train_v, y_train_v, verbose=2)\n    validation_losses.append(val_loss)\n    validation_qwk.append(val_qwk)\n    models.append(model)\n\n# Calculate the average validation loss across all folds\navg_val_loss = np.mean(validation_losses)\navg_val_qwk = np.mean(validation_qwk)\nprint(f\"Average Validation Loss: {avg_val_loss}, Average Validation QWK: {avg_val_qwk}\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T03:04:34.874401Z","iopub.execute_input":"2025-01-26T03:04:34.874862Z","iopub.status.idle":"2025-01-26T03:04:52.316684Z","shell.execute_reply.started":"2025-01-26T03:04:34.874833Z","shell.execute_reply":"2025-01-26T03:04:52.314498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_model_v9():\n    model = keras.models.Sequential([\n        keras.layers.Dense(24, input_shape=(X_train_labelled_pca.shape[1],), activation=\"selu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(20, activation=\"selu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        #keras.layers.Dense(16, activation=\"selu\", kernel_initializer=\"he_normal\"),\n        #keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(1, activation=\"linear\")\n    ])   \n\n    #model.compile(optimizer=Adam(learning_rate=0.005), loss='mean_squared_error', metrics=['mae','mse',quadratic_weighted_kappa_sii],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    #model.compile(optimizer=Adam(learning_rate=0.005), loss=QWKLoss(), metrics=['mae','mse'],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    #model.compile(optimizer=Adam(learning_rate=0.005), loss=soft_quadratic_weighted_kappa, metrics=['mae','mse'],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    #model.compile(optimizer=Adam(learning_rate=0.005), loss='mean_squared_error', metrics=['mae','mse',qwk_sii]) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    model.compile(optimizer=Adam(learning_rate=0.0001), loss=QWKLoss(), metrics=['mae','mse',qwk_sii],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T03:36:55.282081Z","iopub.execute_input":"2025-01-26T03:36:55.282505Z","iopub.status.idle":"2025-01-26T03:36:55.289704Z","shell.execute_reply.started":"2025-01-26T03:36:55.282453Z","shell.execute_reply":"2025-01-26T03:36:55.288335Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = create_model_v9()\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T03:36:58.187382Z","iopub.execute_input":"2025-01-26T03:36:58.187802Z","iopub.status.idle":"2025-01-26T03:36:58.245739Z","shell.execute_reply.started":"2025-01-26T03:36:58.187772Z","shell.execute_reply":"2025-01-26T03:36:58.244782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n# Prepare to store results\nvalidation_losses = []\nvalidation_qwk = []\nmodels = []\n\n# Loop over each fold\nfor train_index, val_index in kf.split(X_train_labelled):\n    # Split the data into training and validation sets for the current fold\n    X_train_t, X_train_v = X_train_labelled_pca_aug1b.loc[train_index], X_train_labelled_pca_aug1b.loc[val_index]\n    y_train_t, y_train_v = y_train_labelled_sii_aug1.loc[train_index], y_train_labelled_sii_aug1.loc[val_index]\n    #weights_labelled_t, weights_labelled_v = weights_labelled_aug2.loc[train_index], weights_labelled_aug2.loc[val_index]\n\n    # Build a new model for each fold\n    model = create_model_v9()\n    \n    # Define early stopping to avoid overfitting\n    #early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n    early_stopping = EarlyStopping(monitor='val_qwk_sii', mode='max', patience=10, restore_best_weights=True)\n    \n    # Train the model on the training set and evaluate on the validation set\n    history = model.fit(\n        X_train_t, y_train_t,\n        validation_data=(X_train_v, y_train_v),\n        epochs=100,\n        batch_size=64,\n        callbacks=[early_stopping],\n        #sample_weight=weights_labelled_t,\n        verbose=2\n    )\n    \n    # Evaluate the model on the validation set\n    val_loss, val_mae, val_mse, val_qwk = model.evaluate(X_train_v, y_train_v, verbose=2)\n    validation_losses.append(val_loss)\n    validation_qwk.append(val_qwk)\n    models.append(model)\n\n# Calculate the average validation loss across all folds\navg_val_loss = np.mean(validation_losses)\navg_val_qwk = np.mean(validation_qwk)\nprint(f\"Average Validation Loss: {avg_val_loss}, Average Validation QWK: {avg_val_qwk}\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T03:37:44.399376Z","iopub.execute_input":"2025-01-26T03:37:44.399802Z","iopub.status.idle":"2025-01-26T03:45:42.994810Z","shell.execute_reply.started":"2025-01-26T03:37:44.399772Z","shell.execute_reply":"2025-01-26T03:45:42.993647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# V10: go back to NMSE\n\ndef create_model_v10():\n    model = keras.models.Sequential([\n        keras.layers.Dense(24, input_shape=(X_train_labelled_pca.shape[1],), activation=\"selu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(20, activation=\"selu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"he_normal\"),\n        keras.layers.Dropout(rate=0.2),\n        keras.layers.Dense(1, activation=\"linear\")\n    ])   \n\n    #model.compile(optimizer=Adam(learning_rate=0.005), loss='mean_squared_error', metrics=['mae','mse',quadratic_weighted_kappa_sii],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    #model.compile(optimizer=Adam(learning_rate=0.005), loss=QWKLoss(), metrics=['mae','mse'],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    #model.compile(optimizer=Adam(learning_rate=0.005), loss=soft_quadratic_weighted_kappa, metrics=['mae','mse'],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    model.compile(optimizer=Adam(learning_rate=0.005), loss='mean_squared_error', metrics=['mae','mse']) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    #model.compile(optimizer=Adam(learning_rate=0.0001), loss=QWKLoss(), metrics=['mae','mse',qwk_sii],run_eagerly=True) #run_eagerly=True is necessary in order for the tf.numpy() function in qwk to work\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:38:21.048876Z","iopub.execute_input":"2025-01-26T05:38:21.049325Z","iopub.status.idle":"2025-01-26T05:38:21.057279Z","shell.execute_reply.started":"2025-01-26T05:38:21.049292Z","shell.execute_reply":"2025-01-26T05:38:21.055678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = create_model_v10()\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:38:40.003450Z","iopub.execute_input":"2025-01-26T05:38:40.003981Z","iopub.status.idle":"2025-01-26T05:38:40.095111Z","shell.execute_reply.started":"2025-01-26T05:38:40.003941Z","shell.execute_reply":"2025-01-26T05:38:40.093997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Prepare to store results\nvalidation_losses = []\n#validation_qwk = []\nmodels = []\n\n# Loop over each fold\nfor train_index, val_index in kf.split(X_train_labelled_pca_aug2b):\n    # Split the data into training and validation sets for the current fold\n    X_train_t, X_train_v = X_train_labelled_pca_aug2b.loc[train_index], X_train_labelled_pca_aug2b.loc[val_index]\n    y_train_t, y_train_v = y_train_labelled_aug2b.loc[train_index], y_train_labelled_aug2b.loc[val_index]\n    #weights_labelled_t, weights_labelled_v = weights_labelled3_aug2.loc[train_index], weights_labelled3_aug2.loc[val_index]\n\n    # Build a new model for each fold\n    model = create_model_v10()\n    \n    # Define early stopping to avoid overfitting\n    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n    #early_stopping = EarlyStopping(monitor='val_qwk_sii', mode='max', patience=10, restore_best_weights=True)\n    \n    # Train the model on the training set and evaluate on the validation set\n    history = model.fit(\n        X_train_t, y_train_t,\n        validation_data=(X_train_v, y_train_v),\n        epochs=300,\n        batch_size=64,\n        callbacks=[early_stopping],\n        #sample_weight=weights_labelled_t,\n        verbose=2\n    )\n    \n    # Evaluate the model on the validation set\n    val_loss, val_mae, val_mse = model.evaluate(X_train_v, y_train_v, verbose=2)\n    validation_losses.append(val_loss)\n    #validation_qwk.append(val_qwk)\n    models.append(model)\n\n# Calculate the average validation loss across all folds\navg_val_loss = np.mean(validation_losses)\n#avg_val_qwk = np.mean(validation_qwk)\nprint(f\"Average Validation Loss: {avg_val_loss}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T05:43:09.502573Z","iopub.execute_input":"2025-01-26T05:43:09.502951Z","iopub.status.idle":"2025-01-26T05:43:23.422025Z","shell.execute_reply.started":"2025-01-26T05:43:09.502922Z","shell.execute_reply":"2025-01-26T05:43:23.420380Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n#model1.evaluate(X_train_v, y_train_v)\nval_loss, val_mae, val_mse = model.evaluate(X_train_v, y_train_v)\nprint(f\"Validation loss: {val_loss:.4f}, Validation MAE: {val_mae:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#plt.plot(history.history['qwk_sii'])\n#plt.plot(history.history['val_qwk_sii'])\n#plt.title('Model QWK')\n#plt.xlabel('Epoch')\n#plt.ylabel('QWK')\n#plt.legend()\n#plt.show()\n\n#model1.evaluate(X_train_v, y_train_v)\n#val_loss, val_mae, val_mse, val_qwk = model.evaluate(X_train_v, y_train_v)\n#print(f\"Validation qwk: {val_qwk:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T04:01:51.008398Z","iopub.execute_input":"2025-01-26T04:01:51.008925Z","iopub.status.idle":"2025-01-26T04:01:52.044702Z","shell.execute_reply.started":"2025-01-26T04:01:51.008892Z","shell.execute_reply":"2025-01-26T04:01:52.042857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#tf.experimental.numpy.experimental_enable_numpy_behavior()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T06:26:06.397850Z","iopub.execute_input":"2025-01-21T06:26:06.398302Z","iopub.status.idle":"2025-01-21T06:26:06.404031Z","shell.execute_reply.started":"2025-01-21T06:26:06.398271Z","shell.execute_reply":"2025-01-21T06:26:06.402832Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final prediction with model","metadata":{}},{"cell_type":"code","source":"y_test = test_data[['id']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T04:10:12.358393Z","iopub.execute_input":"2025-01-26T04:10:12.358919Z","iopub.status.idle":"2025-01-26T04:10:12.365688Z","shell.execute_reply.started":"2025-01-26T04:10:12.358882Z","shell.execute_reply":"2025-01-26T04:10:12.364354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#y_test['PCIAT-PCIAT_Total_dnn'] = np.round(np.clip(np.mean([model.predict(X_test_fimpute) for model in models], axis=0),0,3))\n#y_test['PCIAT-PCIAT_Total_dnn'] = np.round(np.clip(np.mean([model.predict(X_test_pca) for model in models], axis=0),0,3))\ny_test['PCIAT-PCIAT_Total_dnn'] = np.mean([model.predict(X_test_pca) for model in models], axis=0)\n\n#y_test['sii'] = y_test['PCIAT-PCIAT_Total_dnn'].astype('int64')\ny_test['sii'] = y_test.apply(lambda row: 0 if row['PCIAT-PCIAT_Total_dnn']<=30 else \n                             (1 if row['PCIAT-PCIAT_Total_dnn']<50 else (\n                                2 if row['PCIAT-PCIAT_Total_dnn']<80 else (3)\n                            )), axis=1)\n\ny_test.head()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T04:12:00.806967Z","iopub.execute_input":"2025-01-26T04:12:00.807556Z","iopub.status.idle":"2025-01-26T04:12:01.053711Z","shell.execute_reply.started":"2025-01-26T04:12:00.807499Z","shell.execute_reply":"2025-01-26T04:12:01.052593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"solution = y_test[['id','sii']]\nsolution.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T04:12:05.087140Z","iopub.execute_input":"2025-01-26T04:12:05.087606Z","iopub.status.idle":"2025-01-26T04:12:05.097089Z","shell.execute_reply.started":"2025-01-26T04:12:05.087568Z","shell.execute_reply":"2025-01-26T04:12:05.095662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}